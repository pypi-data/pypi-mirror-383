# interface/chat.py
from openai import OpenAI
from ..config.manager import ModelConfig
from ..client.model_client import ModelClient

def interactive_chat(config: ModelConfig):
    """äº¤äº’å¼èŠå¤©æ¨¡å¼ï¼ˆä½¿ç”¨OpenAIæ–¹å¼ï¼‰"""
    client = ModelClient(config)

    print("\n=== è¿›å…¥äº¤äº’æ¨¡å¼ ===")
    print("è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'reset' é‡ç½®å¯¹è¯")

    # ç»´æŠ¤å¯¹è¯å†å²
    conversation_history = []

    while True:
        try:
            prompt = input("\næ‚¨: ").strip()

            if prompt.lower() == 'quit':
                break
            elif prompt.lower() == 'reset':
                conversation_history = []
                print("å¯¹è¯å·²é‡ç½®")
                continue

            if not prompt:
                continue

            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            conversation_history.append({"role": "user", "content": prompt})

            client_model = OpenAI(
                api_key=config.config["api_key"],
                base_url=config.config["api_url"]
            )

            if config.config["search"]:
                print("ğŸ”AIå¤§æ¨¡å‹(è”ç½‘)ï¼š", end="", flush=True)
            else:
                print("AIå¤§æ¨¡å‹ï¼š", end="", flush=True)

            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "model": config.config["model"],
                "messages": conversation_history,
                "temperature": config.config["temperature"],
                "max_tokens": config.config["max_tokens"],
                "stream": config.config["stream"],
            }

            # åªåœ¨æ”¯æŒæœç´¢çš„å¹³å°æ·»åŠ æœç´¢å‚æ•°
            if config.config["platform"] == "é˜¿é‡Œç™¾ç‚¼å¤§æ¨¡å‹æœåŠ¡å¹³å°":
                request_params["extra_body"] = {
                    "enable_search": config.config["search"]
                }

            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨
            try:
                response = client_model.chat.completions.create(**request_params)

                if config.config["stream"]:
                    # æµå¼è¾“å‡ºï¼šé€æ­¥è¾“å‡ºå“åº”å†…å®¹
                    full_response = ""
                    for chunk in response:
                        if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                            ai_response_chunk = chunk.choices[0].delta.content
                            full_response += ai_response_chunk
                            print(ai_response_chunk, end="", flush=True)
                    print()  # æ¢è¡Œ

                    # å°†å®Œæ•´å›å¤æ·»åŠ åˆ°å†å²
                    if full_response:
                        conversation_history.append({"role": "assistant", "content": full_response})
                else:
                    # éæµå¼è¾“å‡º
                    if response.choices and response.choices[0].message:
                        ai_response = response.choices[0].message.content
                        print(ai_response)
                        # æ·»åŠ AIå›å¤åˆ°å†å²
                        conversation_history.append({"role": "assistant", "content": ai_response})
                    else:
                        print("æœªæ”¶åˆ°æœ‰æ•ˆå›å¤")

            except Exception as e:
                print(f"è°ƒç”¨å¤±è´¥: {e}")
                # ä»å†å²ä¸­ç§»é™¤å½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼Œå› ä¸ºè°ƒç”¨å¤±è´¥äº†
                if conversation_history and conversation_history[-1]["role"] == "user":
                    conversation_history.pop()

        except KeyboardInterrupt:
            print("\n\né€€å‡ºäº¤äº’æ¨¡å¼")
            break
        except Exception as e:
            print(f"\nå‘ç”Ÿé”™è¯¯: {e}")