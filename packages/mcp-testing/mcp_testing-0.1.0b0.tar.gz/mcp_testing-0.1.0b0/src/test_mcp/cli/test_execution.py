#!/usr/bin/env python3
"""
Test execution engines and coordination logic
"""

import asyncio
import os
import sys
import time
import uuid
from datetime import datetime
from typing import Any

from pydantic import BaseModel, Field
from rich.live import Live
from rich.table import Table

from ..agent.config import build_agent_config_from_server
from ..config.config_manager import ConfigManager, MCPServerConfig
from ..mcp_client.client_manager import SharedTokenStorage
from ..models.compliance import ComplianceTestSuite
from ..models.conversational import ConversationTestSuite
from ..models.factory import TestSuiteType
from ..models.security import SecurityTestSuite
from ..providers.provider_interface import AnthropicProvider, ProviderInterface
from ..shared.console_shared import get_console
from ..shared.progress_tracker import ProgressTracker
from ..testing.conversation.conversation_judge import ConversationJudge
from ..testing.conversation.conversation_manager import ConversationManager
from ..testing.conversation.conversation_models import ConversationConfig
from ..testing.core.test_models import TestCase, TestRunSummary
from ..utils.performance_monitor import SuiteExecutionMetrics, TestExecutionMetrics
from ..utils.rate_limiter import RateLimiter
from .utils import handle_execution_errors, validate_api_keys


class TestRunConfiguration(BaseModel):
    """Type-safe test run configuration"""

    test_run_token: str = Field(..., description="Unique token for this test run")
    suite: TestSuiteType = Field(..., description="Test suite to execute")
    server_config: MCPServerConfig = Field(..., description="MCP server configuration")
    parallelism: int = Field(
        default=1, description="Number of parallel test executions"
    )
    timeout_seconds: int | None = Field(
        default=None, description="Test timeout in seconds"
    )
    skip_judge: bool = Field(default=False, description="Skip LLM judge evaluation")


async def run_tests_parallel(
    test_suite, provider, max_parallelism=5, rate_limiter=None, suite_metrics=None
):
    """Run tests concurrently using provider interface"""

    # Semaphore for controlling parallelism
    semaphore = asyncio.Semaphore(max_parallelism)

    async def run_single_test(test_case_def, test_index):
        async with semaphore:
            session_id = f"test_{test_case_def.test_id}_{test_index}"

            # Rate limiting
            if rate_limiter:
                await rate_limiter.acquire_request_slot(provider.provider_type.value)

            # Performance tracking
            test_start_time = time.time()
            test_metrics = None
            if suite_metrics:
                test_metrics = TestExecutionMetrics(
                    test_id=test_case_def.test_id, start_time=test_start_time
                )
                suite_metrics.test_metrics.append(test_metrics)

            try:
                # Start isolated session
                await provider.start_session(session_id)

                # Run test using provider interface
                result = await run_conversation_with_provider(
                    provider, test_case_def, session_id
                )

                # Update performance metrics
                if test_metrics:
                    test_metrics.end_time = time.time()
                    test_metrics.duration = (
                        test_metrics.end_time - test_metrics.start_time
                    )
                    test_metrics.success = result.get(
                        "status"
                    ) == "completed" and result.get("result", {}).get("success", False)
                    test_metrics.api_calls_made = (
                        1  # Simplified - one API call per test
                    )

                return result

            except Exception as e:
                # Update performance metrics for failed test
                if test_metrics:
                    test_metrics.end_time = time.time()
                    test_metrics.duration = (
                        test_metrics.end_time - test_metrics.start_time
                    )
                    test_metrics.success = False
                    test_metrics.error_message = str(e)
                raise

            finally:
                # Clean up session
                await provider.end_session(session_id)

    # Execute tests concurrently
    tasks = [
        run_single_test(test_case, i)
        for i, test_case in enumerate(test_suite.test_cases, 1)
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results


async def run_test_suite(
    suite: TestSuiteType,
    server_config: dict | MCPServerConfig,
    verbose: bool = False,
    use_global_dir: bool = False,
):
    """Execute test suite using polymorphic design"""

    # Use consistent interface instead of type-specific branching
    test_cases = suite.get_tests()  # Now consistent across all types
    auth_required = suite.auth_required  # Direct field access

    console = get_console()
    console.print(f"Running {len(test_cases)} tests from suite: {suite.name}")
    console.print(f"Authentication required: {auth_required}")

    if verbose:
        console.print(f"Suite type: {type(suite).__name__}")
        console.print(f"Parallelism: {suite.parallelism}")

    # Convert dict server config to typed config
    if isinstance(server_config, dict):
        server_config = MCPServerConfig(**server_config)

    # Single execution path for all suite types
    return await execute_test_cases(
        test_cases=test_cases,
        server_config=server_config,
        suite_config=suite,
        verbose=verbose,
        use_global_dir=use_global_dir,
    )


async def execute_test_cases(
    test_cases: list,
    server_config: MCPServerConfig,
    suite_config: TestSuiteType,
    verbose: bool = False,
    use_global_dir: bool = False,
) -> dict:
    """Execute test cases with live progress updates"""
    from rich.live import Live

    from ..shared.progress_tracker import ProgressTracker
    from ..testing.core.test_models import TestCase

    # Track execution time
    start_time = time.time()

    console = get_console()
    results = []
    successful_tests = 0

    # Determine test type from suite config
    test_type = (
        getattr(suite_config, "test_type", "conversation")
        if hasattr(suite_config, "test_type")
        else getattr(suite_config, "suite_type", "conversation")
    )

    # Initialize progress tracker
    progress_tracker = ProgressTracker(
        total_tests=len(test_cases),
        parallelism=1,  # Sequential execution for now
        test_types=[test_type],
    )

    # Use Rich Live context for real-time updates (same pattern as enhanced progress)
    with Live(progress_tracker.progress, console=console.console, refresh_per_second=2):
        for _i, test_case_config in enumerate(test_cases, 1):
            # Convert config to TestCase model
            test_case = TestCase.from_config(
                test_case_config, server_config.name, default_test_type=test_type
            )
            test_id = test_case.test_id

            # Update progress - test starting
            progress_tracker.update_simple_progress(test_id, "Initializing...")

            if verbose:
                console.print(f"\n[bold blue]Starting test:[/bold blue] {test_id}")
                console.print(f"  Test type: {test_type}")
                console.print(f"  Server: {server_config.url}")

            try:
                # Execute test using the real engine router (from Phase 1)
                result = await run_single_test_case(test_case, server_config, verbose)

                if result.get("success", False):
                    successful_tests += 1
                    progress_tracker.update_simple_progress(
                        test_id, "Completed", completed=True
                    )
                    if verbose:
                        console.print(
                            f"  ‚úÖ [green]PASSED[/green]: {result.get('message', 'Test completed')}"
                        )
                        # Show detailed compliance check results if available
                        if (
                            "details" in result
                            and "compliance_results" in result["details"]
                        ):
                            compliance_results = result["details"]["compliance_results"]
                            for check_result in compliance_results:
                                check_status = "‚úÖ" if check_result.success else "‚ùå"
                                console.print(
                                    f"    {check_status} {check_result.check_name} - {check_result.message}"
                                )

                        # Show detailed security vulnerability results if available
                        if (
                            "details" in result
                            and "security_result" in result["details"]
                            and result["details"]["security_result"]
                        ):
                            security_report = result["details"]["security_result"]
                            if (
                                hasattr(security_report, "test_results")
                                and security_report.test_results
                            ):
                                console.print(
                                    "    [bold yellow]üîí Security Vulnerability Details:[/bold yellow]"
                                )
                                for sec_result in security_report.test_results:
                                    if sec_result.vulnerability_detected:
                                        severity_color = {
                                            "critical": "red",
                                            "high": "red",
                                            "medium": "yellow",
                                            "low": "blue",
                                        }.get(sec_result.severity, "white")
                                        console.print(
                                            f"      üö® [{severity_color}]{sec_result.severity.upper()}[/{severity_color}]: {sec_result.name}"
                                        )
                                        console.print(
                                            f"        Category: {sec_result.category}"
                                        )
                                        if sec_result.attack_vector:
                                            console.print(
                                                f"        Attack Vector: {sec_result.attack_vector}"
                                            )
                                        if sec_result.evidence:
                                            console.print(
                                                f"        Evidence: {sec_result.evidence[0][:100]}{'...' if len(sec_result.evidence[0]) > 100 else ''}"
                                            )
                                    else:
                                        console.print(
                                            f"      ‚úÖ [green]SECURE[/green]: {sec_result.name} ({sec_result.category})"
                                        )

                                # Show vulnerability summary
                                if security_report.vulnerabilities_found > 0:
                                    console.print(
                                        f"    [dim]Summary: {security_report.critical_vulnerabilities} critical, {security_report.high_vulnerabilities} high, {security_report.medium_vulnerabilities} medium, {security_report.low_vulnerabilities} low[/dim]"
                                    )
                else:
                    progress_tracker.update_simple_progress(
                        test_id, "Failed", completed=True
                    )
                    if verbose:
                        error_msg = result.get(
                            "error", result.get("message", "Test failed")
                        )
                        console.print(f"  ‚ùå [red]FAILED[/red]: {error_msg}")
                        # Show detailed compliance check results if available
                        if (
                            "details" in result
                            and "compliance_results" in result["details"]
                        ):
                            compliance_results = result["details"]["compliance_results"]
                            for check_result in compliance_results:
                                check_status = "‚úÖ" if check_result.success else "‚ùå"
                                console.print(
                                    f"    {check_status} {check_result.check_name} - {check_result.message}"
                                )

                        # Show detailed security vulnerability results if available (even for failed tests)
                        if (
                            "details" in result
                            and "security_result" in result["details"]
                            and result["details"]["security_result"]
                        ):
                            security_report = result["details"]["security_result"]
                            if (
                                hasattr(security_report, "test_results")
                                and security_report.test_results
                            ):
                                console.print(
                                    "    [bold yellow]üîí Security Vulnerability Details:[/bold yellow]"
                                )
                                for sec_result in security_report.test_results:
                                    if sec_result.vulnerability_detected:
                                        severity_color = {
                                            "critical": "red",
                                            "high": "red",
                                            "medium": "yellow",
                                            "low": "blue",
                                        }.get(sec_result.severity, "white")
                                        console.print(
                                            f"      üö® [{severity_color}]{sec_result.severity.upper()}[/{severity_color}]: {sec_result.name}"
                                        )
                                        console.print(
                                            f"        Category: {sec_result.category}"
                                        )
                                        if sec_result.attack_vector:
                                            console.print(
                                                f"        Attack Vector: {sec_result.attack_vector}"
                                            )
                                        if sec_result.evidence:
                                            console.print(
                                                f"        Evidence: {sec_result.evidence[0][:100]}{'...' if len(sec_result.evidence[0]) > 100 else ''}"
                                            )
                                    else:
                                        console.print(
                                            f"      ‚úÖ [green]SECURE[/green]: {sec_result.name} ({sec_result.category})"
                                        )

                                # Show vulnerability summary
                                if security_report.vulnerabilities_found > 0:
                                    console.print(
                                        f"    [dim]Summary: {security_report.critical_vulnerabilities} critical, {security_report.high_vulnerabilities} high, {security_report.medium_vulnerabilities} medium, {security_report.low_vulnerabilities} low[/dim]"
                                    )

                results.append(result)

            except Exception as e:
                error_msg = str(e)

                # Check for OAuth authentication failures - these should stop the test suite
                if (
                    "OAuth authentication failed" in error_msg
                    or "unhandled errors in a TaskGroup" in error_msg
                    or "TokenError" in error_msg
                    or "invalid_token" in error_msg
                ):
                    friendly_msg = "OAuth authentication failed"
                    friendly_error = f"OAuth authentication failed for server '{server_config.url}'. This affects all tests - stopping test suite execution."

                    progress_tracker.update_simple_progress(
                        test_id, friendly_msg, completed=True
                    )
                    results.append(
                        {
                            "test_id": test_case.test_id,
                            "success": False,
                            "message": friendly_error,
                            "execution_time": 0.0,
                            "error": str(e),
                        }
                    )

                    # Stop processing remaining tests for OAuth failures
                    console.print(
                        "\n[red]‚ùå OAuth authentication failed - stopping test suite execution[/red]"
                    )
                    console.print(f"[dim]Error details: {error_msg}[/dim]")
                    break

                # Provide user-friendly error messages for common issues
                if (
                    "Connection refused" in error_msg
                    or "Failed to connect" in error_msg
                ):
                    friendly_msg = "Cannot connect to server"
                    friendly_error = f"Cannot connect to server '{server_config.url}'. Please verify the server URL and ensure it's running."
                elif "timeout" in error_msg.lower() or "TimeoutError" in error_msg:
                    friendly_msg = "Connection timeout"
                    friendly_error = f"Connection timeout to server '{server_config.url}'. Server may be slow to respond or unreachable."
                elif "CancelledError" in error_msg:
                    friendly_msg = "Connection cancelled"
                    friendly_error = f"Connection cancelled: Unable to connect to server '{server_config.url}'. Server may be unreachable or down."
                elif "SSL" in error_msg or "certificate" in error_msg.lower():
                    friendly_msg = "SSL/Certificate error"
                    friendly_error = f"SSL/Certificate error connecting to '{server_config.url}'. Server may have certificate issues."
                elif "API key" in error_msg:
                    friendly_msg = "Authentication error"
                    friendly_error = "Authentication error: Please check your API keys (ANTHROPIC_API_KEY, OPENAI_API_KEY)."
                else:
                    friendly_msg = f"Error: {str(e)[:30]}"
                    friendly_error = f"Test execution error: {e!s}"

                progress_tracker.update_simple_progress(
                    test_id, friendly_msg, completed=True
                )
                results.append(
                    {
                        "test_id": test_case.test_id,
                        "success": False,
                        "message": friendly_error,
                        "execution_time": 0.0,
                        "error": str(e),
                    }
                )

    # Show final summary after live display ends
    if verbose or successful_tests < len(test_cases):
        console.print("\n[bold]Test Execution Summary:[/bold]")
        console.print(f"  Total tests: {len(test_cases)}")
        console.print(f"  Passed: [green]{successful_tests}[/green]")
        console.print(f"  Failed: [red]{len(test_cases) - successful_tests}[/red]")

        # Show failed tests with details
        failed_tests = [r for r in results if not r.get("success", True)]
        if failed_tests:
            console.print("\n[bold red]Failure Details:[/bold red]")
            for result in failed_tests:
                test_id = result.get("test_id", "unknown")
                error_msg = result.get("error", result.get("message", "Unknown error"))
                console.print(f"  ‚ùå {test_id}: {error_msg}")

    # ========== NEW RESULT SAVING LOGIC ==========
    # Generate unique run ID
    run_id = str(uuid.uuid4())

    # Calculate execution time
    execution_time = time.time() - start_time

    # Create test_run data structure for persistence
    test_run = {
        "run_id": run_id,
        "timestamp": datetime.now().isoformat(),
        "test_suite": suite_config.model_dump(),
        "server_config": server_config.model_dump(),
        "configuration": {
            "verbose": verbose,
            "test_type": test_type,
            "use_global_dir": use_global_dir,
        },
        "results": results,
        "summary": {
            "total_tests": len(test_cases),
            "successful_tests": successful_tests,
            "failed_tests": len(test_cases) - successful_tests,
        },
    }

    # Create TestRunSummary object
    from ..testing.core.test_models import TestRunSummary

    summary = TestRunSummary(
        run_id=run_id,
        suite_name=suite_config.name,
        total_tests=len(test_cases),
        pass_rate=successful_tests / len(test_cases) if test_cases else 0.0,
        duration_seconds=execution_time,
        timestamp=datetime.now(),
    )

    try:
        from .utils import write_test_results_with_location

        run_file, eval_file = write_test_results_with_location(
            run_id,
            test_run,
            [],
            summary,
            use_global_dir,  # Empty evaluations list for now
        )

        console.print(f"    Test results: {run_file}")
        # Note: eval_file will be None since evaluations is empty

    except Exception as e:
        console.print(
            f"[yellow]Warning: Could not save results to file: {e!s}[/yellow]"
        )

    # ========== END NEW LOGIC ==========

    # Clean up shared OAuth token storage after test suite completes
    SharedTokenStorage.clear_all()

    # Return final results (existing format preserved)
    return {
        "suite_id": suite_config.suite_id,
        "total_tests": len(test_cases),
        "successful_tests": successful_tests,
        "test_results": results,
        "overall_success": successful_tests == len(test_cases),
    }


async def run_single_test_case(
    test_case: TestCase,
    server_config: MCPServerConfig,
    verbose: bool = False,
    suite_config=None,
) -> dict:
    """Execute a single test case using real test engines"""
    from ..shared.progress_tracker import ProgressTracker

    # Create progress tracker for this single test
    progress_tracker = ProgressTracker(total_tests=1, parallelism=1)
    test_id = test_case.test_id

    # Initialize progress
    progress_tracker.update_simple_progress(test_id, "Initializing...")

    try:
        # Determine test type from test case metadata or default to conversation
        test_type = (
            test_case.metadata.get("test_type", "conversation")
            if test_case.metadata
            else "conversation"
        )

        # Route to appropriate real test engine (existing pattern from enhanced progress)
        if test_type == "compliance":
            result = await execute_compliance_test_real(
                test_case, server_config.__dict__, progress_tracker, test_id, verbose
            )
        elif test_type == "security":
            result = await execute_security_test_real(
                test_case, server_config.__dict__, progress_tracker, test_id
            )
        elif test_type == "multi-provider":
            result = execute_multi_provider_test_real(
                test_case, server_config.__dict__, progress_tracker, test_id
            )
        else:
            # Default to conversation test (most common case)
            result = await execute_conversation_test_real(
                test_case,
                server_config.__dict__,
                progress_tracker,
                test_id,
                verbose,
                suite_config,
            )

        # Mark completion
        if result.get("success", False):
            progress_tracker.update_simple_progress(
                test_id, "Completed", completed=True
            )
        else:
            progress_tracker.update_simple_progress(test_id, "Failed", completed=True)

        # Return standardized result format expected by execute_test_cases()
        return {
            "test_id": test_case.test_id,
            "success": result.get("success", False),
            "message": result.get("message", "Test completed"),
            "execution_time": result.get(
                "response_time", result.get("execution_time", 0.0)
            ),
            "details": result,  # Include full result details
        }

    except Exception as e:
        progress_tracker.update_simple_progress(
            test_id, f"Error: {str(e)[:30]}", completed=True
        )
        return {
            "test_id": test_case.test_id,
            "success": False,
            "message": f"Test execution failed: {e!s}",
            "execution_time": 0.0,
            "error": str(e),
        }


def run_tests_by_type(
    test_type: str, suite_config: dict, server_config: dict, verbose: bool = False
):
    """Route to appropriate test engine based on test type"""

    if test_type == "multi-provider":
        return run_multi_provider_tests(suite_config, server_config, verbose)
    elif test_type == "security":
        # Convert dict to SecurityTestSuite
        security_suite = SecurityTestSuite(**suite_config)
        return asyncio.run(run_security_tests(security_suite, server_config, verbose))
    elif test_type == "compliance":
        # Convert dict to ComplianceTestSuite
        compliance_suite = ComplianceTestSuite(**suite_config)
        return asyncio.run(
            run_compliance_tests(compliance_suite, server_config, verbose)
        )
    elif test_type == "conversational":
        # Convert dict to ConversationTestSuite
        conversation_suite = ConversationTestSuite(**suite_config)
        return run_conversational_tests(conversation_suite, server_config, verbose)
    elif test_type == "basic":
        return run_basic_tests(suite_config, server_config, verbose)
    else:
        raise ValueError(f"Unknown test type: {test_type}")


def run_multi_provider_tests(
    suite_config: dict, server_config: dict, verbose: bool = False
):
    """Execute tests across multiple providers using configuration-driven approach"""
    console = get_console()

    # Get providers from suite configuration
    providers = suite_config.get("providers", ["anthropic"])  # Default to anthropic
    test_cases = suite_config.get("test_cases", [])

    if not test_cases:
        console.print("[yellow]No test cases found[/yellow]")
        return False

    console.print(
        f"Running [cyan]{suite_config.get('name', 'Unknown Suite')}[/cyan] across {len(providers)} providers"
    )
    if verbose:
        console.print(f"Providers: {', '.join(providers)}")

    # Get multi-provider configuration from environment
    provider_configs = get_multi_provider_config_from_env(providers)

    try:
        results = []

        for test_case in test_cases:
            test_id = test_case.get("test_id", "unknown")

            # Run test across all configured providers
            provider_results = asyncio.run(
                run_test_across_providers(test_case, provider_configs, verbose)
            )
            results.append({"test_id": test_id, "provider_results": provider_results})

        # Display simplified comparison
        display_multi_provider_summary(results, providers, verbose)
        return True

    except Exception as e:
        console.print(f"[red]Multi-provider testing failed: {e}[/red]")
        return False


async def run_security_tests(
    suite: SecurityTestSuite, server_config: dict, verbose: bool = False
):
    """Execute security tests using typed suite configuration"""
    console = get_console()

    # Use typed suite properties with type safety
    security_tests = suite.get_tests()
    console.print(
        f"Running [red]{len(security_tests)} security tests[/red] for {suite.name}"
    )

    if verbose:
        console.print(f"Authentication required: {suite.auth_required}")
        console.print(f"Include penetration tests: {suite.include_penetration_tests}")

    try:
        from ..security.security_tester import MCPSecurityTester

        # Use suite.auth_required, suite.include_penetration_tests, etc.
        security_tester = MCPSecurityTester(
            server_config,
            auth_required=suite.auth_required,
            include_penetration_tests=suite.include_penetration_tests,
        )

        # Run security assessment
        async def run_assessment():
            return await security_tester.run_security_assessment(security_tests)

        report = await run_assessment()

        # Display security results
        console.print("\nSecurity Assessment Results")
        console.print(
            f"Overall Security Score: {report.overall_security_score:.1f}/100"
        )
        console.print(f"Vulnerabilities Found: {report.vulnerabilities_found}")

        if report.vulnerabilities_found > 0:
            console.print(f"  Critical: {report.critical_vulnerabilities}")
            console.print(f"  High: {report.high_vulnerabilities}")
            console.print(f"  Medium: {report.medium_vulnerabilities}")
            console.print(f"  Low: {report.low_vulnerabilities}")

        if verbose:
            console.print("\nTest Results:")
            for result in report.test_results[:5]:  # Show first 5 results
                status_icon = "‚úÖ" if result.success else "‚ùå"
                console.print(f"  {status_icon} {result.name}: {result.category}")
                if result.evidence:
                    console.print(f"    Evidence: {result.evidence[0][:60]}...")

        return report.overall_security_score >= 70  # Pass/fail threshold

    except Exception as e:
        console.print(f"[red]Security testing failed: {e}[/red]")
        if verbose:
            import traceback

            console.print(f"[red]{traceback.format_exc()}[/red]")
        return False


async def run_compliance_tests(
    suite: ComplianceTestSuite, server_config: dict, verbose: bool = False
):
    """Execute compliance tests using typed suite configuration"""

    from ..shared.progress_tracker import ProgressTracker
    from ..testing.compliance.mcp_compliance_tester import MCPComplianceTester

    console = get_console()

    try:

        async def run_compliance_testing():
            # Create progress tracker if verbose
            compliance_tests = suite.get_tests()
            total_compliance_tests = len(compliance_tests) if compliance_tests else 15

            progress_tracker = (
                ProgressTracker(
                    total_tests=total_compliance_tests,
                    parallelism=1,  # Sequential execution
                    test_types=["compliance"],
                )
                if verbose
                else None
            )

            # Use suite.oauth_required, suite.strict_mode, etc. with type safety
            compliance_tester = MCPComplianceTester(
                server_config=server_config, progress_tracker=progress_tracker
            )

            console.print(
                f"[cyan]Running {len(compliance_tests)} compliance tests against {server_config.get('name', 'server')}...[/cyan]"
            )

            # Run compliance tests
            results = await compliance_tester.run_compliance_tests()

            # Display results
            total_tests = len(results)
            passed_tests = sum(1 for result in results if result.success)
            failed_tests = total_tests - passed_tests

            console.print("\nüìä Compliance Test Results:")
            console.print(f"Total tests: {total_tests}")
            console.print(f"‚úÖ Passed: {passed_tests}")
            console.print(f"‚ùå Failed: {failed_tests}")

            if verbose and results:
                console.print("\nüìã Test Details:")
                for result in results:
                    status_icon = "‚úÖ" if result.success else "‚ùå"
                    severity = (
                        result.severity if hasattr(result, "severity") else "unknown"
                    )
                    console.print(
                        f"  {status_icon} {result.check_name} ({result.category}) - {severity}"
                    )
                    if (
                        not result.success
                        and hasattr(result, "error_message")
                        and result.error_message
                    ):
                        console.print(f"    Error: {result.error_message}")

            # Consider test successful if most tests pass (allowing for some optional features to fail)
            success_rate = passed_tests / total_tests if total_tests > 0 else 0
            overall_success = success_rate >= 0.7  # 70% pass rate threshold

            if overall_success:
                console.print(
                    f"\n‚úÖ MCP compliance tests completed successfully! ({success_rate:.1%} pass rate)"
                )
            else:
                console.print(
                    f"\n‚ùå MCP compliance tests failed. ({success_rate:.1%} pass rate)"
                )

            return overall_success

        # Run the async compliance testing
        return await run_compliance_testing()

    except Exception as e:
        console.print(f"[red]Compliance testing failed: {e}[/red]")
        if verbose:
            import traceback

            console.print(f"[red]{traceback.format_exc()}[/red]")
        return False


def run_conversational_tests(
    suite: ConversationTestSuite,
    server_config: dict,
    verbose: bool = False,
    use_global_dir: bool = False,
):
    """Execute conversational tests using typed suite configuration"""
    console = get_console()

    test_cases = suite.get_tests()
    console.print(f"Running [cyan]{len(test_cases)} conversation tests[/cyan]")

    # Use suite.user_patience_level, suite.conversation_style, etc.
    config = TestRunConfiguration(
        test_run_token="local",
        suite=suite,  # Now properly typed
        mcp_servers=[MCPServerConfig(**server_config)],
        user_patience_level=suite.user_patience_level,
    )

    import asyncio

    return asyncio.run(
        execute_standard_test_flow(
            config.suite,
            config.server_config.model_dump(),
            verbose,
            use_global_dir=use_global_dir,
        )
    )


def run_basic_tests(suite_config: dict, server_config: dict, verbose: bool = False):
    """Execute basic tests using standard flow"""
    # This would use the existing test flow - essentially the default behavior
    return True


def get_multi_provider_config_from_env(
    providers: list[str],
) -> dict[str, dict[str, str]]:
    """Get provider configurations from environment variables"""
    provider_configs = {}

    for provider in providers:
        if provider == "anthropic":
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if api_key:
                provider_configs["anthropic"] = {
                    "api_key": api_key,
                    "model": "claude-sonnet-4-20250514",
                }
        elif provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                provider_configs["openai"] = {"api_key": api_key, "model": "gpt-4"}
        elif provider == "gemini":
            api_key = os.getenv("GEMINI_API_KEY")
            if api_key:
                provider_configs["gemini"] = {"api_key": api_key, "model": "gemini-pro"}

    return provider_configs


async def run_test_across_providers(
    test_case: dict[str, Any],
    provider_configs: dict[str, dict[str, str]],
    verbose: bool,
) -> dict[str, Any]:
    """Run single test across all configured providers"""
    results = {}
    user_message = test_case.get("user_message", "")

    for provider_name, config in provider_configs.items():
        try:
            start_time = time.perf_counter()

            # Execute test with provider using provider interface
            provider_response = await execute_test_with_provider(
                provider_name, user_message, config
            )

            response_time = (time.perf_counter() - start_time) * 1000

            results[provider_name] = {
                "success": True,
                "response": provider_response,
                "response_time_ms": response_time,
            }

        except Exception as e:
            results[provider_name] = {
                "success": False,
                "error": str(e),
                "response_time_ms": 0,
            }

    return results


async def execute_test_with_provider(
    provider_name: str, message: str, config: dict[str, str]
) -> str:
    """Execute test with specific provider using the provider interface"""

    # Route to appropriate provider implementation
    if provider_name == "anthropic":
        provider = AnthropicProvider(config)
    elif provider_name == "openai":
        from ..providers.openai_provider import OpenAIProvider

        provider = OpenAIProvider(config)
    elif provider_name == "gemini":
        # Import gemini provider when available
        raise NotImplementedError("Gemini provider not yet implemented")
    else:
        raise ValueError(f"Unknown provider: {provider_name}")

    # Execute message using provider interface
    response = await provider.send_message(message)
    return response


def display_multi_provider_summary(
    results: list[dict], providers: list[str], verbose: bool
):
    """Display clear multi-provider comparison"""
    console = get_console()
    console.print("\n[bold blue]Multi-Provider Results[/bold blue]")

    # Simple comparison table
    table = Table()
    table.add_column("Test", style="cyan")

    for provider in providers:
        table.add_column(provider.title(), justify="center")

    for result in results:
        test_id = result["test_id"]
        row_data = [test_id]

        for provider in providers:
            provider_result = result["provider_results"].get(provider, {})
            success = provider_result.get("success", False)
            status = "‚úÖ" if success else "‚ùå"

            if verbose and success:
                response_time = provider_result.get("response_time_ms", 0)
                status += f"\n{response_time:.0f}ms"

            row_data.append(status)

        table.add_row(*row_data)

    console.print(table)


def create_provider_from_config(server_config) -> ProviderInterface:
    """Create appropriate provider from server configuration"""
    # For now, default to Anthropic (backward compatibility)
    # Enterprise Security plan will extend this for multi-provider support
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    if not anthropic_key:
        raise ValueError("ANTHROPIC_API_KEY environment variable is required")

    provider_config = {
        "api_key": anthropic_key,
        "model": "claude-sonnet-4-20250514",
        "mcp_servers": [
            {
                "url": server_config.url,
                "name": server_config.name,
                "authorization_token": server_config.authorization_token,
            }
        ],
    }
    return AnthropicProvider(provider_config)


async def run_conversation_with_provider(
    provider: ProviderInterface, test_case_def, session_id
) -> dict:
    """Run conversation using provider interface"""
    # This replaces the conversation manager logic with provider-based execution
    # Maintains the same conversation flow but uses async provider interface
    start_time = time.time()

    try:
        # Send the user message to the provider
        response = await provider.send_message(
            test_case_def.user_message,
            system_prompt="You are a helpful AI assistant testing MCP functionality.",
        )

        end_time = time.time()
        duration = end_time - start_time

        # Create proper ConversationResult object for judge evaluation
        from ..testing.conversation.conversation_models import (
            ConversationResult,
            ConversationStatus,
            ConversationTurn,
        )

        # Convert TestCaseDefinition to TestCase for compatibility
        test_case = TestCase(
            test_id=test_case_def.test_id,
            user_message=test_case_def.user_message,
            success_criteria=test_case_def.success_criteria,
            mcp_server="provider",  # Set from provider interface
            timeout_seconds=test_case_def.timeout_seconds,
            metadata=test_case_def.metadata or {},
        )

        # Create conversation turns with proper speaker attribute
        turns = [
            ConversationTurn(
                turn_number=1,
                speaker="user",
                message=test_case_def.user_message,
                timestamp=datetime.fromtimestamp(start_time),
            ),
            ConversationTurn(
                turn_number=2,
                speaker="agent",
                message=response,
                timestamp=datetime.fromtimestamp(end_time),
            ),
        ]

        # Create proper ConversationResult
        conversation_result = ConversationResult(
            test_case=test_case,
            conversation_id=session_id,
            turns=turns,
            status=ConversationStatus.GOAL_ACHIEVED,
            completion_reason="Single turn response completed",
            goal_achieved=True,  # Simplified - would need proper evaluation
            start_time=datetime.fromtimestamp(start_time),
            end_time=datetime.fromtimestamp(end_time),
            total_duration_seconds=duration,
            total_turns=2,
            user_turns=1,
            agent_turns=1,
            tools_used=[],  # Would be populated if tools were used
            raw_conversation_data=[
                {"role": "user", "content": test_case_def.user_message},
                {"role": "assistant", "content": response},
            ],
        )

        # Create result dict that maintains backward compatibility
        result = {
            "test_case": {
                "test_id": test_case_def.test_id,
                "user_message": test_case_def.user_message,
                "success_criteria": test_case_def.success_criteria,
                "timeout_seconds": test_case_def.timeout_seconds,
                "metadata": test_case_def.metadata or {},
            },
            "result": {
                "status": {"value": "goal_achieved"},  # Simplified status
                "turns": [
                    {"role": "user", "content": test_case_def.user_message},
                    {"role": "assistant", "content": response},
                ],
                "duration": duration,
                "success": True,  # Simplified success determination
            },
            "result_obj": conversation_result,  # Now a proper ConversationResult object
            "status": "completed",
        }

        return result

    except Exception as e:
        return {
            "test_case": test_case_def.model_dump(),
            "result": None,
            "error": str(e),
            "status": "failed",
        }


def display_performance_summary(suite_metrics, verbose: bool):
    """Display performance summary (always enabled)"""
    console = get_console()

    stats = suite_metrics.get_summary_stats()

    if stats.get("status") == "no_completed_tests":
        console.print("\n[yellow]No completed tests to analyze[/yellow]")
        return

    if verbose:
        console.print("\n[bold blue]Performance Summary[/bold blue]")
        console.print(f"Success Rate: {stats['success_rate']:.1%}")
        console.print(f"Average Duration: {stats['average_duration']:.1f}s")
        console.print(f"Total API Calls: {stats['total_api_calls']}")
        console.print(f"Efficiency: {stats['parallelism_efficiency']:.1f} tests/sec")
    else:
        # Show minimal summary
        console.print(
            f"Performance: {stats['success_rate']:.0%} success, {stats['average_duration']:.1f}s avg"
        )


async def execute_standard_test_flow(
    suite_config: dict,
    server_config: dict | MCPServerConfig,
    verbose: bool = False,
    max_turns: int = 10,
    skip_judge: bool = False,
    use_global_dir: bool = False,
) -> bool:
    """Execute standard test flow with configuration-driven approach"""
    console = get_console()

    # Convert dict configs to proper objects if needed - suite_config should already be typed
    # Removed TestSuiteDefinition usage as we now use type-safe models
    if isinstance(server_config, dict):
        server_config = MCPServerConfig(**server_config)

    console.print("Using standard test flow...")

    # Build agent configuration
    console.print("Building agent configuration...")
    anthropic_key, openai_key = validate_api_keys()
    agent_config = build_agent_config_from_server(server_config, anthropic_key)

    # Setup conversation configuration with suite parameters
    conversation_config = ConversationConfig(
        max_turns=max_turns,
        timeout_seconds=300,
        user_patience_level=getattr(suite_config, "user_patience_level", "medium"),
    )

    # Initialize conversation manager
    ConversationManager(config=agent_config, conversation_config=conversation_config)

    # Get parallelism from test suite configuration
    parallelism = getattr(suite_config, "parallelism", 5)
    console.print(f"Parallelism: {parallelism} concurrent tests")

    # Create resource management components
    console.print("Setting up resource management...")
    rate_limiter = RateLimiter()

    suite_metrics = SuiteExecutionMetrics(
        suite_id=suite_config.suite_id,
        start_time=time.time(),
        parallelism_used=parallelism,
    )

    # Create provider from server configuration
    console.print("Creating provider interface...")
    provider = create_provider_from_config(server_config)

    # Run tests with parallel execution
    console.print("Running test cases in parallel...")

    # Execute tests concurrently with resource management
    start_time = time.time()
    try:
        test_results = await run_tests_parallel(
            suite_config,
            provider,
            max_parallelism=parallelism,
            rate_limiter=rate_limiter,
            suite_metrics=suite_metrics,
        )

        # Process results and handle any execution errors
        successful_results, error_results = handle_execution_errors(
            test_results, suite_config
        )

    except KeyboardInterrupt:
        console.print("\nTest execution interrupted by user")
        console.print("Partial results may be available in test_results/ directory")
        return False
    except Exception as e:
        console.print(f"Critical error during test execution: {e!s}")
        if verbose:
            import traceback

            console.print(traceback.format_exc())
        return False

    execution_time = time.time() - start_time
    suite_metrics.total_duration = execution_time

    console.print(f"\nExecution completed in {execution_time:.1f} seconds")

    # Count successful tests
    successful_tests = len(
        [
            r
            for r in test_results
            if r.get("status") == "completed"
            and r.get("result_obj")
            and r["result_obj"].status.value == "goal_achieved"
        ]
    )

    # Generate run ID and save results
    run_id = str(uuid.uuid4())
    test_run = {
        "run_id": run_id,
        "timestamp": datetime.now().isoformat(),
        "test_suite": suite_config.model_dump(),
        "server_config": server_config.model_dump(),
        "configuration": {"max_turns": max_turns, "skip_judge": skip_judge},
        "results": test_results,
        "summary": {
            "total_tests": len(suite_config.test_cases),
            "completed_tests": len(
                [r for r in test_results if r.get("status") == "completed"]
            ),
            "successful_tests": successful_tests,
        },
    }

    # Run judge evaluation if not skipped
    evaluations = []
    if not skip_judge and test_results:
        console.print("\nRunning LLM evaluation...")
        try:
            judge = ConversationJudge()
            for result in test_results:
                if result.get("status") == "completed" and result.get("result_obj"):
                    eval_result = judge.evaluate_conversation(
                        conversation=result["result_obj"]
                    )
                    evaluations.append(eval_result.model_dump())
        except Exception as e:
            console.print(f"    Judge evaluation failed: {e!s}")

    # Generate summary
    summary = TestRunSummary(
        run_id=run_id,
        suite_name=suite_config.name,
        total_tests=len(suite_config.test_cases),
        pass_rate=(
            successful_tests / len(suite_config.test_cases)
            if suite_config.test_cases
            else 0.0
        ),
        duration_seconds=execution_time,
        timestamp=datetime.now(),
    )

    from .utils import write_test_results_with_location

    run_file, eval_file = write_test_results_with_location(
        run_id, test_run, evaluations, summary, use_global_dir
    )

    console.print(f"    Test results: {run_file}")
    if eval_file:
        console.print(f"    Evaluations: {eval_file}")

    # Return success status
    return successful_tests == len(suite_config.test_cases)


def run_with_mcpt_inference(
    test_type: str, server_id: str, verbose: bool, use_global_dir: bool = False
):
    """Run tests with smart suite inference using direct core functions"""
    config_manager = ConfigManager()
    console = get_console()

    # Smart inference based on test type
    suite_id = f"{test_type}-tests"  # Standard naming convention

    try:
        suite_config = config_manager.get_suite_by_id(suite_id)
        server_config = config_manager.get_server_by_id(server_id)

        console.print(
            f"Running {test_type} tests against [cyan]{server_config.name}[/cyan]"
        )

        # Use core test execution functions directly
        try:
            if test_type == "compliance":
                success = asyncio.run(
                    run_compliance_tests(suite_config, server_config, verbose)
                )
            elif test_type == "security":
                success = asyncio.run(
                    run_security_tests(suite_config, server_config, verbose)
                )
            else:
                # Default to standard test execution
                success = asyncio.run(
                    execute_standard_test_flow(
                        suite_config,
                        server_config,
                        verbose,
                        use_global_dir=use_global_dir,
                    )
                )

            if success:
                console.print(
                    f"[green]‚úÖ {test_type.capitalize()} tests completed successfully![/green]"
                )
                sys.exit(0)
            else:
                console.print(f"[red]‚ùå {test_type.capitalize()} tests failed![/red]")
                sys.exit(1)
        except Exception as e:
            console.print(
                f"[red]‚ùå {test_type.capitalize()} test execution failed: {e!s}[/red]"
            )
            if verbose:
                import traceback

                console.print(traceback.format_exc())
            sys.exit(1)

    except KeyError:
        console.print(f"[yellow]No {test_type} test suite found.[/yellow]")
        console.print(f"Use 'mcp-t init' to create a {test_type} test configuration")
        sys.exit(1)


async def run_tests_with_enhanced_progress(
    suite_config, server_config, verbose: bool = False
) -> bool:
    """Run tests using real test execution engines with enhanced progress tracking"""
    console = get_console()
    test_cases = suite_config.test_cases

    if not test_cases:
        console.print("[yellow]No test cases found[/yellow]")
        return False

    suite_name = getattr(suite_config, "name", "Test Suite")
    server_name = getattr(server_config, "name", "MCP Server")
    test_type = getattr(suite_config, "test_type", "basic")

    # Initialize progress tracker
    progress_tracker = ProgressTracker(
        total_tests=len(test_cases),
        parallelism=1,  # Sequential for enhanced UI
        test_types=[test_type],
    )

    success_count = 0
    results = []

    with Live(progress_tracker.progress, console=console.console, refresh_per_second=2):
        for i, test_case in enumerate(test_cases, 1):
            test_id = getattr(test_case, "test_id", f"test_{i}")

            # Update progress - test starting
            progress_tracker.update_simple_progress(test_id, "Initializing...")

            try:
                # Use REAL test execution based on test type
                if test_type == "compliance":
                    result = await execute_compliance_test_real(
                        test_case, server_config, progress_tracker, test_id
                    )
                elif test_type == "security":
                    result = await execute_security_test_real(
                        test_case, server_config, progress_tracker, test_id
                    )
                elif test_type == "multi-provider":
                    result = execute_multi_provider_test_real(
                        test_case, server_config, progress_tracker, test_id
                    )
                else:
                    # Default conversation test using existing engine
                    result = await execute_conversation_test_real(
                        test_case,
                        server_config,
                        progress_tracker,
                        test_id,
                        False,
                        suite_config,
                    )

                if result.get("success", False):
                    success_count += 1
                    progress_tracker.update_simple_progress(
                        test_id, "Completed", completed=True
                    )
                    results.append(
                        {"test_id": test_id, "status": "PASS", "success": True}
                    )
                else:
                    progress_tracker.update_simple_progress(
                        test_id, "Failed", completed=True
                    )
                    results.append(
                        {
                            "test_id": test_id,
                            "status": "FAIL",
                            "success": False,
                            "error": result.get("error", "Test failed"),
                        }
                    )

            except Exception as e:
                progress_tracker.update_simple_progress(
                    test_id, f"Error: {str(e)[:30]}", completed=True
                )
                results.append(
                    {
                        "test_id": test_id,
                        "status": "ERROR",
                        "success": False,
                        "error": str(e),
                    }
                )

    # Display final results (keeping existing formatting)
    display_enhanced_final_results(
        results, suite_name, server_name, success_count, test_cases, verbose
    )
    return success_count == len(test_cases)


async def execute_conversation_test_real(
    test_case: dict,
    server_config: dict,
    progress_tracker,
    test_id: str,
    verbose: bool = False,
    suite_config=None,
) -> dict:
    """Execute real conversation test with detailed progress tracking"""
    progress_tracker.update_simple_progress(test_id, "Building agent config...")

    try:
        # Validate API keys
        anthropic_key, openai_key = validate_api_keys()
        progress_tracker.update_simple_progress(test_id, "Connecting to server...")

        # Build agent config from server config (handle both dict and MCPServerConfig objects)
        server_model = (
            server_config
            if isinstance(server_config, MCPServerConfig)
            else MCPServerConfig(**server_config)
        )
        agent_config = build_agent_config_from_server(server_model, anthropic_key)

        # Setup conversation configuration with suite parameters
        conversation_config = ConversationConfig(
            max_turns=10,
            timeout_seconds=300,
            user_patience_level=(
                getattr(suite_config, "user_patience_level", "medium")
                if suite_config
                else "medium"
            ),
        )
        conversation_manager = ConversationManager(
            config=agent_config, conversation_config=conversation_config
        )

        progress_tracker.update_simple_progress(test_id, "Running conversation...")

        # Convert test case to proper model
        server_name = (
            server_model.name
            if hasattr(server_model, "name")
            else server_model.get("name", "unknown")
        )
        metadata = getattr(test_case, "metadata", {})
        if metadata is None:
            metadata = {}
        test_case_model = TestCase(
            test_id=test_case.test_id,
            user_message=test_case.user_message,
            success_criteria=test_case.success_criteria,
            mcp_server=server_name,
            timeout_seconds=getattr(test_case, "timeout_seconds", 300),
            metadata=metadata,
        )

        # Execute conversation using real engine
        result = await conversation_manager.run_conversation(test_case_model)

        progress_tracker.update_simple_progress(test_id, "Evaluating results...")

        # Return standardized result
        success_value = (
            result.status.value == "goal_achieved" if result.status else False
        )

        if verbose:
            progress_tracker.update_simple_progress(
                test_id,
                f"Result: {result.status.value if result.status else 'No status'} -> {'PASS' if success_value else 'FAIL'}",
                completed=False,
            )

        return {
            "success": success_value,
            "response_time": (
                result.total_duration_seconds
                if hasattr(result, "total_duration_seconds")
                else 0.0
            ),
            "message": f"Conversation completed with {len(result.turns) if result.turns else 0} turns",
            "conversation_result": result,
        }

    except TimeoutError:
        progress_tracker.update_simple_progress(
            test_id, "Connection timeout", completed=True
        )
        return {
            "success": False,
            "error": f"Connection timeout: Could not connect to server '{server_model.url}' within timeout period. Please check if the server is running.",
            "response_time": 0.0,
        }
    except asyncio.CancelledError:
        progress_tracker.update_simple_progress(
            test_id, "Connection cancelled", completed=True
        )
        return {
            "success": False,
            "error": f"Connection cancelled: Unable to connect to server '{server_model.url}'. Server may be unreachable or down.",
            "response_time": 0.0,
        }
    except Exception as e:
        progress_tracker.update_simple_progress(test_id, "Test failed", completed=True)
        error_msg = str(e)

        # Provide user-friendly error messages for common issues
        if "Failed to connect" in error_msg or "Connection refused" in error_msg:
            friendly_error = f"Cannot connect to server '{server_model.url}'. Please verify the server URL and ensure it's running."
        elif "timeout" in error_msg.lower():
            friendly_error = f"Connection timeout to server '{server_model.url}'. Server may be slow to respond or unreachable."
        elif "SSL" in error_msg or "certificate" in error_msg.lower():
            friendly_error = f"SSL/Certificate error connecting to '{server_model.url}'. Server may have certificate issues."
        elif "API key" in error_msg or "authentication" in error_msg.lower():
            friendly_error = "Authentication error: Please check your API keys (ANTHROPIC_API_KEY, OPENAI_API_KEY)."
        else:
            friendly_error = f"Conversation test failed: {error_msg}"

        return {
            "success": False,
            "error": friendly_error,
            "response_time": 0.0,
        }


async def execute_compliance_test_real(
    test_case: dict,
    server_config: dict,
    progress_tracker,
    test_id: str,
    verbose: bool = False,
) -> dict:
    """Execute real compliance test using existing MCPComplianceTester"""
    progress_tracker.update_simple_progress(
        test_id, "Initializing compliance tester..."
    )

    try:
        from ..testing.compliance.mcp_compliance_tester import MCPComplianceTester

        # Handle both dict and MCPServerConfig objects
        server_model = (
            server_config
            if isinstance(server_config, MCPServerConfig)
            else MCPServerConfig(**server_config)
        )

        # Extract check_categories from test case or infer from test_id
        check_categories = None
        if hasattr(test_case, "check_categories"):
            check_categories = test_case.check_categories
        elif isinstance(test_case, dict) and "check_categories" in test_case:
            check_categories = test_case["check_categories"]
        else:
            # If we have a TestCase object, infer category from test_id
            test_id_to_category = {
                "protocol_handshake": ["handshake"],
                "capabilities_discovery": ["capabilities"],
                "tool_enumeration": ["tools"],
                "oauth_flow": ["auth"],
            }
            test_case_id = (
                test_case.test_id
                if hasattr(test_case, "test_id")
                else test_case.get("test_id")
            )
            check_categories = test_id_to_category.get(test_case_id)

        # Create compliance tester with server config
        compliance_tester = MCPComplianceTester(server_model.__dict__, progress_tracker)

        progress_tracker.update_simple_progress(test_id, "Running compliance checks...")

        # Execute compliance tests with filtering by categories
        if check_categories:
            results = await compliance_tester.run_compliance_tests(
                check_categories=check_categories
            )
        else:
            # Fallback to all tests if no categories specified
            results = await compliance_tester.run_compliance_tests()

        # Filter results to only include the specific categories for this test
        if check_categories and results:
            filtered_results = [
                r
                for r in results
                if r.category.lower() in [cat.lower() for cat in check_categories]
            ]
            results = filtered_results if filtered_results else results

        # Extract result for our specific test
        if results and len(results) > 0:
            overall_success = all(result.success for result in results)
            passed_count = sum(1 for r in results if r.success)

            # Create more specific message for this test
            category_name = check_categories[0] if check_categories else "compliance"
            test_description = f"{category_name.title()} compliance test"

            return {
                "success": overall_success,
                "response_time": 0.0,  # Compliance tests don't track individual timing
                "message": f"{test_description} completed: {len(results)} checks, {passed_count} passed",
                "compliance_results": results,
            }
        else:
            return {
                "success": False,
                "response_time": 0.0,
                "message": f"No compliance test results returned for categories: {check_categories}",
                "compliance_results": [],
            }

    except TimeoutError:
        progress_tracker.update_simple_progress(
            test_id, "Connection timeout", completed=True
        )
        return {
            "success": False,
            "error": f"Connection timeout: Could not connect to server '{server_model.url}' within 30 seconds. Please check if the server is running.",
            "response_time": 0.0,
        }
    except asyncio.CancelledError:
        progress_tracker.update_simple_progress(
            test_id, "Connection cancelled", completed=True
        )
        return {
            "success": False,
            "error": f"Connection cancelled: Unable to connect to server '{server_model.url}'. Server may be unreachable or down.",
            "response_time": 0.0,
        }
    except Exception as e:
        progress_tracker.update_simple_progress(test_id, "Test failed", completed=True)
        error_msg = str(e)

        # Provide user-friendly error messages for common issues
        if "Failed to connect" in error_msg or "Connection refused" in error_msg:
            friendly_error = f"Cannot connect to server '{server_model.url}'. Please verify the server URL and ensure it's running."
        elif "timeout" in error_msg.lower():
            friendly_error = f"Connection timeout to server '{server_model.url}'. Server may be slow to respond or unreachable."
        elif "SSL" in error_msg or "certificate" in error_msg.lower():
            friendly_error = f"SSL/Certificate error connecting to '{server_model.url}'. Server may have certificate issues."
        else:
            friendly_error = f"Compliance test failed: {error_msg}"

        return {
            "success": False,
            "error": friendly_error,
            "response_time": 0.0,
        }


async def execute_security_test_real(
    test_case: dict, server_config: dict, progress_tracker, test_id: str
) -> dict:
    """Execute real security test using existing SecurityTester"""
    progress_tracker.update_simple_progress(test_id, "Initializing security tester...")

    try:
        from ..security.security_tester import MCPSecurityTester

        # Handle both dict and MCPServerConfig objects
        server_model = (
            server_config
            if isinstance(server_config, MCPServerConfig)
            else MCPServerConfig(**server_config)
        )

        # Create security tester with server config dict
        security_tester = MCPSecurityTester(
            server_model.__dict__,
            auth_required=getattr(server_model, "auth_required", False),
            include_penetration_tests=True,
        )

        progress_tracker.update_simple_progress(
            test_id, "Running security assessment..."
        )

        # Execute security assessment (this method actually exists)
        results = await security_tester.run_security_assessment()

        if results:
            return {
                "success": results.overall_security_score >= 70,  # Pass threshold
                "response_time": 0.0,  # Security tests don't track individual timing
                "message": f"Security assessment completed: {results.overall_security_score:.1f}/100, {results.vulnerabilities_found} vulnerabilities",
                "security_result": results,
            }
        else:
            return {
                "success": False,
                "response_time": 0.0,
                "message": "Security assessment returned no results",
                "security_result": None,
            }

    except TimeoutError:
        progress_tracker.update_simple_progress(
            test_id, "Connection timeout", completed=True
        )
        return {
            "success": False,
            "error": f"Connection timeout: Could not connect to server '{server_model.url}' within 30 seconds. Please check if the server is running.",
            "response_time": 0.0,
        }
    except asyncio.CancelledError:
        progress_tracker.update_simple_progress(
            test_id, "Connection cancelled", completed=True
        )
        return {
            "success": False,
            "error": f"Connection cancelled: Unable to connect to server '{server_model.url}'. Server may be unreachable or down.",
            "response_time": 0.0,
        }
    except Exception as e:
        progress_tracker.update_simple_progress(test_id, "Test failed", completed=True)
        error_msg = str(e)

        # Provide user-friendly error messages for common issues
        if "Failed to connect" in error_msg or "Connection refused" in error_msg:
            friendly_error = f"Cannot connect to server '{server_model.url}'. Please verify the server URL and ensure it's running."
        elif "timeout" in error_msg.lower():
            friendly_error = f"Connection timeout to server '{server_model.url}'. Server may be slow to respond or unreachable."
        elif "SSL" in error_msg or "certificate" in error_msg.lower():
            friendly_error = f"SSL/Certificate error connecting to '{server_model.url}'. Server may have certificate issues."
        else:
            friendly_error = f"Security test failed: {error_msg}"

        return {
            "success": False,
            "error": friendly_error,
            "response_time": 0.0,
        }


def execute_multi_provider_test_real(
    test_case: dict, server_config: dict, progress_tracker, test_id: str
) -> dict:
    """Execute real multi-provider test using existing multi-provider system"""
    progress_tracker.update_simple_progress(
        test_id, "Setting up multi-provider test..."
    )

    try:
        # Use existing multi-provider execution
        providers = ["anthropic", "openai"]  # Default providers
        provider_configs = get_multi_provider_config_from_env(providers)

        progress_tracker.update_simple_progress(
            test_id, f"Testing across {len(providers)} providers..."
        )

        # Convert TestCase object to dict format expected by run_test_across_providers
        test_case_dict = {
            "test_id": test_case.test_id,
            "user_message": test_case.user_message,
            "success_criteria": test_case.success_criteria,
            "timeout_seconds": getattr(test_case, "timeout_seconds", 300),
        }

        # Execute across all providers using existing function
        provider_results = asyncio.run(
            run_test_across_providers(test_case_dict, provider_configs, verbose=False)
        )

        # Determine overall success
        successful_providers = sum(
            1 for result in provider_results.values() if result.get("success", False)
        )
        total_providers = len(provider_results)

        return {
            "success": successful_providers
            > 0,  # Success if at least one provider succeeds
            "response_time": 0.0,  # Multi-provider tests don't track individual timing
            "message": f"Multi-provider test: {successful_providers}/{total_providers} providers succeeded",
            "provider_results": provider_results,
            "success_rate": (
                successful_providers / total_providers if total_providers > 0 else 0.0
            ),
        }

    except Exception as e:
        return {
            "success": False,
            "error": f"Multi-provider test failed: {e!s}",
            "response_time": 0.0,
        }


def display_enhanced_final_results(
    results: list,
    suite_name: str,
    server_name: str,
    success_count: int,
    test_cases: list,
    verbose: bool,
):
    """Enhanced final results display with real test information"""
    console = get_console()

    console.print()
    console.print(f"[bold blue]Results: {suite_name} ‚Üí {server_name}[/bold blue]")
    console.print(f"Successful: [green]{success_count}[/green] / {len(test_cases)}")

    if success_count == len(test_cases):
        console.print("[green]‚úÖ All tests passed![/green]")
    else:
        failed = len(test_cases) - success_count
        console.print(f"[red]‚ùå {failed} test(s) failed[/red]")

        if verbose:
            console.print("\n[bold]Test Details:[/bold]")
            for result in results:
                status_icon = "‚úÖ" if result["success"] else "‚ùå"
                console.print(
                    f"  {status_icon} {result['test_id']}: {result['status']}"
                )
                if not result["success"] and "error" in result:
                    console.print(f"    Error: {result['error']}")
