# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from __future__ import annotations

from typing import Dict

import httpx

from ..types import (
    DatasetSortField,
    dataset_list_params,
    dataset_create_params,
    dataset_update_params,
)
from .._types import Body, Omit, Query, Headers, NotGiven, omit, not_given
from .._utils import maybe_transform, async_maybe_transform
from .._compat import cached_property
from .._resource import SyncAPIResource, AsyncAPIResource
from .._response import (
    to_raw_response_wrapper,
    to_streamed_response_wrapper,
    async_to_raw_response_wrapper,
    async_to_streamed_response_wrapper,
)
from ..pagination import SyncDefaultPagination, AsyncDefaultPagination
from .._base_client import AsyncPaginator, make_request_options
from ..types.dataset import Dataset
from ..types.dataset_sort_field import DatasetSortField
from ..types.dataset_filter_param import DatasetFilterParam
from ..types.dataset_search_param import DatasetSearchParam
from ..types.shared.delete_response import DeleteResponse
from ..types.shared_params.ownership import Ownership

__all__ = ["DatasetsResource", "AsyncDatasetsResource"]


class DatasetsResource(SyncAPIResource):
    @cached_property
    def with_raw_response(self) -> DatasetsResourceWithRawResponse:
        """
        This property can be used as a prefix for any HTTP method call to return
        the raw response object instead of the parsed content.

        For more information, see https://docs.nvidia.com/nemo/microservices/latest/pysdk/index.html#accessing-raw-response-data-e-g-headers
        """
        return DatasetsResourceWithRawResponse(self)

    @cached_property
    def with_streaming_response(self) -> DatasetsResourceWithStreamingResponse:
        """
        An alternative to `.with_raw_response` that doesn't eagerly read the response body.

        For more information, see https://docs.nvidia.com/nemo/microservices/latest/pysdk/index.html#with_streaming_response
        """
        return DatasetsResourceWithStreamingResponse(self)

    def create(
        self,
        *,
        files_url: str,
        custom_fields: Dict[str, object] | Omit = omit,
        description: str | Omit = omit,
        format: str | Omit = omit,
        hf_endpoint: str | Omit = omit,
        limit: int | Omit = omit,
        name: str | Omit = omit,
        namespace: str | Omit = omit,
        ownership: Ownership | Omit = omit,
        project: str | Omit = omit,
        split: str | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> Dataset:
        """
        Create a new dataset.

        Args:
          files_url: The location where the artifact files are stored. This can be a URL pointing to
              NDS, Hugging Face, S3, or any other accessible resource location.

          custom_fields: A set of custom fields that the user can define and use for various purposes.

          description: The description of the entity.

          format: Specifies the dataset format, referring to the schema of the dataset rather than
              the file format. Examples include SQuAD, BEIR, etc.

          hf_endpoint: For HuggingFace URLs, the endpoint that should be used. By default, this is set
              to the Data Store URL. For HuggingFace Hub, this should be set to
              "https://huggingface.co".

          limit: The maximum number of items to be used from the dataset.

          name: The name of the entity. Must be unique inside the namespace. If not specified,
              it will be the same as the automatically generated id.

          namespace: The namespace of the entity. This can be missing for namespace entities or in
              deployments that don't use namespaces.

          ownership: Information about ownership of an entity.

              If the entity is a namespace, the `access_policies` will typically apply to all
              entities inside the namespace.

          project: The URN of the project associated with this entity.

          split: The split of the dataset. Examples include train, validation, test, etc.

          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        return self._post(
            "/v1/datasets",
            body=maybe_transform(
                {
                    "files_url": files_url,
                    "custom_fields": custom_fields,
                    "description": description,
                    "format": format,
                    "hf_endpoint": hf_endpoint,
                    "limit": limit,
                    "name": name,
                    "namespace": namespace,
                    "ownership": ownership,
                    "project": project,
                    "split": split,
                },
                dataset_create_params.DatasetCreateParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Dataset,
        )

    def retrieve(
        self,
        dataset_name: str,
        *,
        namespace: str,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> Dataset:
        """
        Get Dataset

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not namespace:
            raise ValueError(f"Expected a non-empty value for `namespace` but received {namespace!r}")
        if not dataset_name:
            raise ValueError(f"Expected a non-empty value for `dataset_name` but received {dataset_name!r}")
        return self._get(
            f"/v1/datasets/{namespace}/{dataset_name}",
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Dataset,
        )

    def update(
        self,
        dataset_name: str,
        *,
        namespace: str,
        custom_fields: Dict[str, object] | Omit = omit,
        description: str | Omit = omit,
        files_url: str | Omit = omit,
        format: str | Omit = omit,
        hf_endpoint: str | Omit = omit,
        limit: int | Omit = omit,
        ownership: Ownership | Omit = omit,
        project: str | Omit = omit,
        split: str | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> Dataset:
        """Update dataset metadata.

        If the request body has an empty field, keep the old
        value.

        Args:
          custom_fields: A set of custom fields that the user can define and use for various purposes.

          description: The description of the entity.

          files_url: The location where the artifact files are stored. This can be a URL pointing to
              NDS, Hugging Face, S3, or any other accessible resource location.

          format: Specifies the dataset format, referring to the schema of the dataset rather than
              the file format. Examples include SQuAD, BEIR, etc.

          hf_endpoint: For HuggingFace URLs, the endpoint that should be used. By default, this is set
              to the Data Store URL. For HuggingFace Hub, this should be set to
              "https://huggingface.co".

          limit: The maximum number of items to be used from the dataset.

          ownership: Information about ownership of an entity.

              If the entity is a namespace, the `access_policies` will typically apply to all
              entities inside the namespace.

          project: The URN of the project associated with this entity.

          split: The split of the dataset. Examples include train, validation, test, etc.

          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not namespace:
            raise ValueError(f"Expected a non-empty value for `namespace` but received {namespace!r}")
        if not dataset_name:
            raise ValueError(f"Expected a non-empty value for `dataset_name` but received {dataset_name!r}")
        return self._patch(
            f"/v1/datasets/{namespace}/{dataset_name}",
            body=maybe_transform(
                {
                    "custom_fields": custom_fields,
                    "description": description,
                    "files_url": files_url,
                    "format": format,
                    "hf_endpoint": hf_endpoint,
                    "limit": limit,
                    "ownership": ownership,
                    "project": project,
                    "split": split,
                },
                dataset_update_params.DatasetUpdateParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Dataset,
        )

    def list(
        self,
        *,
        filter: DatasetFilterParam | Omit = omit,
        page: int | Omit = omit,
        page_size: int | Omit = omit,
        search: DatasetSearchParam | Omit = omit,
        sort: DatasetSortField | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> SyncDefaultPagination[Dataset]:
        """
        List all datasets.

        Args:
          filter: Filter configs on various criteria.

          page: Page number.

          page_size: Page size.

          search: Search datasets using substring matching. You can combine multiple search fields
              and filters.

              For example:

              - `?search[name]=imagenet`: searches all datasets with 'imagenet' in the name.
              - `?search[format]=csv`: searches all datasets with 'csv' in the format.
              - `?search[split]=train`: searches all datasets with 'train' in the split.
              - `?search[namespace]=research`: searches all datasets with 'research' in the
                namespace.
              - `?search[name]=imagenet&search[split]=validation`: searches all datasets with
                'imagenet' in the name AND 'validation' in the split.
              - `?search[name]=imagenet&search[name]=coco`: searches all datasets with
                'imagenet' OR 'coco' in the name.
              - `?search[updated_at][start]=2024-01-01T00:00:00` finds all datasets updated on
                or after the start date
              - `?search[created_at][start]=2022-01-01&search[updated_at][end]=2024-01-01`
                finds all datasets created from start date up to and including end date

          sort: The field to sort by. To sort in decreasing order, use `-` in front of the field
              name.

          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        return self._get_api_list(
            "/v1/datasets",
            page=SyncDefaultPagination[Dataset],
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                query=maybe_transform(
                    {
                        "filter": filter,
                        "page": page,
                        "page_size": page_size,
                        "search": search,
                        "sort": sort,
                    },
                    dataset_list_params.DatasetListParams,
                ),
            ),
            model=Dataset,
        )

    def delete(
        self,
        dataset_name: str,
        *,
        namespace: str,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> DeleteResponse:
        """
        Delete Dataset

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not namespace:
            raise ValueError(f"Expected a non-empty value for `namespace` but received {namespace!r}")
        if not dataset_name:
            raise ValueError(f"Expected a non-empty value for `dataset_name` but received {dataset_name!r}")
        return self._delete(
            f"/v1/datasets/{namespace}/{dataset_name}",
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=DeleteResponse,
        )


class AsyncDatasetsResource(AsyncAPIResource):
    @cached_property
    def with_raw_response(self) -> AsyncDatasetsResourceWithRawResponse:
        """
        This property can be used as a prefix for any HTTP method call to return
        the raw response object instead of the parsed content.

        For more information, see https://docs.nvidia.com/nemo/microservices/latest/pysdk/index.html#accessing-raw-response-data-e-g-headers
        """
        return AsyncDatasetsResourceWithRawResponse(self)

    @cached_property
    def with_streaming_response(self) -> AsyncDatasetsResourceWithStreamingResponse:
        """
        An alternative to `.with_raw_response` that doesn't eagerly read the response body.

        For more information, see https://docs.nvidia.com/nemo/microservices/latest/pysdk/index.html#with_streaming_response
        """
        return AsyncDatasetsResourceWithStreamingResponse(self)

    async def create(
        self,
        *,
        files_url: str,
        custom_fields: Dict[str, object] | Omit = omit,
        description: str | Omit = omit,
        format: str | Omit = omit,
        hf_endpoint: str | Omit = omit,
        limit: int | Omit = omit,
        name: str | Omit = omit,
        namespace: str | Omit = omit,
        ownership: Ownership | Omit = omit,
        project: str | Omit = omit,
        split: str | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> Dataset:
        """
        Create a new dataset.

        Args:
          files_url: The location where the artifact files are stored. This can be a URL pointing to
              NDS, Hugging Face, S3, or any other accessible resource location.

          custom_fields: A set of custom fields that the user can define and use for various purposes.

          description: The description of the entity.

          format: Specifies the dataset format, referring to the schema of the dataset rather than
              the file format. Examples include SQuAD, BEIR, etc.

          hf_endpoint: For HuggingFace URLs, the endpoint that should be used. By default, this is set
              to the Data Store URL. For HuggingFace Hub, this should be set to
              "https://huggingface.co".

          limit: The maximum number of items to be used from the dataset.

          name: The name of the entity. Must be unique inside the namespace. If not specified,
              it will be the same as the automatically generated id.

          namespace: The namespace of the entity. This can be missing for namespace entities or in
              deployments that don't use namespaces.

          ownership: Information about ownership of an entity.

              If the entity is a namespace, the `access_policies` will typically apply to all
              entities inside the namespace.

          project: The URN of the project associated with this entity.

          split: The split of the dataset. Examples include train, validation, test, etc.

          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        return await self._post(
            "/v1/datasets",
            body=await async_maybe_transform(
                {
                    "files_url": files_url,
                    "custom_fields": custom_fields,
                    "description": description,
                    "format": format,
                    "hf_endpoint": hf_endpoint,
                    "limit": limit,
                    "name": name,
                    "namespace": namespace,
                    "ownership": ownership,
                    "project": project,
                    "split": split,
                },
                dataset_create_params.DatasetCreateParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Dataset,
        )

    async def retrieve(
        self,
        dataset_name: str,
        *,
        namespace: str,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> Dataset:
        """
        Get Dataset

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not namespace:
            raise ValueError(f"Expected a non-empty value for `namespace` but received {namespace!r}")
        if not dataset_name:
            raise ValueError(f"Expected a non-empty value for `dataset_name` but received {dataset_name!r}")
        return await self._get(
            f"/v1/datasets/{namespace}/{dataset_name}",
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Dataset,
        )

    async def update(
        self,
        dataset_name: str,
        *,
        namespace: str,
        custom_fields: Dict[str, object] | Omit = omit,
        description: str | Omit = omit,
        files_url: str | Omit = omit,
        format: str | Omit = omit,
        hf_endpoint: str | Omit = omit,
        limit: int | Omit = omit,
        ownership: Ownership | Omit = omit,
        project: str | Omit = omit,
        split: str | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> Dataset:
        """Update dataset metadata.

        If the request body has an empty field, keep the old
        value.

        Args:
          custom_fields: A set of custom fields that the user can define and use for various purposes.

          description: The description of the entity.

          files_url: The location where the artifact files are stored. This can be a URL pointing to
              NDS, Hugging Face, S3, or any other accessible resource location.

          format: Specifies the dataset format, referring to the schema of the dataset rather than
              the file format. Examples include SQuAD, BEIR, etc.

          hf_endpoint: For HuggingFace URLs, the endpoint that should be used. By default, this is set
              to the Data Store URL. For HuggingFace Hub, this should be set to
              "https://huggingface.co".

          limit: The maximum number of items to be used from the dataset.

          ownership: Information about ownership of an entity.

              If the entity is a namespace, the `access_policies` will typically apply to all
              entities inside the namespace.

          project: The URN of the project associated with this entity.

          split: The split of the dataset. Examples include train, validation, test, etc.

          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not namespace:
            raise ValueError(f"Expected a non-empty value for `namespace` but received {namespace!r}")
        if not dataset_name:
            raise ValueError(f"Expected a non-empty value for `dataset_name` but received {dataset_name!r}")
        return await self._patch(
            f"/v1/datasets/{namespace}/{dataset_name}",
            body=await async_maybe_transform(
                {
                    "custom_fields": custom_fields,
                    "description": description,
                    "files_url": files_url,
                    "format": format,
                    "hf_endpoint": hf_endpoint,
                    "limit": limit,
                    "ownership": ownership,
                    "project": project,
                    "split": split,
                },
                dataset_update_params.DatasetUpdateParams,
            ),
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=Dataset,
        )

    def list(
        self,
        *,
        filter: DatasetFilterParam | Omit = omit,
        page: int | Omit = omit,
        page_size: int | Omit = omit,
        search: DatasetSearchParam | Omit = omit,
        sort: DatasetSortField | Omit = omit,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> AsyncPaginator[Dataset, AsyncDefaultPagination[Dataset]]:
        """
        List all datasets.

        Args:
          filter: Filter configs on various criteria.

          page: Page number.

          page_size: Page size.

          search: Search datasets using substring matching. You can combine multiple search fields
              and filters.

              For example:

              - `?search[name]=imagenet`: searches all datasets with 'imagenet' in the name.
              - `?search[format]=csv`: searches all datasets with 'csv' in the format.
              - `?search[split]=train`: searches all datasets with 'train' in the split.
              - `?search[namespace]=research`: searches all datasets with 'research' in the
                namespace.
              - `?search[name]=imagenet&search[split]=validation`: searches all datasets with
                'imagenet' in the name AND 'validation' in the split.
              - `?search[name]=imagenet&search[name]=coco`: searches all datasets with
                'imagenet' OR 'coco' in the name.
              - `?search[updated_at][start]=2024-01-01T00:00:00` finds all datasets updated on
                or after the start date
              - `?search[created_at][start]=2022-01-01&search[updated_at][end]=2024-01-01`
                finds all datasets created from start date up to and including end date

          sort: The field to sort by. To sort in decreasing order, use `-` in front of the field
              name.

          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        return self._get_api_list(
            "/v1/datasets",
            page=AsyncDefaultPagination[Dataset],
            options=make_request_options(
                extra_headers=extra_headers,
                extra_query=extra_query,
                extra_body=extra_body,
                timeout=timeout,
                query=maybe_transform(
                    {
                        "filter": filter,
                        "page": page,
                        "page_size": page_size,
                        "search": search,
                        "sort": sort,
                    },
                    dataset_list_params.DatasetListParams,
                ),
            ),
            model=Dataset,
        )

    async def delete(
        self,
        dataset_name: str,
        *,
        namespace: str,
        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.
        # The extra values given here take precedence over values defined on the client or passed to this method.
        extra_headers: Headers | None = None,
        extra_query: Query | None = None,
        extra_body: Body | None = None,
        timeout: float | httpx.Timeout | None | NotGiven = not_given,
    ) -> DeleteResponse:
        """
        Delete Dataset

        Args:
          extra_headers: Send extra headers

          extra_query: Add additional query parameters to the request

          extra_body: Add additional JSON properties to the request

          timeout: Override the client-level default timeout for this request, in seconds
        """
        if not namespace:
            raise ValueError(f"Expected a non-empty value for `namespace` but received {namespace!r}")
        if not dataset_name:
            raise ValueError(f"Expected a non-empty value for `dataset_name` but received {dataset_name!r}")
        return await self._delete(
            f"/v1/datasets/{namespace}/{dataset_name}",
            options=make_request_options(
                extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout
            ),
            cast_to=DeleteResponse,
        )


class DatasetsResourceWithRawResponse:
    def __init__(self, datasets: DatasetsResource) -> None:
        self._datasets = datasets

        self.create = to_raw_response_wrapper(
            datasets.create,
        )
        self.retrieve = to_raw_response_wrapper(
            datasets.retrieve,
        )
        self.update = to_raw_response_wrapper(
            datasets.update,
        )
        self.list = to_raw_response_wrapper(
            datasets.list,
        )
        self.delete = to_raw_response_wrapper(
            datasets.delete,
        )


class AsyncDatasetsResourceWithRawResponse:
    def __init__(self, datasets: AsyncDatasetsResource) -> None:
        self._datasets = datasets

        self.create = async_to_raw_response_wrapper(
            datasets.create,
        )
        self.retrieve = async_to_raw_response_wrapper(
            datasets.retrieve,
        )
        self.update = async_to_raw_response_wrapper(
            datasets.update,
        )
        self.list = async_to_raw_response_wrapper(
            datasets.list,
        )
        self.delete = async_to_raw_response_wrapper(
            datasets.delete,
        )


class DatasetsResourceWithStreamingResponse:
    def __init__(self, datasets: DatasetsResource) -> None:
        self._datasets = datasets

        self.create = to_streamed_response_wrapper(
            datasets.create,
        )
        self.retrieve = to_streamed_response_wrapper(
            datasets.retrieve,
        )
        self.update = to_streamed_response_wrapper(
            datasets.update,
        )
        self.list = to_streamed_response_wrapper(
            datasets.list,
        )
        self.delete = to_streamed_response_wrapper(
            datasets.delete,
        )


class AsyncDatasetsResourceWithStreamingResponse:
    def __init__(self, datasets: AsyncDatasetsResource) -> None:
        self._datasets = datasets

        self.create = async_to_streamed_response_wrapper(
            datasets.create,
        )
        self.retrieve = async_to_streamed_response_wrapper(
            datasets.retrieve,
        )
        self.update = async_to_streamed_response_wrapper(
            datasets.update,
        )
        self.list = async_to_streamed_response_wrapper(
            datasets.list,
        )
        self.delete = async_to_streamed_response_wrapper(
            datasets.delete,
        )
