Metadata-Version: 2.1
Name: Vidar
Version: 1.0.2
Summary: AI observability platform for LLM applications - Python SDK
Home-page: https://github.com/tanmaysharma2001/vidar-python-sdk
Author: Vidar Team
Author-email: support@vidar.io
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: System :: Monitoring
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: requests>=2.25.0
Requires-Dist: typing-extensions>=4.0.0
Requires-Dist: opentelemetry-api>=1.20.0
Requires-Dist: opentelemetry-sdk>=1.20.0
Requires-Dist: opentelemetry-exporter-otlp-proto-grpc>=1.20.0
Requires-Dist: opentelemetry-instrumentation>=0.41b0
Requires-Dist: opentelemetry-instrumentation-openai>=0.20.0
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.3.0; extra == "anthropic"
Provides-Extra: async
Requires-Dist: httpx>=0.23.0; extra == "async"
Requires-Dist: aiohttp>=3.8.0; extra == "async"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.10.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: pre-commit>=2.20.0; extra == "dev"
Provides-Extra: full
Requires-Dist: httpx>=0.23.0; extra == "full"
Requires-Dist: aiohttp>=3.8.0; extra == "full"
Requires-Dist: openai>=1.0.0; extra == "full"
Requires-Dist: anthropic>=0.3.0; extra == "full"
Provides-Extra: openai
Requires-Dist: openai>=1.0.0; extra == "openai"

# Vidar Python SDK

AI observability platform for LLM applications with automatic tracing.

## Installation

```bash
pip install vidar
```

For full functionality including async support and LLM client integrations:

```bash
pip install vidar[full]
```

## Quick Start

```python
import vidar

# Initialize once (like Laminar)
vidar.initialize(api_key="your-api-key")

# All LLM calls are now automatically traced!
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
# â†‘ This is automatically traced with cost, latency, tokens, etc.
```

## Features

- **Zero-code tracing**: Just initialize and go
- **Automatic cost tracking**: Real-time cost calculation
- **Performance monitoring**: Latency, token usage, error rates  
- **Session correlation**: Group related LLM calls
- **Multi-provider support**: OpenAI, Anthropic, and more
- **Async support**: Works with httpx, aiohttp
- **Framework integration**: FastAPI, Django, Flask examples

## Usage Examples

### Manual Tracing
```python
@vidar.trace("My AI Function")
def process_text(text):
    vidar.add_attribute("input_length", len(text))
    
    # Your LLM calls here
    result = client.chat.completions.create(...)
    
    vidar.add_attribute("output_length", len(result))
    return result
```

### Session Tracking
```python
with vidar.trace("Document Analysis"):
    vidar.add_attribute("session_id", "session_123")
    
    # Multiple related LLM calls
    summary = summarize_document(doc)
    questions = generate_questions(doc)
    answers = answer_questions(questions, doc)
```

### Error Handling
```python
try:
    response = client.chat.completions.create(...)
except Exception as e:
    vidar.set_error(e)
    raise
```

## Direct Proxy Usage

You can also use Vidar as a proxy without the SDK:

```python
import requests

response = requests.post(
    "http://localhost:8080/proxy/openai/v1/chat/completions",
    headers={"Authorization": "Bearer your-vidar-api-key"},
    json={"model": "gpt-4", "messages": [...]}
)
```

## Configuration

```python
vidar.initialize(
    api_key="your-api-key",
    base_url="http://localhost:8080",  # Vidar server
    project_id="my-project", 
    timeout=30.0
)
```

## Web Framework Integration

### FastAPI
```python
from fastapi import FastAPI
import vidar

app = FastAPI()

@app.on_event("startup")
async def startup():
    vidar.initialize(api_key="your-key")

@app.post("/chat")
async def chat(message: str):
    with vidar.trace("Chat API"):
        # Your LLM logic here
        return {"response": "..."}
```

### Django
```python
# settings.py
MIDDLEWARE = [
    'myapp.middleware.VidarMiddleware',
    # ... other middleware
]

# middleware.py
import vidar

class VidarMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response
        vidar.initialize(api_key="your-key")
    
    def __call__(self, request):
        with vidar.trace(f"{request.method} {request.path}"):
            return self.get_response(request)
```

## Documentation

- [Full Documentation](https://docs.vidar.io)
- [API Reference](https://docs.vidar.io/api)
- [Examples](https://github.com/tanmaysharma2001/vidar/tree/main/examples)

## License

MIT License
