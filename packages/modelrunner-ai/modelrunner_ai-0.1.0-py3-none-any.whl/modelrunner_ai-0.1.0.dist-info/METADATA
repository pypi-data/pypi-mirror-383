Metadata-Version: 2.4
Name: modelrunner-ai
Version: 0.1.0
Summary: Python client for modelrunner.ai
Author: Features & Labels <support@modelrunner.ai>
Project-URL: homepage, https://modelrunner.ai
Project-URL: repository, https://github.com/modelrunner/modelrunner
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: httpx<1,>=0.21.0
Requires-Dist: httpx-sse<0.5,>=0.4.0
Provides-Extra: docs
Requires-Dist: sphinx; extra == "docs"
Requires-Dist: sphinx-rtd-theme; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints; extra == "docs"
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pytest-asyncio; extra == "test"
Requires-Dist: pytest-mock; extra == "test"
Requires-Dist: pillow; extra == "test"
Provides-Extra: dev
Requires-Dist: modelrunner_ai[docs,test]; extra == "dev"

# modelrunner.ai Python client

This is a Python client library for interacting with ML models deployed on [modelrunner.ai](https://modelrunner.ai).

## Getting started

To install the client, run:

```bash
pip install modelrunner-ai
```

To use the client, you need to have an API key. You can get one by signing up at [modelrunner.ai](https://modelrunner.ai). Once you have it, set
it as an environment variable:

```bash
export MODELRUNNER_KEY=your-api-key
```

Now you can use the client to interact with your models. Here's an example of how to use it:


```python
import asyncio
import modelrunner_ai

async def main():
    response = await modelrunner_ai.submit_async("bytedance/sdxl-lightning-4step", arguments={"prompt": "two friends cooking together"})

    logs_index = 0
    async for event in response.iter_events(with_logs=True):
        if isinstance(event, modelrunner_ai.Queued):
            print("Queued. Position:", event.position)
        elif isinstance(event, (modelrunner_ai.InProgress, modelrunner_ai.Completed)):
            new_logs = event.logs[logs_index:]
            for log in new_logs:
                print(log["message"])
            logs_index = len(event.logs)

    result = await response.get()
    print(result["output"])


asyncio.run(main())
```

## Uploading files

If the model requires files as input, you can upload them directly to media.modelrunner.ai (our CDN) and pass the URLs to the client. Here's an example:

```python
import modelrunner_ai

input_image = modelrunner_ai.upload_file("./image.jpg")
print(input_image)
response = modelrunner_ai.run("swook/inspyrenet", arguments={"image_path": input_image})
print(response["output"])
```


