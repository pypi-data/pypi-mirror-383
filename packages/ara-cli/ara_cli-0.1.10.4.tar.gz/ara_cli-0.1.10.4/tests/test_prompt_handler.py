import pytest
import os
import shutil
import base64
import re
from unittest.mock import patch, MagicMock, mock_open, call
from pathlib import Path

from ara_cli import prompt_handler
from ara_cli.ara_config import ARAconfig, LLMConfigItem, ConfigManager
from ara_cli.classifier import Classifier

from langfuse.api.resources.commons.errors import NotFoundError


@pytest.fixture(autouse=True)
def mock_langfuse():
    """Mock Langfuse client to prevent network calls during tests."""
    with patch.object(prompt_handler.LLMSingleton, 'langfuse', None):
        mock_langfuse_instance = MagicMock()
        
        # Mock the get_prompt method to raise NotFoundError (simulating prompt not found)
        mock_langfuse_instance.get_prompt.side_effect = NotFoundError(
            # status_code=404, 
            body={'message': "Prompt not found", 'error': 'LangfuseNotFoundError'}
        )
        
        # Mock the span context manager
        mock_span = MagicMock()
        mock_span.__enter__ = MagicMock(return_value=mock_span)
        mock_span.__exit__ = MagicMock(return_value=None)
        mock_langfuse_instance.start_as_current_span.return_value = mock_span
        
        with patch.object(prompt_handler.LLMSingleton, 'langfuse', mock_langfuse_instance):
            yield mock_langfuse_instance


@pytest.fixture
def mock_config():
    """Mocks a standard ARAconfig object for testing."""
    config = ARAconfig(
        ext_code_dirs=[{"code": "./src"}],
        glossary_dir="./glossary",
        doc_dir="./docs",
        local_prompt_templates_dir="./ara/.araconfig/custom-prompt-modules",
        ara_prompt_given_list_includes=["*.py", "*.md"],
        llm_config={
            "gpt-4o": LLMConfigItem(provider="openai", model="openai/gpt-4o", temperature=0.8, max_tokens=1024),
            "o3-mini": LLMConfigItem(provider="openai", model="openai/o3-mini", temperature=0.9, max_tokens=2048),
        },
        default_llm="gpt-4o",
        extraction_llm="o3-mini"
    )
    return config


@pytest.fixture(autouse=True)
def mock_config_manager(mock_config):
    """Patches ConfigManager to ensure it always returns the mock_config."""
    with patch.object(ConfigManager, 'get_config') as mock_get_config:
        mock_get_config.return_value = mock_config
        yield mock_get_config


@pytest.fixture(autouse=True)
def reset_singleton():
    """Resets the LLMSingleton and ConfigManager before each test for isolation."""
    prompt_handler.LLMSingleton._instance = None
    prompt_handler.LLMSingleton._default_model = None
    prompt_handler.LLMSingleton._extraction_model = None
    ConfigManager.reset()
    yield
    ConfigManager.reset()


class TestLLMSingleton:
    """Tests the behavior of the LLMSingleton class."""

    def test_get_instance_creates_with_default_model(self, mock_config_manager):
        instance = prompt_handler.LLMSingleton.get_instance()
        assert instance is not None
        assert prompt_handler.LLMSingleton.get_default_model() == "gpt-4o"
        assert prompt_handler.LLMSingleton.get_extraction_model() == "o3-mini"
        assert instance.default_config_params['temperature'] == 0.8
        assert instance.extraction_config_params['temperature'] == 0.9

    def test_get_instance_creates_with_first_model_if_no_default(self, mock_config_manager, mock_config):
        mock_config.default_llm = None
        instance = prompt_handler.LLMSingleton.get_instance()
        assert instance is not None
        assert prompt_handler.LLMSingleton.get_default_model() == "gpt-4o"

    def test_get_instance_no_extraction_llm_falls_back_to_default(self, mock_config_manager, mock_config):
        mock_config.extraction_llm = None
        instance = prompt_handler.LLMSingleton.get_instance()
        assert instance is not None
        assert prompt_handler.LLMSingleton.get_extraction_model() == "gpt-4o"

    def test_get_instance_no_llm_config_raises_error(self, mock_config_manager, mock_config):
        mock_config.llm_config = {}
        mock_config.default_llm = None  # This is crucial to hit the correct check
        with pytest.raises(ValueError, match="No LLM configurations are defined in the configuration file."):
            prompt_handler.LLMSingleton.get_instance()

    def test_get_instance_constructor_raises_for_missing_extraction_config(self, mock_config_manager, mock_config):
        mock_config.extraction_llm = "missing-model"
        with pytest.raises(ValueError, match="No configuration found for the extraction model: missing-model"):
            prompt_handler.LLMSingleton.get_instance()

    def test_get_instance_returns_same_instance(self, mock_config_manager):
        instance1 = prompt_handler.LLMSingleton.get_instance()
        instance2 = prompt_handler.LLMSingleton.get_instance()
        assert instance1 is instance2

    def test_get_config_by_purpose(self, mock_config_manager):
        default_params = prompt_handler.LLMSingleton.get_config_by_purpose('default')
        extraction_params = prompt_handler.LLMSingleton.get_config_by_purpose('extraction')
        assert default_params['model'] == 'openai/gpt-4o'
        assert extraction_params['model'] == 'openai/o3-mini'

    def test_set_default_model_switches_model(self, mock_config_manager):
        initial_instance = prompt_handler.LLMSingleton.get_instance()
        assert prompt_handler.LLMSingleton.get_default_model() == "gpt-4o"
        
        new_instance = prompt_handler.LLMSingleton.set_default_model("o3-mini")

        assert prompt_handler.LLMSingleton.get_default_model() == "o3-mini"
        assert new_instance.default_config_params['temperature'] == 0.9
        assert initial_instance is not new_instance

    def test_set_default_model_to_same_model_does_nothing(self, mock_config_manager):
        instance1 = prompt_handler.LLMSingleton.get_instance()
        instance2 = prompt_handler.LLMSingleton.set_default_model("gpt-4o")
        assert instance1 is instance2

    def test_set_default_model_to_invalid_raises_error(self, mock_config_manager):
        with pytest.raises(ValueError, match="No configuration found for the default model: invalid-model"):
            prompt_handler.LLMSingleton.set_default_model("invalid-model")

    def test_set_extraction_model_switches_model(self, mock_config_manager):
        initial_instance = prompt_handler.LLMSingleton.get_instance()
        new_instance = prompt_handler.LLMSingleton.set_extraction_model("gpt-4o")
        assert prompt_handler.LLMSingleton.get_extraction_model() == "gpt-4o"
        assert new_instance.extraction_config_params['temperature'] == 0.8
        assert initial_instance is not new_instance

    def test_set_extraction_model_to_same_model_does_nothing(self, mock_config_manager):
        instance1 = prompt_handler.LLMSingleton.get_instance()
        instance2 = prompt_handler.LLMSingleton.set_extraction_model("o3-mini")
        assert instance1 is instance2

    def test_set_extraction_model_to_invalid_raises_error(self, mock_config_manager):
        with pytest.raises(ValueError, match="No configuration found for the extraction model: invalid-model"):
            prompt_handler.LLMSingleton.set_extraction_model("invalid-model")

    def test_get_default_model_initializes_if_needed(self, mock_config_manager):
        assert prompt_handler.LLMSingleton._instance is None
        model = prompt_handler.LLMSingleton.get_default_model()
        assert model == "gpt-4o"
        assert prompt_handler.LLMSingleton._instance is not None

    def test_get_extraction_model_initializes_if_needed(self, mock_config_manager):
        assert prompt_handler.LLMSingleton._instance is None
        model = prompt_handler.LLMSingleton.get_extraction_model()
        assert model == "o3-mini"
        assert prompt_handler.LLMSingleton._instance is not None


class TestFileIO:
    """Tests file I/O helper functions."""

    def test_write_and_read_string_from_file(self, tmp_path):
        file_path = tmp_path / "test.txt"
        test_string = "Hello World"
        
        prompt_handler.write_string_to_file(file_path, test_string, 'w')
        
        content = prompt_handler.read_string_from_file(file_path)
        assert test_string in content
        
        content_get = prompt_handler.get_file_content(file_path)
        assert content.strip() == test_string

    def test_get_partial_file_content(self, tmp_path):
        file_path = tmp_path / "test.txt"
        file_path.write_text("\n".join(f"Line {i}" for i in range(1, 21)))

        content = prompt_handler.get_partial_file_content(str(file_path), "2:4,18:19")
        expected = "Line 2\nLine 3\nLine 4\nLine 18\nLine 19\n"
        assert content == expected


class TestCoreLogic:
    """Tests functions related to the main business logic."""
    
    @pytest.fixture(autouse=True)
    def setup_test_env(self, tmp_path):
        """Changes CWD to a temporary directory for test isolation."""
        original_cwd = os.getcwd()
        os.chdir(tmp_path)
        yield
        os.chdir(original_cwd)

    @pytest.mark.parametrize("message, expected", [
        ({"content": "Hello"}, True),
        ({"content": "  "}, False),
        ({"content": ""}, False),
        ({"content": "\n\t"}, False),
        ({"content": [{"type": "text", "text": " "}]}, False),
        ({"content": [{"type": "text", "text": "Valid text"}]}, True),
        ({"content": [{"type": "image_url"}, {"type": "text", "text": "More text"}]}, True),
        ({"content": []}, False),
        ({"content": 123}, False),
        ({}, False),
    ])
    def test_is_valid_message(self, message, expected):
        assert prompt_handler._is_valid_message(message) == expected

    @patch('ara_cli.prompt_handler.litellm.completion')
    def test_send_prompt(self, mock_completion, mock_config, mock_config_manager):
        """Tests that send_prompt uses the default LLM by default."""
        mock_chunk = MagicMock()
        mock_chunk.choices[0].delta.content = "test chunk"
        mock_completion.return_value = [mock_chunk]

        prompt = [{"role": "user", "content": "A test"}]
        
        result = list(prompt_handler.send_prompt(prompt))

        expected_params = mock_config.llm_config['gpt-4o'].model_dump(exclude_none=True)
        del expected_params['provider']

        mock_completion.assert_called_once_with(
            messages=prompt, stream=True, **expected_params
        )
        assert len(result) == 1
        assert result[0].choices[0].delta.content == "test chunk"

    @patch('ara_cli.prompt_handler.litellm.completion')
    def test_send_prompt_filters_invalid_messages(self, mock_completion, mock_config_manager):
        prompt = [
            {"role": "user", "content": "Valid message"},
            {"role": "user", "content": " "},
            {"role": "assistant", "content": "Another valid one"},
        ]
        valid_prompt = [prompt[0], prompt[2]]

        list(prompt_handler.send_prompt(prompt))

        mock_completion.assert_called_once()
        called_args = mock_completion.call_args[1]
        assert called_args['messages'] == valid_prompt

    @patch('ara_cli.prompt_handler.litellm.completion')
    def test_send_prompt_uses_extraction_llm(self, mock_completion, mock_config, mock_config_manager):
        """Tests that send_prompt uses the extraction LLM when specified."""
        mock_completion.return_value = []
        prompt = [{"role": "user", "content": "Extract this"}]
        
        list(prompt_handler.send_prompt(prompt, purpose='extraction'))

        expected_params = mock_config.llm_config['o3-mini'].model_dump(exclude_none=True)
        del expected_params['provider']

        mock_completion.assert_called_once_with(
            messages=prompt, stream=True, **expected_params
        )

    @patch('ara_cli.prompt_handler.send_prompt')
    def test_describe_image(self, mock_send_prompt, tmp_path, mock_langfuse):
        fake_image_path = tmp_path / "test.jpeg"
        fake_image_content = b"fakeimagedata"
        fake_image_path.write_bytes(fake_image_content)
        
        mock_send_prompt.return_value = iter([])
        
        # Ensure the langfuse mock is properly set up for this instance
        instance = prompt_handler.LLMSingleton.get_instance()
        instance.langfuse = mock_langfuse
        
        prompt_handler.describe_image(fake_image_path)
        
        mock_send_prompt.assert_called_once()
        called_args, called_kwargs = mock_send_prompt.call_args
        
        assert called_kwargs == {'purpose': 'extraction'}
        message_content = called_args[0][0]['content']
        assert message_content[0]['type'] == 'text'
        assert message_content[1]['type'] == 'image_url'
        
        encoded_image = base64.b64encode(fake_image_content).decode('utf-8')
        expected_url = f"data:image/jpeg;base64,{encoded_image}"
        assert message_content[1]['image_url']['url'] == expected_url

    @patch('ara_cli.prompt_handler.send_prompt')
    def test_describe_image_returns_response_text(self, mock_send_prompt, tmp_path, mock_langfuse):
        fake_image_path = tmp_path / "test.gif"
        fake_image_path.touch()

        mock_chunk1 = MagicMock()
        mock_chunk1.choices[0].delta.content = "This is "
        mock_chunk2 = MagicMock()
        mock_chunk2.choices[0].delta.content = "a description."
        mock_chunk3 = MagicMock()
        mock_chunk3.choices[0].delta.content = None # Test empty chunk
        mock_send_prompt.return_value = iter([mock_chunk1, mock_chunk3, mock_chunk2])

        # Ensure the langfuse mock is properly set up for this instance
        instance = prompt_handler.LLMSingleton.get_instance()
        instance.langfuse = mock_langfuse

        description = prompt_handler.describe_image(fake_image_path)
        assert description == "This is a description."

    @patch('ara_cli.prompt_handler.Classifier.get_sub_directory', return_value="test_classifier")
    def test_append_headings(self, mock_get_sub, tmp_path):
        os.makedirs("ara/test_classifier/my_param.data", exist_ok=True)
        log_file = tmp_path / "ara/test_classifier/my_param.data/test_classifier.prompt_log.md"
        log_file.touch()

        prompt_handler.append_headings("test_classifier", "my_param", "PROMPT")
        assert "## PROMPT_1" in log_file.read_text()
        
        prompt_handler.append_headings("test_classifier", "my_param", "PROMPT")
        assert "## PROMPT_2" in log_file.read_text()

    @patch('ara_cli.prompt_handler.Classifier.get_sub_directory', return_value="test_classifier")
    def test_append_headings_creates_file_if_not_exists(self, mock_get_sub, tmp_path):
        os.makedirs("ara/test_classifier/my_param.data", exist_ok=True)
        log_file = tmp_path / "ara/test_classifier/my_param.data/test_classifier.prompt_log.md"
        assert not log_file.exists()

        prompt_handler.append_headings("test_classifier", "my_param", "HEADING")
        assert log_file.exists()
        assert "## HEADING_1" in log_file.read_text()

    @patch('ara_cli.prompt_handler.Classifier.get_sub_directory', return_value="test_classifier")
    def test_write_prompt_result(self, mock_get_sub, tmp_path):
        os.makedirs("ara/test_classifier/my_param.data", exist_ok=True)
        log_file = tmp_path / "ara/test_classifier/my_param.data/test_classifier.prompt_log.md"

        prompt_handler.write_prompt_result("test_classifier", "my_param", "Test content")
        assert "Test content" in log_file.read_text()

    def test_prepend_system_prompt(self, mock_langfuse):
        # Ensure the langfuse mock is properly set up for this instance
        instance = prompt_handler.LLMSingleton.get_instance()
        instance.langfuse = mock_langfuse
        
        messages = [{"role": "user", "content": "Hi"}]
        result = prompt_handler.prepend_system_prompt(messages)
        assert len(result) == 2
        assert result[0]['role'] == 'system'
        assert result[1]['role'] == 'user'

    @patch('logging.getLogger')
    def test_append_images_to_message_logic(self, mock_get_logger):
        # Test case 1: No images, should return original message
        message_no_img = {"role": "user", "content": "Hello"}
        result = prompt_handler.append_images_to_message(message_no_img, [])
        assert result == {"role": "user", "content": "Hello"}

        # Test case 2: Add images to a text-only message
        message_with_text = {"role": "user", "content": "Describe these."}
        images = [{"type": "image_url", "image_url": {"url": "data:..."}}]
        result = prompt_handler.append_images_to_message(message_with_text, images)
        expected_content = [
            {"type": "text", "text": "Describe these."},
            {"type": "image_url", "image_url": {"url": "data:..."}}
        ]
        assert result["content"] == expected_content
        
        # Test case 3: Add images to an existing list content
        message_with_list = {"role": "user", "content": [{"type": "text", "text": "Initial text."}]}
        result = prompt_handler.append_images_to_message(message_with_list, images)
        expected_content_2 = [
            {"type": "text", "text": "Initial text."},
            {"type": "image_url", "image_url": {"url": "data:..."}}
        ]
        assert result["content"] == expected_content_2


class TestFileOperations:
    """Tests for complex file operations and parsing."""

    @pytest.fixture(autouse=True)
    def setup_fs(self, tmp_path):
        self.root = tmp_path
        os.chdir(self.root)
        yield

    def test_write_template_files_to_config(self):
        base_path = self.root / "templates"
        (base_path / "rules").mkdir(parents=True)
        (base_path / "rules" / "b.rules.md").touch()
        (base_path / "rules" / "a.rules.md").touch()
        
        m = mock_open()
        with patch('builtins.open', m):
            prompt_handler.write_template_files_to_config("rules", m(), str(base_path))

        # Check that files were written in sorted order with correct spacing
        calls = m().write.call_args_list
        assert calls[0] == call("  - [] rules/a.rules.md\n")
        assert calls[1] == call("  - [] rules/b.rules.md\n")

    def test_find_files_with_endings(self):
        (self.root / "a.rules.md").touch()
        (self.root / "b.intention.md").touch()
        (self.root / "c.rules.md").touch()
        (self.root / "d.other.md").touch()
        (self.root / "subdir").mkdir()
        (self.root / "subdir" / "e.rules.md").touch()

        endings = [".intention.md", ".rules.md"]
        files = prompt_handler.find_files_with_endings(str(self.root), endings)

        # Should only find files in the root, not subdir, and sorted by ending order
        # Sort results to make test independent of filesystem list order
        assert sorted(files) == sorted(["b.intention.md", "a.rules.md", "c.rules.md"])

    def test_move_and_copy_files(self):
        prompt_data = self.root / "prompt.data"
        prompt_archive = self.root / "prompt.archive"
        source_dir = self.root / "source"
        prompt_data.mkdir()
        prompt_archive.mkdir()
        source_dir.mkdir()

        source_file = source_dir / "new.rules.md"
        source_file.write_text("new rules")
        
        existing_file = prompt_data / "old.rules.md"
        existing_file.write_text("old rules")

        unrelated_source = source_dir / "unrelated.txt"
        unrelated_source.touch()
        
        missing_source = source_dir / "nonexistent.rules.md"
        
        with patch('builtins.print') as mock_print:
            # Test move and copy
            prompt_handler.move_and_copy_files(str(source_file), str(prompt_data), str(prompt_archive))
            assert not existing_file.exists()
            assert (prompt_archive / "old.rules.md").exists()
            assert (prompt_data / "new.rules.md").read_text() == "new rules"
            
            # Test skipping unrelated files
            prompt_handler.move_and_copy_files(str(unrelated_source), str(prompt_data), str(prompt_archive))
            assert mock_print.call_args_list[-1] == call("File name unrelated.txt does not end with one of the specified patterns, skipping move and copy.")
            
            # Test warning for missing source
            prompt_handler.move_and_copy_files(str(missing_source), str(prompt_data), str(prompt_archive))
            assert mock_print.call_args_list[-1] == call(f"WARNING: template {missing_source} does not exist.")

    def test_extract_and_load_markdown_files_complex_hierarchy(self):
        md_content = """
# L1
- [x] l1.md
## L2-A
- [x] l2a.md
### L3
- [] l3_unchecked.md
- [x] l3.md
## L2-B
- [x] l2b.md
# L1-Again
- [x] l1_again.md
"""
        m = mock_open(read_data=md_content)
        with patch('builtins.open', m):
            paths = prompt_handler.extract_and_load_markdown_files("dummy_path")
        
        expected = [
            'L1/l1.md',
            'L1/L2-A/l2a.md',
            'L1/L2-A/L3/l3.md',
            'L1/L2-B/l2b.md',
            'L1-Again/l1_again.md',
        ]
        assert paths == expected

    @patch('ara_cli.prompt_handler.get_partial_file_content')
    @patch('ara_cli.prompt_handler.get_file_content')
    def test_load_givens(self, mock_get_content, mock_get_partial, tmp_path):
        # Setup files
        md_config = tmp_path / "config.givens.md"
        text_file = tmp_path / "file.txt"
        image_file = tmp_path / "image.png"
        
        text_file.write_text("Full content")
        image_file.write_bytes(b"imagedata")
        
        md_content = f"""
# src
- [x] {text_file}
- [x] [1:2] {text_file}
# assets
- [x] {image_file}
"""
        md_config.write_text(md_content)

        # Mocks
        mock_get_content.return_value = "Full content"
        mock_get_partial.return_value = "Partial content"
        
        # Execute
        with patch('ara_cli.prompt_handler.extract_and_load_markdown_files', return_value=[str(text_file), f"[1:2] {text_file}", str(image_file)]):
             # The regex in load_givens is flawed, so we manually mock the extracted items
            match = re.match(r".*?\[(\d+:\d+(?:,\s*\d+:\d+)*)\]\s+(.+)", f"[1:2] {text_file}")
            assert match is not None

            content, image_data = prompt_handler.load_givens(str(md_config))

        # Assertions
        assert "Full content" in content
        assert "Partial content" in content
        mock_get_content.assert_called_once_with(str(text_file))
        mock_get_partial.assert_called_once_with(str(text_file), "1:2")

        assert len(image_data) == 1
        assert image_data[0]['type'] == 'image_url'
        encoded = base64.b64encode(b"imagedata").decode("utf-8")
        assert encoded in image_data[0]['image_url']['url']
        assert f"![{image_file}](data:image/png;base64,{encoded})" in content

    @patch('ara_cli.prompt_handler.load_givens')
    @patch('ara_cli.prompt_handler.get_file_content')
    @patch('ara_cli.prompt_handler.find_files_with_endings')
    def test_collect_file_content_by_extension(self, mock_find, mock_get, mock_load):
        prompt_data_path = "/fake/path"
        mock_find.side_effect = [["rules.rules.md"], ["givens.prompt_givens.md"]]
        mock_get.return_value = "Rules content"
        mock_load.return_value = ("Givens content", ["image_data"])
        
        extensions = [".rules.md", ".prompt_givens.md"]
        content, images = prompt_handler.collect_file_content_by_extension(prompt_data_path, extensions)

        mock_find.assert_has_calls([call(prompt_data_path, [ext]) for ext in extensions])
        mock_get.assert_called_once_with(os.path.join(prompt_data_path, "rules.rules.md"))
        mock_load.assert_called_once_with(os.path.join(prompt_data_path, "givens.prompt_givens.md"))
        
        assert "Rules content" in content
        assert "Givens content" in content
        assert images == ["image_data"]


class TestArtefactAndTemplateHandling:
    """Tests functions that manage artefact and template files."""

    @pytest.fixture(autouse=True)
    def setup_fs(self, tmp_path):
        self.root = tmp_path
        os.chdir(self.root)
        self.mock_classifier = "my_artefact"
        self.mock_param = "my_param"
        
        self.classifier_patch = patch('ara_cli.prompt_handler.Classifier.get_sub_directory', return_value=self.mock_classifier)
        self.mock_get_sub_dir = self.classifier_patch.start()
        
        yield
        
        self.classifier_patch.stop()

    def test_prompt_data_directory_creation(self):
        path = prompt_handler.prompt_data_directory_creation(self.mock_classifier, self.mock_param)
        expected_path = self.root / "ara" / self.mock_classifier / f"{self.mock_param}.data" / "prompt.data"
        assert os.path.exists(expected_path)
        assert Path(path).resolve() == expected_path.resolve()

    @patch('ara_cli.prompt_handler.generate_markdown_listing')
    @patch('ara_cli.prompt_handler.ArtefactCreator')
    def test_initialize_prompt_templates(self, mock_artefact_creator, mock_generate_listing, mock_config_manager):
        # This side effect creates the file that the function expects to read
        def create_dummy_file(*args, **kwargs):
            file_path = args[2]
            Path(file_path).parent.mkdir(parents=True, exist_ok=True)
            Path(file_path).touch()

        mock_generate_listing.side_effect = create_dummy_file
        
        prompt_handler.initialize_prompt_templates(self.mock_classifier, self.mock_param)
        
        prompt_data_path = self.root / "ara" / self.mock_classifier / f"{self.mock_param}.data" / "prompt.data"
        prompt_log_path = prompt_data_path.parent

        mock_artefact_creator.return_value.create_artefact_prompt_files.assert_called_once()
        assert mock_generate_listing.call_count == 2


    @patch('ara_cli.prompt_handler.generate_markdown_listing')
    def test_generate_config_prompt_template_file(self, mock_generate_listing, mock_config_manager):
        prompt_data_path = "prompt/data"
        with patch('ara_cli.prompt_handler.TemplatePathManager.get_template_base_path', return_value="/global/templates"):
            prompt_handler.generate_config_prompt_template_file(prompt_data_path, "config.md")

        mock_generate_listing.assert_called_once()
        args, _ = mock_generate_listing.call_args
        assert any("custom-prompt-modules" in d for d in args[0])
        assert any("prompt-modules" in d for d in args[0])
        assert "*.blueprint.md" in args[1]
        assert args[2] == os.path.join(prompt_data_path, "config.md")

    @patch('ara_cli.prompt_handler.generate_markdown_listing')
    def test_generate_config_prompt_givens_file(self, mock_generate_listing, mock_config_manager):
        prompt_data_path = prompt_handler.prompt_data_directory_creation(self.mock_classifier, self.mock_param)
        
        prompt_handler.generate_config_prompt_givens_file(prompt_data_path, "config.givens.md")
        
        mock_generate_listing.assert_called_once()
        args, _ = mock_generate_listing.call_args
        assert "ara" in args[0]
        assert "./src" in args[0]
        assert args[1] == ["*.py", "*.md"]
        assert args[2] == os.path.join(prompt_data_path, "config.givens.md")

    @patch('ara_cli.prompt_handler.generate_markdown_listing')
    def test_generate_config_prompt_givens_file_marks_artefact(self, mock_generate_listing, mock_config_manager):
        prompt_data_path = Path(prompt_handler.prompt_data_directory_creation(self.mock_classifier, self.mock_param))
        config_path = prompt_data_path / "config.givens.md"
        artefact_to_mark = "file.py"

        def create_fake_file(*args, **kwargs):
            content = f"- [] some_other_file.txt\n- [] {artefact_to_mark}\n"
            with open(args[2], 'w') as f:
                f.write(content)

        mock_generate_listing.side_effect = create_fake_file

        prompt_handler.generate_config_prompt_givens_file(
            str(prompt_data_path), "config.givens.md", artefact_to_mark=artefact_to_mark
        )
        
        content = config_path.read_text()
        assert f"- [x] {artefact_to_mark}" in content
        assert f"- [] some_other_file.txt" in content

    @patch('ara_cli.prompt_handler.extract_and_load_markdown_files')
    @patch('ara_cli.prompt_handler.move_and_copy_files')
    @patch('ara_cli.prompt_handler.TemplatePathManager.get_template_base_path', return_value="/global/templates")
    def test_load_selected_prompt_templates(self, mock_base_path, mock_move, mock_extract, mock_config_manager):
        prompt_data_path = prompt_handler.prompt_data_directory_creation(self.mock_classifier, self.mock_param)
        config_file = Path(prompt_data_path) / "config.prompt_templates.md"
        config_file.touch()

        mock_extract.return_value = [
            "custom-prompt-modules/my_custom.rules.md",
            "prompt-modules/global.intention.md",
            "unrecognized/file.md"
        ]
        
        with patch('builtins.print') as mock_print:
            prompt_handler.load_selected_prompt_templates(self.mock_classifier, self.mock_param)

        archive_path = os.path.join(prompt_data_path, "prompt.archive")

        assert mock_move.call_count == 2
        mock_print.assert_any_call("WARNING: Unrecognized template type for item unrecognized/file.md.")

    def test_load_selected_prompt_templates_no_config_file_warns_and_returns(self):
        prompt_handler.prompt_data_directory_creation(self.mock_classifier, self.mock_param)
        
        with patch('builtins.print') as mock_print:
            prompt_handler.load_selected_prompt_templates(self.mock_classifier, self.mock_param)
        
        mock_print.assert_called_once_with("WARNING: config.prompt_templates.md does not exist.")

    @patch('ara_cli.prompt_handler.send_prompt')
    @patch('ara_cli.prompt_handler.collect_file_content_by_extension')
    @patch('ara_cli.prompt_handler.append_images_to_message', side_effect=lambda msg, img: msg) # Passthrough
    def test_create_and_send_custom_prompt_handles_empty_chunks(self, mock_append, mock_collect, mock_send, tmp_path):
        # Create the directory structure the function expects
        prompt_data_path = Path(f"ara/{self.mock_classifier}/{self.mock_param}.data/prompt.data")
        prompt_data_path.mkdir(parents=True, exist_ok=True)

        mock_collect.return_value = ("Test Content", [])
        
        mock_chunk_ok = MagicMock()
        mock_chunk_ok.choices[0].delta.content = "response"
        mock_chunk_empty = MagicMock()
        mock_chunk_empty.choices[0].delta.content = None
        mock_send.return_value = iter([mock_chunk_empty, mock_chunk_ok])

        log_file = tmp_path / "ara" / self.mock_classifier / f"{self.mock_param}.data" / f"{self.mock_classifier}.prompt_log.md"
        log_file.touch()

        prompt_handler.create_and_send_custom_prompt(self.mock_classifier, self.mock_param)

        log_content = log_file.read_text()
        assert "response" in log_content
        assert "None" not in log_content

    @patch('ara_cli.prompt_handler.send_prompt')
    @patch('ara_cli.prompt_handler.collect_file_content_by_extension')
    @patch('ara_cli.prompt_handler.append_images_to_message')
    def test_create_and_send_custom_prompt(self, mock_append_images, mock_collect, mock_send, mock_config_manager):
        prompt_handler.prompt_data_directory_creation(self.mock_classifier, self.mock_param)

        mock_collect.return_value = ("### GIVENS\ncontent", [{"type": "image_url"}])
        
        # append_images_to_message returns a single dict, not a list of dicts.
        returned_message_dict = {'role': 'user', 'content': ['### GIVENS\ncontent', {'type': 'image_url'}]}
        mock_append_images.return_value = returned_message_dict

        mock_send.return_value = iter([MagicMock(choices=[MagicMock(delta=MagicMock(content="llm response"))])])

        prompt_handler.create_and_send_custom_prompt(self.mock_classifier, self.mock_param)
        
        mock_collect.assert_called_once()

        # Assert that append_images_to_message was called with a single dict (the bug fix)
        mock_append_images.assert_called_once_with(
            {'role': 'user', 'content': '### GIVENS\ncontent'},
            [{'type': 'image_url'}]
        )

        # Assert that send_prompt was called with a list containing the dict returned from append_images_to_message
        mock_send.assert_called_once_with([returned_message_dict])

        log_file = self.root / "ara" / self.mock_classifier / f"{self.mock_param}.data" / f"{self.mock_classifier}.prompt_log.md"
        assert "llm response" in log_file.read_text()

    @patch('ara_cli.global_file_lister.generate_global_markdown_listing')
    def test_generate_config_prompt_global_givens_file(self, mock_global_lister, mock_config_manager, mock_config):
        """Tests that the global givens file is generated correctly when global_dirs are present."""
        prompt_data_path = self.root / "prompt/data"
        prompt_data_path.mkdir(parents=True)
        
        # Scenario 1: No global_dirs are configured, should return early and do nothing.
        mock_config.global_dirs = []
        prompt_handler.generate_config_prompt_global_givens_file(str(prompt_data_path), "global.md")
        mock_global_lister.assert_not_called()
        
        # Scenario 2: With global_dirs, should call the global lister with correct arguments.
        mock_config.global_dirs = [{"source_dir": "/global/src1"}, {"path": "/global/src2"}]
        mock_config.ara_prompt_given_list_includes = ["*.py", "*.md"]
        
        # Use patch to suppress print output during the test
        with patch('builtins.print'):
            prompt_handler.generate_config_prompt_global_givens_file(str(prompt_data_path), "global.md")
        
        mock_global_lister.assert_called_once()
        args, _ = mock_global_lister.call_args
        assert args[0] == ["/global/src1", "/global/src2"]
        assert args[1] == ["*.py", "*.md"]
        assert args[2] == os.path.join(prompt_data_path, "global.md")