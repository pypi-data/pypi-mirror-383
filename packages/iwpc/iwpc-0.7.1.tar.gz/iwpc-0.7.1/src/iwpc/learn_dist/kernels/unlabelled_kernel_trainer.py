from typing import Tuple, Optional

import torch
from lightning import LightningModule
from torch import optim, Tensor
from torch.nn import Module
from torch.nn.functional import logsigmoid
from torch.optim import Optimizer

from iwpc.learn_dist.kernels.trainable_kernel_base import TrainableKernelBase


class KernelKLDivergenceGradientLoss:
    """
    Given a data distribution p and a model q, the gradient of this loss w.r.t. model parameters is equal to the
    gradient of the negative log-probability of the observed data within the model. So minimizing this loss is
    equivalent to maximising the probability of the data with respect to the model parameters.

    The loss is only valid if the model q is assumed to take the form of a fixed base distribution convolved with a
    trainable kernel
    """
    def __call__(
        self,
        cond: Tensor,
        kernel: TrainableKernelBase,
        log_p_over_q_model: Module,
        weights: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Parameters
        ----------
        cond
            Samples from the q base distribution
        kernel
            The TrainableKernelBase used to produce q
        log_p_over_q_model
            A model that provides an estimate of log(p(x) / q(x)) likely obtained by training a classifier
        weights
            An optional array of sample weights

        Returns
        -------
        Tensor
            The scalar loss
        """
        weights = torch.ones(cond.shape[0], dtype=torch.float32, device=cond.device) if weights is None else weights
        samples, log_prob = kernel.draw_with_log_prob(cond)

        with torch.no_grad():
            p_over_q = torch.exp(log_p_over_q_model(samples))[:, 0]

        return -(weights * log_prob * p_over_q).mean()


class UnLabelledKernelTrainer(LightningModule):
    def __init__(
        self,
        kernel: TrainableKernelBase,
        log_p_over_q_model,
        start_kernel_training_epoch: int = 10
    ):
        """
        A LightningModule that trains a TrainableKernelBase to maximise the probability of data samples within a model
        generated by convolving a base-distribution with a TrainableKernelBase. Only samples from the base-distribution
        are required, the probability distribution itself is not required

        Parameters
        ----------
        kernel
            The TrainableKernelBase to train
        log_p_over_q_model
            A classifier model trained to learn the probability ratio of a given sample originating from p or from q
        start_kernel_training_epoch
            The epoch at which to start training the kernel. Generally useful to give the log_p_over_q_model a head start
        """
        super().__init__()
        self.kernel = kernel
        self.log_p_over_q_model = log_p_over_q_model
        self.loss = KernelKLDivergenceGradientLoss()
        self.start_kernel_training_epoch = start_kernel_training_epoch
        self.automatic_optimization = False
        self.register_buffer('log_two', torch.log(torch.tensor(2.)))

    def calculate_cross_entropy(self, batch: Tuple[Tensor, Tensor, Tensor]) -> Tensor:
        """
        Calculates the binary cross entropy loss of the predictions made by self.log_p_over_q_model classifying between
        p and q

        Parameters
        ----------
        batch
            The conditioning information, samples, and weights in the batch

        Returns
        -------
        Tensor
            The binary cross entropy loss of self.log_p_over_q_model
        """
        cond, samples, weights = batch
        labels, cond = cond[:, 0], cond[:, 1:]
        mask = labels == 1
        p = samples[~mask]
        q = self.kernel.draw(cond[mask])

        return (
            logsigmoid(self.log_p_over_q_model(p)).mean()
            + logsigmoid(-self.log_p_over_q_model(q)).mean()
        ) / 2

    def calculate_kernel_loss(self, batch: Tuple[Tensor, Tensor, Tensor]) -> Tensor:
        """
        Calculates the kernel loss given the learned values of self.log_p_over_q_model

        Parameters
        ----------
        batch

        Returns
        -------
        Tensor
            The loss of the kernel
        """
        cond, samples, weights = batch
        labels, cond = cond[:, 0], cond[:, 1:]
        mask = labels == 1
        return self.loss(cond[mask], self.kernel, self.log_p_over_q_model, weights[mask])

    def training_step(self, batch: Tuple[Tensor, Tensor, Tensor]) -> None:
        """
        Optimizes log_p_over_q_model and the parameters in self.kernel to maximise the probability of the p samples
        in q. Logs the current learned divergence between p and q
        """
        discriminator_optimizer, kernel_optimizer = self.optimizers()

        if self.current_epoch > self.start_kernel_training_epoch:
            kernel_loss = self.calculate_kernel_loss(batch)
            kernel_optimizer.zero_grad()
            self.log('train_kernel_loss', kernel_loss, on_step=True, on_epoch=True, prog_bar=False)
            kernel_optimizer.step()

        bce = self.calculate_cross_entropy(batch)
        self.log('train_divergence', 1 - bce / self.log_two, on_step=True, on_epoch=True, prog_bar=True)
        discriminator_optimizer.zero_grad()
        bce.backward()
        discriminator_optimizer.step()

    def validation_step(self, batch: Tuple[Tensor, Tensor, Tensor]) -> None:
        """
        Calculates the validation learned divergence between p and q
        """
        bce = self.calculate_cross_entropy(batch)
        self.log('val_divergence', 1 - bce / self.log_two, on_step=False, on_epoch=True, prog_bar=True)

    def configure_optimizers(self) -> Tuple[Optimizer, Optimizer]:
        """
        Returns
        -------
        Tuple[Optimizer, Optimizer]
            The classifier's and kernel's optimizer
        """
        discriminator_optimizer = optim.Adam(self.log_p_over_q_model.parameters(), lr=1e-3)
        kernel_optimizer = optim.Adam(self.kernel.parameters(), lr=1e-4)
        return discriminator_optimizer, kernel_optimizer
