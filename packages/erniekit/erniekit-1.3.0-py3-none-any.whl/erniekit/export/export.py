# Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import shutil
import time
from typing import Any, Optional
import json

import paddle
from paddleformers.mergekit import MergeConfig, MergeModel
from paddleformers.trainer import get_last_checkpoint
from paddleformers.utils.download import resolve_file_path
from paddleformers.utils.env import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME
from paddleformers.utils.log import logger
from paddleformers import __version__ as paddleformers_version

from ernie.utils.download_utils import check_download_repo

from ..hparams import get_export_args, read_args
from ..utils.process import is_valid_model_dir


def logger_merge_config(merge_config, lora_merge):
    """
    Logs the merge configuration details to debug output, with different formatting
    for LoRA merges versus standard model merges.

    Args:
        merge_config (object): Configuration object containing merge parameters.
                              Expected to have attributes accessible via __dict__.
        lora_merge (bool): Flag indicating whether this is a LoRA merge operation.
                           When True, logs only LoRA-specific parameters.
                           When False, logs standard merge parameters.

    Outputs:
        Writes formatted configuration details to the logger at DEBUG level.
        For LoRA merges: Displays centered "LoRA Merge Info" header and specific paths.
        For standard merges: Displays centered "Mergekit Config Info" header and all
        parameters except excluded ones.
    """
    if lora_merge:
        logger.debug("{:^40}".format("LoRA Merge Info"))
        for k, v in merge_config.__dict__.items():
            if k in ["lora_model_path", "base_model_path"]:
                logger.debug(f"{k:30}: {v}")
    else:
        logger.debug("{:^40}".format("Mergekit Config Info"))
        for k, v in merge_config.__dict__.items():
            if k in ["model_path_str", "device", "tensor_type", "merge_preifx"]:
                continue
            logger.debug(f"{k:30}: {v}")


def run_export(args: Optional[dict[str, Any]] = None) -> None:
    """_summary_

    Args:
        args (Optional[dict[str, Any]], optional): _description_. Defaults to None.
    """

    args = read_args(args)
    model_args, data_args, generating_args, finetuning_args, export_args = (
        get_export_args(args)
    )

    paddle.set_device(finetuning_args.device)

    last_checkpoint = None
    if os.path.isdir(finetuning_args.output_dir):
        # Check if the output directory is a valid model directory (contains .safetensors or .pdparams files)
        if is_valid_model_dir(finetuning_args.output_dir):
            last_checkpoint = finetuning_args.output_dir
        # If not a model directory but still a valid path, try to find the latest checkpoint
        else:
            last_checkpoint = get_last_checkpoint(finetuning_args.output_dir)
    if last_checkpoint is not None:
        logger.info(f"Starting model export from checkpoint: {last_checkpoint}")
    else:
        raise FileNotFoundError(
            f"No valid checkpoint found in: {finetuning_args.output_dir}"
        )

    if model_args.lora:
        start = time.time()
        logger.info("***** Start merging LoRA model *****")

        model_args.model_name_or_path = check_download_repo(
            model_args.model_name_or_path,
            download_hub=model_args.download_hub,
        )

        try:
            from paddleformers.utils.download import (
                DownloadSource,
            )  # test if paddleformers is the newest
        except Exception:
            DownloadSource = None

        download_source_kwargs = {}
        if DownloadSource is None:
            if model_args.download_hub == "huggingface":
                download_source_kwargs["from_hf_hub"] = True
            elif model_args.download_hub == "aistudio":
                download_source_kwargs["from_aistudio"] = True
            elif model_args.download_hub == "modelscope":
                download_source_kwargs["from_modelscope"] = True
        else:
            download_source_kwargs["download_hub"] = model_args.download_hub

        resolve_result = resolve_file_path(
            model_args.model_name_or_path,
            [SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME],
            **download_source_kwargs,
        )

        convert_from_hf = False
        save_to_hf = False
        if resolve_result is not None:
            resolve_path = os.path.dirname(resolve_result)
            config_json = os.path.join(resolve_path, "config.json")
            with open(config_json) as f:
                config_dict = json.load(f)
            if "torch_dtype" in config_dict:
                convert_from_hf = True
                save_to_hf = True
            logger.info(f"base model path parsed:{resolve_path}")
        else:
            logger.error(f"{model_args.model_name_or_path} does not found.")

        config = {}
        config["base_model_path"] = resolve_path
        config["lora_model_path"] = last_checkpoint
        config["output_path"] = os.path.join(finetuning_args.output_dir, "export")
        if paddleformers_version >= "0.3":
            config["convert_from_hf"] = convert_from_hf
            config["save_to_hf"] = save_to_hf

        if export_args.copy_tokenizer:
            if convert_from_hf:
                config["copy_file_list"] = [
                    "added_tokens.json",
                    "chat_template.jinja",
                    "tokenizer.json",
                    "tokenizer.model",
                    "tokenizer_config.json",
                    "special_tokens_map.json",
                ]
            else:
                config["copy_file_list"] = [
                    "tokenizer.model",
                    "tokenizer_config.json",
                    "special_tokens_map.json",
                    # "config.json",
                ]

        merge_config = MergeConfig(**config)
        mergekit = MergeModel(merge_config)
        logger_merge_config(merge_config, model_args.lora)
        mergekit.merge_model()
        src_file = os.path.join(config["base_model_path"], "config.json")
        dst_file = os.path.join(config["output_path"], "config.json")
        if os.path.isfile(src_file):
            shutil.copy2(src_file, dst_file)
        else:
            logger.debug(
                f'Copy failed: "config.json" not found in {config["base_model_path"]}'
            )
        src_file = os.path.join(config["base_model_path"], "preprocessor_config.json")
        dst_file = os.path.join(config["output_path"], "preprocessor_config.json")
        if os.path.isfile(src_file):
            shutil.copy2(src_file, dst_file)
        else:
            logger.debug(
                f'Copy failed: "preprocessor_config.json" not found in {config["base_model_path"]}'
            )
        logger.info(
            f"***** Successfully finished merging LoRA model. Time cost: {time.time() - start} s *****"
        )
    else:
        raise ValueError(
            "Only support merge lora checkpoint, but get model_args.lora is False."
        )
