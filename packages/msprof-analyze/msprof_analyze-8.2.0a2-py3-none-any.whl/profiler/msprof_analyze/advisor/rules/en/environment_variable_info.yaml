ASCEND_GLOBAL_LOG_LEVEL:
  desc: "log level: 0-debug, 1-info, 2-warning, 3-error.\nDefault is error level."
  suggest: "Debug or info level may lead to training performance degradation,\n
            recommended setting error level by execute command 'export ASCEND_GLOBAL_LOG_LEVEL=3."
HCCL_RDAM_TC:
  desc: "Configure the DSCP value of RoCE packets sent by the network port.\n
         In the DS field of IP datagram header, the rightmost 6 bits are DSCP, and leftmost 2 bits are 0.\n
         It should be set to DSCP * 4. Default value is 132, that is, DSCP is 33 (132=33*4)."
  suggest: "Please refer to https://support.huawei.com/enterprise/zh/doc/EDOC1100371278/5eeeed85?idPath=23710424"
  suggest_html: "Please refer to <a href=https://support.huawei.com/enterprise/zh/doc/EDOC1100371278/5eeeed85?idPath=23710424 target='_blank'>LINK</a>"
HCCL_RDMA_SL:
  desc: "Specify the priority of the RDMA NIC.\n
         The value must be the same as the PFC priority for the NIC.\n
         Otherwise, the performance may deteriorate.\n
         The value range is [0, 7], and default value is 4."
  suggest: "Please refer to https://support.huawei.com/enterprise/zh/doc/EDOC1100371278/5eeeed85?idPath=23710424"
  suggest_html: "Please refer to <a href=https://support.huawei.com/enterprise/zh/doc/EDOC1100371278/5eeeed85?idPath=23710424 target='_blank'>LINK</a>"
ACLNN_CACHE_LIMIT:
  desc: "Number of cached aclnn operators."
  suggest: "Setting a large number when aclnn and host bound, such as 'export ACLNN_CACHE_LIMIT=100000'"
HOST_CACHE_CAPACITY:
  desc: "Enable dynamic shape cache.\n
         The default value is 0, indicating that the data cache is disabled.\n
         If it is set to a non-zero positive integer, for example, 10, the system caches the execution data of 10 inputs shapes that frequently occur recently.\n
         When the cached shapes appear again, the host execution performance will be improved, but the host memory usage increase.\n
         The specific increase is proportional to the value of the HOST_CACHE_CAPACITY and size of the model."
  suggest: "Setting a non-zero number, such as 'export HOST_CACHE_CAPACITY=20'"
PYTORCH_NPU_ALLOC_CONF:
  desc: "Controlling cache allocator behavior.\n
         The optional parameter is max_split_size_mb, garbage_collection_threshold and expandable_segments.\n
         1. max_split_size_mb:v--the memory block that is greater than v will be not split.\n
         2. garbage_collection_threshold:t--after the threshold is set, if the NPU memory usage exceed threshold, the cached allocator starts to reclaim memory block. The range of t is (0.0, 1.0).\n
         3. expandable_segments:True/False--The default value is False. If True, this setting instructs cache allocator to create specific memory blocks that can be expanded later to better handle frequent changed in memory usage."
  suggest: "export PYTORCH_NPU_ALLOC_CONF=expandable_segments:True"
ASCEND_LAUNCH_BLOCKING:
  desc: "Whether to enable the synchronization mode during operation execution.\n
         When set to 1, force the operator to run in synchronous mode, making it easier to debug and track down problems in the code.\n
         If the set to 0, the task is executed in asynchronous mode."
  suggest: "export ASCEND_LAUNCH_BLOCKING=0"
HCCL_ALGO:
  desc: "The environment variable is used to configure the cross-server communication algorithm. The options are as follows: ring, H-D_R, NHR, NHR_V1, NB, pipeline.\n
        When not setting, the algorithm is automatically seleted based on the chip, number of nodes and data volume."
  suggest: "Recommend unsetting HCCL_ALGO by execute command 'unset HCCL_ALGO'"