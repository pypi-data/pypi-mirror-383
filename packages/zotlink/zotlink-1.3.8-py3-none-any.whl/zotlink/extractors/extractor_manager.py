#!/usr/bin/env python3
"""
ğŸ”— ZotLink æå–å™¨ç®¡ç†å™¨

ç»Ÿä¸€ç®¡ç†ä¸åŒå­¦æœ¯æ•°æ®åº“çš„æå–å™¨ï¼Œç°åœ¨æ”¯æŒæµè§ˆå™¨é©±åŠ¨æ¨¡å¼
"""

import requests
import logging
import asyncio
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse

from .base_extractor import BaseExtractor
from .nature_extractor import NatureExtractor
from .cvf_extractor import CVFExtractor
from .generic_extractor import GenericOpenAccessExtractor
from .browser_extractor import BrowserExtractor, PLAYWRIGHT_AVAILABLE
from .biorxiv_direct_extractor import BioRxivDirectExtractor
from .preprint_extractor import PreprintExtractor

logger = logging.getLogger(__name__)

class ExtractorManager:
    """æå–å™¨ç®¡ç†å™¨ï¼Œæ”¯æŒHTTPå’Œæµè§ˆå™¨åŒæ¨¡å¼"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        self.session = requests.Session()
        self.extractors: List[BaseExtractor] = []
        self.cookies_store: Dict[str, str] = {}
        
        # æ³¨å†Œæ‰€æœ‰å¯ç”¨çš„æå–å™¨
        self._register_extractors()
        
        # æµè§ˆå™¨æ¨¡å¼è®¾ç½®
        self.browser_available = PLAYWRIGHT_AVAILABLE
        if self.browser_available:
            logger.info("âœ… æµè§ˆå™¨æ¨¡å¼å¯ç”¨")
        else:
            logger.warning("âš ï¸ æµè§ˆå™¨æ¨¡å¼ä¸å¯ç”¨ï¼Œéœ€è¦å®‰è£…Playwright")
    
    def _register_extractors(self):
        """æ³¨å†Œæ‰€æœ‰æå–å™¨"""
        try:
            # æ³¨å†Œä¸“ç”¨æå–å™¨ (ä¼˜å…ˆçº§é«˜)
            # bioRxivä¸“ç”¨æå–å™¨ - æœ€é«˜ä¼˜å…ˆçº§
            biorxiv_extractor = BioRxivDirectExtractor(self.session)
            self.extractors.append(biorxiv_extractor)
            logger.info("âœ… æ³¨å†ŒBioRxivä¸“ç”¨æå–å™¨")
            
            # medRxiv/chemRxivä¸“ç”¨æå–å™¨
            preprint_extractor = PreprintExtractor(self.session)
            self.extractors.append(preprint_extractor)
            logger.info("âœ… æ³¨å†ŒPreprintæå–å™¨ (medRxiv/chemRxiv)")
            
            nature_extractor = NatureExtractor(self.session)
            self.extractors.append(nature_extractor)
            logger.info("âœ… æ³¨å†ŒNatureæå–å™¨")
            
            cvf_extractor = CVFExtractor(self.session)
            self.extractors.append(cvf_extractor)
            logger.info("âœ… æ³¨å†ŒCVFæå–å™¨")
            
            # æ³¨å†Œé€šç”¨æå–å™¨ (ä½œä¸ºåå¤‡)
            generic_extractor = GenericOpenAccessExtractor(self.session)
            self.extractors.append(generic_extractor)
            logger.info("âœ… æ³¨å†Œé€šç”¨å¼€æºæå–å™¨")
            
            logger.info(f"ğŸ“Š æ€»å…±æ³¨å†Œäº† {len(self.extractors)} ä¸ªHTTPæå–å™¨")
            if PLAYWRIGHT_AVAILABLE:
                logger.info("ğŸŒ æµè§ˆå™¨æå–å™¨åŠ¨æ€å¯ç”¨")
            
        except Exception as e:
            logger.error(f"âŒ æ³¨å†Œæå–å™¨å¤±è´¥: {e}")
    
    def _should_use_browser(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼"""
        if not self.browser_available:
            return False
            
        domain = urlparse(url).netloc.lower()
        browser_domains = [
            'biorxiv.org',
            'medrxiv.org', 
            'chemrxiv.org',
            'psyarxiv.com',
            'osf.io',
            'socarxiv.org'
        ]
        
        for browser_domain in browser_domains:
            if browser_domain in domain:
                logger.info(f"ğŸŒ æ£€æµ‹åˆ°éœ€è¦æµè§ˆå™¨æ¨¡å¼çš„åŸŸå: {domain}")
                return True
        return False
    
    async def extract_metadata(self, url: str) -> Dict[str, Any]:
        """
        æå–è®ºæ–‡å…ƒæ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨æ¨¡å¼å¤„ç†åçˆ¬è™«ç½‘ç«™
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼
        if self._should_use_browser(url):
            return await self._extract_with_browser(url)
        
        # ä½¿ç”¨HTTPæå–å™¨
        return self._extract_with_http(url)
    
    async def _extract_with_browser(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨æµè§ˆå™¨æå–å™¨"""
        try:
            async with BrowserExtractor() as browser_extractor:
                metadata = await browser_extractor._async_extract_metadata(url)
                if metadata:
                    logger.info(f"ğŸŒ æµè§ˆå™¨æ¨¡å¼æˆåŠŸæå–å…ƒæ•°æ®: {url}")
                    metadata['extractor'] = 'Browser-Driven'
                    return metadata
                else:
                    logger.warning(f"âš ï¸ æµè§ˆå™¨æ¨¡å¼æå–å¤±è´¥ï¼Œå›é€€åˆ°HTTPæ¨¡å¼: {url}")
                    return self._extract_with_http(url)
        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨æ¨¡å¼å¼‚å¸¸ï¼Œå›é€€åˆ°HTTPæ¨¡å¼: {url} - {e}")
            return self._extract_with_http(url)
    
    def _extract_with_http(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨HTTPæå–å™¨"""
        extractor = self.get_extractor_for_url(url)
        
        if not extractor:
            return {
                'error': 'ä¸æ”¯æŒçš„URLæˆ–æ•°æ®åº“',
                'url': url,
                'supported_databases': [e.get_database_name() for e in self.extractors]
            }
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è®¤è¯
            if extractor.requires_authentication():
                database_name = extractor.get_database_name()
                database_key = database_name.lower()
                if database_key not in self.cookies_store:
                    return {
                        'error': f'{database_name}éœ€è¦è®¤è¯ï¼Œè¯·å…ˆè®¾ç½®cookies',
                        'database': database_name,
                        'requires_auth': True,
                        'url': url
                    }
                
                # è®¾ç½®cookies
                cookies = self.cookies_store[database_key]
                if not extractor.set_cookies(cookies):
                    return {
                        'error': f'è®¾ç½®{database_name} cookieså¤±è´¥',
                        'database': database_name,
                        'url': url
                    }
            
            # æ‰§è¡Œå…ƒæ•°æ®æå–
            metadata = extractor.extract_metadata(url)
            
            # æ·»åŠ æå–å™¨ä¿¡æ¯
            if 'error' not in metadata:
                metadata['extractor'] = extractor.get_database_name()
                metadata['authenticated'] = extractor.requires_authentication()
            
            logger.info(f"ğŸ“Š HTTPæ¨¡å¼æˆåŠŸæå–å…ƒæ•°æ®: {extractor.get_database_name()}")
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ HTTPæ¨¡å¼å…ƒæ•°æ®æå–å¼‚å¸¸: {e}")
            return {
                'error': f'æå–è¿‡ç¨‹å¼‚å¸¸: {e}',
                'database': extractor.get_database_name(),
                'url': url
            }
    
    def get_extractor_for_url(self, url: str) -> Optional[BaseExtractor]:
        """æ ¹æ®URLè·å–åˆé€‚çš„HTTPæå–å™¨"""
        for extractor in self.extractors:
            try:
                if extractor.can_handle(url):
                    logger.info(f"ğŸ¯ é€‰æ‹©HTTPæå–å™¨: {extractor.get_database_name()}")
                    return extractor
            except Exception as e:
                logger.warning(f"âš ï¸ æ£€æŸ¥æå–å™¨å¤±è´¥: {e}")
                continue
        
        return None
    
    def set_database_cookies(self, database_name: str, cookies: str) -> bool:
        """ä¸ºç‰¹å®šæ•°æ®åº“è®¾ç½®cookies"""
        try:
            database_key = database_name.lower()
            self.cookies_store[database_key] = cookies
            logger.info(f"âœ… ä¸º{database_name}å­˜å‚¨cookies")
            
            # ä¸ºç›¸åº”çš„æå–å™¨è®¾ç½®cookies
            for extractor in self.extractors:
                if extractor.get_database_name().lower() == database_name.lower():
                    return extractor.set_cookies(cookies)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®{database_name} cookieså¤±è´¥: {e}")
            return False
    
    def get_supported_databases(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ”¯æŒçš„æ•°æ®åº“ä¿¡æ¯"""
        databases = []
        
        # HTTPæå–å™¨æ”¯æŒçš„æ•°æ®åº“
        for extractor in self.extractors:
            try:
                extractor_name = extractor.get_database_name()
                database_info = {
                    'name': extractor_name,
                    'requires_auth': extractor.requires_authentication(),
                    'has_cookies': extractor_name.lower() in self.cookies_store,
                    'supported_types': extractor.get_supported_item_types(),
                    'mode': 'HTTP'
                }
                databases.append(database_info)
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æå–å™¨ä¿¡æ¯å¤±è´¥: {e}")
        
        # æµè§ˆå™¨æ¨¡å¼æ”¯æŒçš„æ•°æ®åº“
        if self.browser_available:
            for domain, info in BrowserExtractor.BROWSER_REQUIRED_DOMAINS.items():
                database_info = {
                    'name': info['source'],
                    'requires_auth': False,
                    'has_cookies': False,
                    'supported_types': [info['itemType']],
                    'mode': 'Browser',
                    'domain': domain
                }
                databases.append(database_info)
        
        return databases
    
    def get_supported_domains(self) -> Dict[str, str]:
        """è·å–æ”¯æŒçš„åŸŸååˆ—è¡¨"""
        domains = {}
        
        # HTTPæå–å™¨æ”¯æŒçš„åŸŸå
        for extractor in self.extractors:
            if hasattr(extractor, 'OPEN_ACCESS_PATTERNS'):
                for pattern, info in extractor.OPEN_ACCESS_PATTERNS.items():
                    domains[pattern] = f"{extractor.__class__.__name__} (HTTP)"
            elif hasattr(extractor, 'SUPPORTED_DOMAINS'):
                for domain in extractor.SUPPORTED_DOMAINS:
                    domains[domain] = f"{extractor.__class__.__name__} (HTTP)"
        
        # æµè§ˆå™¨æå–å™¨æ”¯æŒçš„åŸŸå
        if self.browser_available:
            for domain, info in BrowserExtractor.BROWSER_REQUIRED_DOMAINS.items():
                domains[domain] = f"BrowserExtractor (æµè§ˆå™¨æ¨¡å¼)"
        
        return domains
    
    def test_database_access(self, database_name: str) -> Dict:
        """æµ‹è¯•ç‰¹å®šæ•°æ®åº“çš„è®¿é—®çŠ¶æ€"""
        for extractor in self.extractors:
            if extractor.get_database_name().lower() == database_name.lower():
                try:
                    # å¦‚æœéœ€è¦è®¤è¯ï¼Œå…ˆè®¾ç½®cookies
                    if extractor.requires_authentication():
                        database_key = database_name.lower()
                        if database_key not in self.cookies_store:
                            return {
                                'database': database_name,
                                'status': 'no_cookies',
                                'message': 'éœ€è¦å…ˆè®¾ç½®cookies'
                            }
                        
                        extractor.set_cookies(self.cookies_store[database_key])
                    
                    # æµ‹è¯•è®¿é—®
                    has_access = extractor.test_access()
                    
                    return {
                        'database': database_name,
                        'status': 'success' if has_access else 'access_denied',
                        'message': 'è®¿é—®æ­£å¸¸' if has_access else 'è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½éœ€è¦æ›´æ–°cookies'
                    }
                    
                except Exception as e:
                    return {
                        'database': database_name,
                        'status': 'error',
                        'message': f'æµ‹è¯•å¤±è´¥: {e}'
                    }
        
        return {
            'database': database_name,
            'status': 'not_supported',
            'message': 'ä¸æ”¯æŒçš„æ•°æ®åº“'
        }
