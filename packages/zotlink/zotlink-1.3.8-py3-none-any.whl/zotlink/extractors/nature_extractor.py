#!/usr/bin/env python3
"""
ğŸ”— ZotLink Nature æå–å™¨

ä¸“é—¨å¤„ç†Natureç³»åˆ—æœŸåˆŠçš„å­¦æœ¯è®ºæ–‡æå–
"""

import re
import requests
from bs4 import BeautifulSoup
from typing import Dict, Optional
import logging
from urllib.parse import urljoin, urlparse

from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class NatureExtractor(BaseExtractor):
    """NatureæœŸåˆŠæå–å™¨"""
    
    def can_handle(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºNatureç³»åˆ—ç½‘ç«™"""
        nature_domains = ['nature.com', 'nature.org', 'springernature.com']
        return any(domain in url.lower() for domain in nature_domains)
    
    def get_database_name(self) -> str:
        return "Nature"
    
    def requires_authentication(self) -> bool:
        return True
    
    def extract_metadata(self, url: str) -> Dict:
        """ä»Nature URLæå–è®ºæ–‡å…ƒæ•°æ®"""
        try:
            logger.info(f"ğŸ”„ å¼€å§‹æå–Natureè®ºæ–‡å…ƒæ•°æ®: {url}")
            
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                return {'error': f'æ— æ³•è®¿é—®é¡µé¢ï¼ŒçŠ¶æ€ç : {response.status_code}', 'url': url}
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            metadata = {
                'url': url,
                'itemType': 'journalArticle',
                'journal': 'Nature'
            }
            
            # æå–æ ‡é¢˜
            title_selectors = [
                'h1[data-test="article-title"]',
                'h1.c-article-title', 
                'h1.article-title',
                'h1'
            ]
            
            for selector in title_selectors:
                try:
                    element = soup.select_one(selector)
                    if element:
                        title = element.get_text(strip=True)
                        if title and len(title) > 3:
                            metadata['title'] = title
                            logger.info(f"ğŸ“ æå–æ ‡é¢˜: {title}")
                            break
                except Exception:
                    continue
            
            # æå–ä½œè€…
            authors = self._extract_authors(soup)
            if authors:
                metadata['authors'] = authors
                logger.info(f"ğŸ‘¥ æå–ä½œè€…: {authors[:50]}...")
            
            # æå–æ‘˜è¦
            abstract_selectors = [
                '[data-test="abstract-content"]',
                '.c-article-section__content',
                '.abstract'
            ]
            
            for selector in abstract_selectors:
                try:
                    element = soup.select_one(selector)
                    if element:
                        abstract = element.get_text(strip=True)
                        if abstract and len(abstract) > 20:
                            metadata['abstract'] = abstract
                            logger.info(f"ğŸ“„ æå–æ‘˜è¦: {len(abstract)} å­—ç¬¦")
                            break
                except Exception:
                    continue
            
            # æå–DOI
            doi = self._extract_doi(soup, url)
            if doi:
                metadata['doi'] = doi
                logger.info(f"ğŸ”— æå–DOI: {doi}")
            
            # æå–PDF URL
            pdf_url = self._extract_pdf_url(soup, url)
            if pdf_url:
                metadata['pdf_url'] = pdf_url
                logger.info(f"ğŸ“¥ æå–PDF URL: {pdf_url}")
            
            logger.info("âœ… Natureå…ƒæ•°æ®æå–å®Œæˆ")
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ Natureå…ƒæ•°æ®æå–å¤±è´¥: {e}")
            return {'error': f'æå–å¤±è´¥: {e}', 'url': url}
    
    def _extract_authors(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–ä½œè€…"""
        authors_selectors = [
            '[data-test="author-name"]',
            '.c-article-author-list__item',
            '.AuthorName'
        ]
        
        authors = []
        for selector in authors_selectors:
            try:
                author_elements = soup.select(selector)
                if author_elements:
                    for elem in author_elements:
                        author_text = elem.get_text(strip=True)
                        if author_text and author_text not in authors:
                            authors.append(author_text)
                    break
            except Exception:
                continue
        
        return ', '.join(authors[:10]) if authors else None
    
    def _extract_doi(self, soup: BeautifulSoup, url: str) -> Optional[str]:
        """æå–DOI"""
        # ä»URLä¸­æå–DOI
        doi_match = re.search(r'nature\.com/articles/([^/?]+)', url)
        if doi_match:
            article_id = doi_match.group(1)
            if article_id.startswith('s'):
                return f"10.1038/{article_id}"
        
        return None
    
    def _extract_pdf_url(self, soup: BeautifulSoup, url: str) -> Optional[str]:
        """æå–PDF URL - ä¼˜å…ˆä¸»æ–‡ç« PDF"""
        try:
            # æ–¹æ³•1: æ„é€ æ ‡å‡†PDF URLï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            doi_match = re.search(r'nature\.com/articles/([^/?]+)', url)
            if doi_match:
                article_id = doi_match.group(1)
                # Natureçš„æ ‡å‡†PDF URLæ ¼å¼
                constructed_pdf_url = f"https://www.nature.com/articles/{article_id}.pdf"
                logger.info(f"ğŸ”— æ„é€ æ ‡å‡†PDF URL: {constructed_pdf_url}")
                return constructed_pdf_url
            
            # æ–¹æ³•2: æŸ¥æ‰¾ä¸»æ–‡ç« PDFä¸‹è½½é“¾æ¥
            main_pdf_selectors = [
                'a[data-track-action="download pdf"]',
                '.c-pdf-download a',
                '.pdf-download-link',
                '[data-test="pdf-link"]',
                'a[title*="Download PDF"]',
                'a[aria-label*="Download PDF"]'
            ]
            
            for selector in main_pdf_selectors:
                try:
                    pdf_links = soup.select(selector)
                    for link in pdf_links:
                        href = link.get('href')
                        if href and self._is_main_article_pdf(href):
                            # è½¬æ¢ä¸ºç»å¯¹URL
                            if href.startswith('/'):
                                pdf_url = urljoin(url, href)
                            elif href.startswith('http'):
                                pdf_url = href
                            else:
                                continue
                            
                            logger.info(f"ğŸ¯ æ‰¾åˆ°ä¸»æ–‡ç« PDFé“¾æ¥: {pdf_url}")
                            return pdf_url
                except Exception as e:
                    logger.debug(f"PDFé€‰æ‹©å™¨ {selector} å¤±è´¥: {e}")
                    continue
            
            # æ–¹æ³•3: é€šç”¨PDFé“¾æ¥ï¼ˆè¿‡æ»¤è¡¥å……ææ–™ï¼‰
            general_pdf_selectors = [
                'a[href*=".pdf"]',
                'a[href*="/pdf/"]'
            ]
            
            for selector in general_pdf_selectors:
                try:
                    pdf_links = soup.select(selector)
                    for link in pdf_links:
                        href = link.get('href')
                        if href and self._is_main_article_pdf(href):
                            if href.startswith('/'):
                                pdf_url = urljoin(url, href)
                            elif href.startswith('http'):
                                pdf_url = href
                            else:
                                continue
                            
                            logger.info(f"ğŸ“„ æ‰¾åˆ°PDFé“¾æ¥: {pdf_url}")
                            return pdf_url
                except Exception:
                    continue
            
            logger.warning("âŒ æœªæ‰¾åˆ°ä¸»æ–‡ç« PDFé“¾æ¥")
            return None
            
        except Exception as e:
            logger.error(f"âŒ PDF URLæå–å¤±è´¥: {e}")
            return None
    
    def _is_main_article_pdf(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ä¸»æ–‡ç« PDFï¼ˆæ’é™¤è¡¥å……ææ–™ï¼‰"""
        url_lower = url.lower()
        
        # æ’é™¤è¡¥å……ææ–™çš„å…³é”®è¯
        exclude_patterns = [
            'moesm',  # Supplementary materials
            'supplement', 
            'supporting',
            'additional',
            'si.pdf',  # Supporting Information
            'supp',
            'appendix',
            'mediaobjects'
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤æ¨¡å¼
        if any(pattern in url_lower for pattern in exclude_patterns):
            logger.debug(f"æ’é™¤è¡¥å……ææ–™PDF: {url}")
            return False
        
        # å¿…é¡»åŒ…å«PDFç›¸å…³æ¨¡å¼
        include_patterns = ['.pdf', '/pdf/', 'download']
        if not any(pattern in url_lower for pattern in include_patterns):
            return False
            
        return True
