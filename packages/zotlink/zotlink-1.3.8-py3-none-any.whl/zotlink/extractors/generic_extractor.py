#!/usr/bin/env python3
"""
ğŸ”— é€šç”¨å¼€æºå­¦æœ¯è®ºæ–‡æå–å™¨

åŸºäºæ ‡å‡†å…ƒæ•°æ®æ ‡ç­¾çš„é€šç”¨æå–ç­–ç•¥ï¼Œæ”¯æŒå¤§éƒ¨åˆ†å¼€æºå­¦æœ¯ç½‘ç«™
"""

import re
import json
import requests
import logging
from typing import Dict, List, Optional
from urllib.parse import urlparse, urljoin
from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class GenericOpenAccessExtractor(BaseExtractor):
    """é€šç”¨å¼€æºå­¦æœ¯è®ºæ–‡æå–å™¨"""
    
    # å¼€æºç½‘ç«™åŸŸåæ¨¡å¼è¯†åˆ«
    OPEN_ACCESS_PATTERNS = {
        # ğŸ”§ ä¿®å¤åŸŸååŒ¹é…ï¼šä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼Œé¿å…è¯¯åŒ¹é…
        r'(?<!soc)(?<!med)(?<!bio)arxiv\.org': {'type': 'preprint', 'source': 'arXiv', 'priority': 0},  # å·²æœ‰ä¸“ç”¨ï¼Œæ’é™¤å…¶ä»–rxiv
        r'medrxiv\.org': {'type': 'preprint', 'source': 'medRxiv', 'priority': 1},
        r'biorxiv\.org': {'type': 'preprint', 'source': 'bioRxiv', 'priority': 1},
        r'chemrxiv\.org': {'type': 'preprint', 'source': 'ChemRxiv', 'priority': 1},
        r'psyarxiv\.com': {'type': 'preprint', 'source': 'PsyArXiv', 'priority': 1},
        r'socarxiv\.org': {'type': 'preprint', 'source': 'SocArXiv', 'priority': 1},
        r'osf\.io/preprints/psyarxiv': {'type': 'preprint', 'source': 'PsyArXiv', 'priority': 1},  # ğŸ”§ ä¿®å¤ï¼šOSFæ ¼å¼çš„PsyArXiv
        r'osf\.io/preprints/socarxiv': {'type': 'preprint', 'source': 'SocArXiv', 'priority': 1},  # ğŸ”§ ä¿®å¤ï¼šOSFæ ¼å¼çš„SocArXiv
        r'openaccess\.thecvf\.com': {'type': 'conferencePaper', 'source': 'CVF', 'priority': 0},  # å·²æœ‰ä¸“ç”¨
        r'proceedings\.mlr\.press': {'type': 'conferencePaper', 'source': 'MLR Press', 'priority': 1},
        r'openreview\.net': {'type': 'conferencePaper', 'source': 'OpenReview', 'priority': 1},
        r'plos\.org|plosone\.org': {'type': 'journalArticle', 'source': 'PLOS', 'priority': 1},
        r'ncbi\.nlm\.nih\.gov/pmc': {'type': 'journalArticle', 'source': 'PMC', 'priority': 1},
        r'frontiersin\.org': {'type': 'journalArticle', 'source': 'Frontiers', 'priority': 1},
        r'mdpi\.com': {'type': 'journalArticle', 'source': 'MDPI', 'priority': 1},
        r'hindawi\.com': {'type': 'journalArticle', 'source': 'Hindawi', 'priority': 1},
        r'nature\.com': {'type': 'journalArticle', 'source': 'Nature', 'priority': 0},  # å·²æœ‰ä¸“ç”¨
        # æœºæ„åº“æ¨¡å¼
        r'dspace\.|eprints\.|repository\.|digital\.': {'type': 'journalArticle', 'source': 'Repository', 'priority': 2},
    }
    
    # Citationæ ‡ç­¾æ˜ å°„
    CITATION_FIELDS = {
        'citation_title': 'title',
        'citation_author': 'authors',
        'citation_journal_title': 'publicationTitle',
        'citation_conference_title': 'publicationTitle',
        'citation_publisher': 'publicationTitle',  # æœ‰äº›ç½‘ç«™ç”¨è¿™ä¸ªå­—æ®µ
        'citation_publication_date': 'date',
        'citation_date': 'date',
        'citation_online_date': 'date',  # åœ¨çº¿å‘å¸ƒæ—¥æœŸ
        'citation_doi': 'DOI',
        'citation_pmid': 'pmid',
        'citation_pmcid': 'pmcid',
        'citation_pdf_url': 'pdf_url',
        'citation_fulltext_pdf_url': 'pdf_url',  # ğŸ”§ æ–°å¢ï¼šå¦ä¸€ç§PDFæ ‡ç­¾
        'citation_fulltext_html_url': 'html_url',
        'citation_abstract': 'abstract',
        'citation_keywords': 'tags',
        'citation_volume': 'volume',
        'citation_issue': 'issue',
        'citation_firstpage': 'pages_start',
        'citation_lastpage': 'pages_end',
        # é¢„å°æœ¬ç‰¹æœ‰å­—æ®µ
        'citation_preprint_server': 'repository',
        'citation_archive_id': 'archiveID',
        'citation_preprint_doi': 'DOI',
    }
    
    # Dublin Coreæ ‡ç­¾æ˜ å°„
    DUBLIN_CORE_FIELDS = {
        'DC.title': 'title',
        'DC.creator': 'authors',
        'DC.date': 'date',
        'DC.publisher': 'publisher',
        'DC.description': 'abstract',
        'DC.subject': 'tags',
        'DC.identifier': 'identifier',
    }
    
    def __init__(self, session: requests.Session = None):
        """åˆå§‹åŒ–é€šç”¨æå–å™¨"""
        super().__init__(session)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def can_handle(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†æ­¤URL"""
        domain_info = self._identify_domain(url)
        # priority > 0 è¡¨ç¤ºå¯ä»¥ç”¨é€šç”¨æå–å™¨å¤„ç†
        return domain_info['priority'] > 0
    
    def requires_authentication(self) -> bool:
        """å¤§å¤šæ•°å¼€æºç½‘ç«™ä¸éœ€è¦è®¤è¯"""
        return False
    
    def get_database_name(self) -> str:
        """è·å–æ•°æ®åº“åç§°"""
        return "Generic"
    
    def extract_metadata(self, url: str) -> Dict:
        """é€šç”¨å…ƒæ•°æ®æå–"""
        try:
            # è·å–ç½‘é¡µå†…å®¹
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return {'error': f'æ— æ³•è®¿é—®é¡µé¢: {response.status_code}', 'url': url}
            
            html_content = response.text
            
            # ç¬¬ä¸€å±‚ï¼šæ ‡å‡†å…ƒæ•°æ®æ ‡ç­¾æå–
            metadata = self._extract_citation_tags(html_content)
            
            # ç¬¬äºŒå±‚ï¼šç»“æ„åŒ–æ•°æ®æå– (JSON-LD, RDFa)
            if not self._is_metadata_sufficient(metadata):
                structured_data = self._extract_structured_data(html_content)
                metadata.update(structured_data)
            
            # ç¬¬ä¸‰å±‚ï¼šå¯å‘å¼HTMLè§£æ
            if not self._is_metadata_sufficient(metadata):
                heuristic_data = self._extract_heuristic(html_content)
                metadata.update(heuristic_data)
            
            # ç¬¬å››å±‚ï¼šURLæ¨¡å¼è¯†åˆ«å’Œè¡¥å……
            domain_info = self._identify_domain(url)
            metadata.update({
                'extractor': f"Generic-{domain_info['source']}",
                'itemType': domain_info['type'],
                'source': domain_info['source'],
                'url': url
            })
            
            # ğŸ†• å¢å¼ºï¼šä»URLæå–é¢å¤–ä¿¡æ¯
            metadata = self._extract_from_url_patterns(metadata, url)
            
            # ğŸ”§ æ”¹è¿›ï¼šå¦‚æœè¿˜æ²¡æœ‰PDFé“¾æ¥ï¼Œå°è¯•å¯å‘å¼æœç´¢
            if not metadata.get('pdf_url'):
                metadata = self._search_pdf_links_in_html(html_content, url, metadata)
            
            # æ•°æ®æ¸…ç†å’Œæ ‡å‡†åŒ–
            metadata = self._clean_and_standardize(metadata)
            
            # ğŸ†• å¢å¼ºï¼šè¡¥å……é¢„å°æœ¬ç‰¹æœ‰å­—æ®µ
            metadata = self._enhance_preprint_fields(metadata, url)
            
            logger.info(f"âœ… é€šç”¨æå–å™¨æˆåŠŸå¤„ç†: {metadata.get('title', 'Unknown')[:50]}...")
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ é€šç”¨å…ƒæ•°æ®æå–å¤±è´¥: {e}")
            return {
                'error': f'é€šç”¨å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}',
                'url': url
            }
    
    def _identify_domain(self, url: str) -> Dict:
        """è¯†åˆ«åŸŸåç±»å‹"""
        for pattern, info in self.OPEN_ACCESS_PATTERNS.items():
            if re.search(pattern, url, re.IGNORECASE):
                return info
        
        # é»˜è®¤å¤„ç†
        return {'type': 'journalArticle', 'source': 'Unknown', 'priority': 1}
    
    def _extract_citation_tags(self, html_content: str) -> Dict:
        """æå–Citationå…ƒæ•°æ®æ ‡ç­¾"""
        metadata = {}
        authors = []
        
        for meta_name, field_name in self.CITATION_FIELDS.items():
            if meta_name in ['citation_author']:
                # å¤„ç†å¤šä½œè€…
                matches = re.findall(f'<meta name="{meta_name}" content="([^"]+)"', html_content, re.IGNORECASE)
                if matches:
                    authors.extend(matches)
            else:
                # å¤„ç†å•å€¼å­—æ®µ
                match = re.search(f'<meta name="{meta_name}" content="([^"]+)"', html_content, re.IGNORECASE)
                if match:
                    value = match.group(1).strip()
                    if field_name == 'date':
                        value = self._normalize_date(value)
                    metadata[field_name] = value
        
        # å¤„ç†ä½œè€…åˆ—è¡¨
        if authors:
            formatted_authors = []
            for author in authors:
                author = author.strip()
                if ',' in author:
                    formatted_authors.append(author)  # "Last, First"æ ¼å¼
                else:
                    # "First Last" â†’ "Last, First"
                    parts = author.split()
                    if len(parts) >= 2:
                        last_name = parts[-1]
                        first_names = ' '.join(parts[:-1])
                        formatted_authors.append(f"{last_name}, {first_names}")
                    else:
                        formatted_authors.append(author)
            
            metadata['authors'] = '; '.join(formatted_authors)
        
        return metadata
    
    def _extract_dublin_core(self, html_content: str) -> Dict:
        """æå–Dublin Coreå…ƒæ•°æ®"""
        metadata = {}
        
        for meta_name, field_name in self.DUBLIN_CORE_FIELDS.items():
            pattern = f'<meta name="{meta_name}" content="([^"]+)"'
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                metadata[field_name] = match.group(1).strip()
        
        return metadata
    
    def _extract_structured_data(self, html_content: str) -> Dict:
        """æå–ç»“æ„åŒ–æ•°æ® (JSON-LD, Schema.org)"""
        metadata = {}
        
        # æå–JSON-LD
        jsonld_pattern = r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
        jsonld_matches = re.findall(jsonld_pattern, html_content, re.DOTALL | re.IGNORECASE)
        
        for jsonld_content in jsonld_matches:
            try:
                data = json.loads(jsonld_content.strip())
                
                # å¤„ç†å•ä¸ªå¯¹è±¡æˆ–æ•°ç»„
                if isinstance(data, list):
                    data = data[0] if data else {}
                
                # æ˜ å°„Schema.orgå­—æ®µ
                if data.get('@type') in ['ScholarlyArticle', 'Article', 'CreativeWork']:
                    if data.get('headline'):
                        metadata['title'] = data['headline']
                    elif data.get('name'):
                        metadata['title'] = data['name']
                    
                    if data.get('author'):
                        authors = data['author']
                        if isinstance(authors, list):
                            author_names = [auth.get('name', '') for auth in authors if auth.get('name')]
                        else:
                            author_names = [authors.get('name', '')] if authors.get('name') else []
                        
                        if author_names:
                            metadata['authors'] = '; '.join(author_names)
                    
                    if data.get('datePublished'):
                        metadata['date'] = self._normalize_date(data['datePublished'])
                    
                    if data.get('description'):
                        metadata['abstract'] = data['description']
                    
                    if data.get('publisher', {}).get('name'):
                        metadata['publisher'] = data['publisher']['name']
                
            except json.JSONDecodeError:
                continue
        
        return metadata
    
    def _extract_heuristic(self, html_content: str) -> Dict:
        """å¯å‘å¼HTMLè§£æ"""
        metadata = {}
        
        # æ ‡é¢˜å¯å‘å¼åŒ¹é…
        if not metadata.get('title'):
            title_patterns = [
                r'<h1[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</h1>',
                r'<div[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</div>',
                r'<span[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</span>',
                r'<title>([^<]+)</title>',
                r'<h1[^>]*>([^<]+)</h1>',  # ç®€å•h1æ ‡ç­¾
            ]
            
            for pattern in title_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
                if match:
                    title = match.group(1).strip()
                    title = re.sub(r'<[^>]+>', '', title)  # æ¸…ç†HTMLæ ‡ç­¾
                    title = re.sub(r'\s+', ' ', title).strip()
                    if len(title) > 10 and len(title) < 300:  # åˆç†çš„æ ‡é¢˜é•¿åº¦
                        metadata['title'] = title
                        break
        
        # ä½œè€…å¯å‘å¼åŒ¹é…
        if not metadata.get('authors'):
            author_patterns = [
                r'<div[^>]*class="[^"]*author[^"]*"[^>]*>([^<]+)</div>',
                r'<span[^>]*class="[^"]*author[^"]*"[^>]*>([^<]+)</span>',
                r'<p[^>]*class="[^"]*author[^"]*"[^>]*>([^<]+)</p>',
            ]
            
            for pattern in author_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                if matches:
                    authors = [match.strip() for match in matches if match.strip()]
                    if authors:
                        metadata['authors'] = '; '.join(authors[:10])  # é™åˆ¶ä½œè€…æ•°é‡
                        break
        
        # æ‘˜è¦å¯å‘å¼åŒ¹é…
        if not metadata.get('abstract'):
            abstract_patterns = [
                r'<div[^>]*class="[^"]*abstract[^"]*"[^>]*>(.*?)</div>',
                r'<p[^>]*class="[^"]*abstract[^"]*"[^>]*>(.*?)</p>',
                r'<section[^>]*class="[^"]*abstract[^"]*"[^>]*>(.*?)</section>',
            ]
            
            for pattern in abstract_patterns:
                match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
                if match:
                    abstract = match.group(1).strip()
                    abstract = re.sub(r'<[^>]+>', '', abstract)  # æ¸…ç†HTMLæ ‡ç­¾
                    abstract = re.sub(r'\s+', ' ', abstract).strip()
                    if len(abstract) > 50:  # æœ€å°æ‘˜è¦é•¿åº¦
                        metadata['abstract'] = abstract
                        break
        
        return metadata
    
    def _is_metadata_sufficient(self, metadata: Dict) -> bool:
        """åˆ¤æ–­å…ƒæ•°æ®æ˜¯å¦è¶³å¤Ÿå®Œæ•´"""
        required_fields = ['title']
        optional_fields = ['authors', 'abstract', 'date']
        
        # å¿…é¡»æœ‰æ ‡é¢˜
        if not metadata.get('title'):
            return False
        
        # è‡³å°‘æœ‰ä¸€ä¸ªå¯é€‰å­—æ®µ
        has_optional = any(metadata.get(field) for field in optional_fields)
        return has_optional
    
    def _normalize_date(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        if not date_str:
            return ""
        
        # å¸¸è§æ—¥æœŸæ ¼å¼æ¨¡å¼
        patterns = [
            (r'(\d{4})-(\d{2})-(\d{2})', r'\1-\2-\3'),  # YYYY-MM-DD
            (r'(\d{4})/(\d{2})/(\d{2})', r'\1-\2-\3'),  # YYYY/MM/DD
            (r'(\d{2})/(\d{2})/(\d{4})', r'\3-\1-\2'),  # MM/DD/YYYY
            (r'(\d{1,2})\s+(\w+)\s+(\d{4})', self._convert_month_name),  # DD Month YYYY
        ]
        
        for pattern, replacement in patterns:
            if callable(replacement):
                match = re.search(pattern, date_str)
                if match:
                    return replacement(match)
            else:
                if re.search(pattern, date_str):
                    return re.sub(pattern, replacement, date_str)
        
        return date_str  # è¿”å›åŸæ ¼å¼
    
    def _convert_month_name(self, match) -> str:
        """è½¬æ¢æœˆä»½åç§°ä¸ºæ•°å­—"""
        day, month_name, year = match.groups()
        
        months = {
            'january': '01', 'jan': '01',
            'february': '02', 'feb': '02',
            'march': '03', 'mar': '03',
            'april': '04', 'apr': '04',
            'may': '05',
            'june': '06', 'jun': '06',
            'july': '07', 'jul': '07',
            'august': '08', 'aug': '08',
            'september': '09', 'sep': '09',
            'october': '10', 'oct': '10',
            'november': '11', 'nov': '11',
            'december': '12', 'dec': '12'
        }
        
        month_num = months.get(month_name.lower(), '01')
        return f"{year}-{month_num}-{day.zfill(2)}"
    
    def _extract_from_url_patterns(self, metadata: Dict, url: str) -> Dict:
        """ä»URLæ¨¡å¼ä¸­æå–é¢å¤–ä¿¡æ¯ - ğŸ”§ ä¿®å¤ï¼šä»…æå–DOIï¼Œä¸æ„é€ å¯èƒ½æ— æ•ˆçš„PDFé“¾æ¥"""
        
        # medRxiv URLæ¨¡å¼: https://www.medrxiv.org/content/10.1101/2025.09.01.25334224v1
        if 'medrxiv.org' in url.lower():
            # DOIæå–
            if not metadata.get('DOI'):
                doi_match = re.search(r'10\.1101/([0-9]{4}\.[0-9]{2}\.[0-9]{2}(?:\.[0-9]+)?)', url)
                if doi_match:
                    full_doi = f"10.1101/{doi_match.group(1)}"
                    metadata['DOI'] = full_doi
                    
            # ä»DOIæå–æ—¥æœŸ
            if metadata.get('DOI'):
                date_match = re.search(r'10\.1101/([0-9]{4})\.([0-9]{2})\.([0-9]{2})', metadata['DOI'])
                if date_match:
                    year, month, day = date_match.groups()
                    metadata['date'] = f"{year}-{month}-{day}"
                    
        # bioRxiv URLæ¨¡å¼: https://www.biorxiv.org/content/10.1101/2024.09.16.613241v1
        elif 'biorxiv.org' in url.lower():
            # DOIæå–
            if not metadata.get('DOI'):
                doi_match = re.search(r'10\.1101/([0-9]{4}\.[0-9]{2}\.[0-9]{2}(?:\.[0-9]+)?)', url)
                if doi_match:
                    metadata['DOI'] = f"10.1101/{doi_match.group(1)}"
                    
            # ä»DOIæå–æ—¥æœŸ
            if metadata.get('DOI'):
                date_match = re.search(r'10\.1101/([0-9]{4})\.([0-9]{2})\.([0-9]{2})', metadata['DOI'])
                if date_match:
                    year, month, day = date_match.groups()
                    metadata['date'] = f"{year}-{month}-{day}"
                    
        # PLOS DOIæå–
        elif 'plos.org' in url.lower():
            if not metadata.get('DOI'):
                doi_match = re.search(r'10\.1371/journal\.[^&\s]+', url)
                if doi_match:
                    metadata['DOI'] = doi_match.group(0)
                    
        # PMC IDæå–
        elif 'ncbi.nlm.nih.gov/pmc' in url.lower():
            pmc_match = re.search(r'PMC(\d+)', url)
            if pmc_match:
                metadata['pmcid'] = f"PMC{pmc_match.group(1)}"
                
        # ğŸ”§ æ³¨æ„ï¼šä¸å†æ„é€ å¯èƒ½æ— æ•ˆçš„PDFé“¾æ¥
        # PDFé“¾æ¥åº”è¯¥ä¼˜å…ˆä»HTMLçš„citationæ ‡ç­¾ä¸­æå–
        # åªæœ‰åœ¨HTMLè§£ææˆåŠŸæ—¶æ‰ä¼šæœ‰å¯é çš„PDFé“¾æ¥
                
        return metadata
    
    def _enhance_preprint_fields(self, metadata: Dict, url: str) -> Dict:
        """å¢å¼ºé¢„å°æœ¬ç‰¹æœ‰å­—æ®µ"""
        
        # åªå¤„ç†é¢„å°æœ¬ç±»å‹
        if metadata.get('itemType') != 'preprint':
            return metadata
            
        source = metadata.get('source', '').lower()
        
        # ä¸ºé¢„å°æœ¬è®¾ç½®Repositoryå­—æ®µ
        if not metadata.get('repository'):
            if 'medrxiv' in source:
                metadata['repository'] = 'medRxiv'
            elif 'biorxiv' in source:
                metadata['repository'] = 'bioRxiv'
            elif 'chemrxiv' in source:
                metadata['repository'] = 'ChemRxiv'
            elif 'psyarxiv' in source:
                metadata['repository'] = 'PsyArXiv'
            elif 'socarxiv' in source:
                metadata['repository'] = 'SocArXiv'
            else:
                metadata['repository'] = metadata.get('source', 'Preprint Server')
        
        # è®¾ç½®Archive IDï¼ˆåŸºäºDOIï¼‰
        if metadata.get('DOI') and not metadata.get('archiveID'):
            # å¯¹äºbioRxiv/medRxivçš„DOIæ ¼å¼
            if metadata['DOI'].startswith('10.1101/'):
                metadata['archiveID'] = metadata['DOI']
            else:
                metadata['archiveID'] = metadata['DOI']
                
        # ğŸ†• å¯¹äºé¢„å°æœ¬ï¼Œå¼ºåˆ¶ä½¿ç”¨æœåŠ¡å™¨åç§°ä½œä¸ºPublication Title
        # è¿™æ ·ç”¨æˆ·åœ¨Zoteroä¸­çœ‹åˆ°çš„æ˜¯"medRxiv"è€Œä¸æ˜¯"Cold Spring Harbor Laboratory Press"
        metadata['publicationTitle'] = metadata.get('repository', metadata.get('source', 'Preprint'))
            
        # è®¾ç½®Library Catalog
        if not metadata.get('libraryCatalog'):
            if 'medrxiv' in source:
                metadata['libraryCatalog'] = 'medRxiv.org'
            elif 'biorxiv' in source:
                metadata['libraryCatalog'] = 'bioRxiv.org'
            else:
                domain_match = re.search(r'https?://([^/]+)', url)
                if domain_match:
                    metadata['libraryCatalog'] = domain_match.group(1)
        
        # è®¾ç½®è®¿é—®æ—¥æœŸ
        if not metadata.get('accessDate'):
            import time
            metadata['accessDate'] = time.strftime('%Y-%m-%d')
            
        return metadata
    
    def _search_pdf_links_in_html(self, html_content: str, url: str, metadata: Dict) -> Dict:
        """åœ¨HTMLä¸­å¯å‘å¼æœç´¢PDFé“¾æ¥"""
        
        try:
            # ğŸ”§ ç‰¹å®šç½‘ç«™çš„PDFé“¾æ¥æ„é€ é€»è¾‘
            if 'chemrxiv.org' in url.lower():
                # ChemRxivç‰¹æ®Šå¤„ç†ï¼šåŸºäºæ–‡ç« IDæ„é€ PDFé“¾æ¥
                article_match = re.search(r'article-details/([a-f0-9]{24,})', url)
                if article_match:
                    article_id = article_match.group(1)
                    # ä½¿ç”¨å‘ç°çš„æœ‰æ•ˆPDF URLæ ¼å¼
                    chemrxiv_pdf_url = f"https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/{article_id}/original/manuscript.pdf"
                    metadata['pdf_url'] = chemrxiv_pdf_url
                    logger.info(f"ğŸ§ª ChemRxivæ„é€ PDFé“¾æ¥: {chemrxiv_pdf_url}")
                    return metadata
            
            elif 'osf.io/preprints' in url.lower():
                # OSF preprints (PsyArXiv, SocArXivç­‰)
                # URLæ ¼å¼: https://osf.io/preprints/socarxiv/rhqmu_v1
                preprint_match = re.search(r'osf\.io/preprints/[^/]+/([a-z0-9]+)', url)
                if preprint_match:
                    preprint_id = preprint_match.group(1)
                    osf_pdf_url = f"https://osf.io/{preprint_id}/download"
                    metadata['pdf_url'] = osf_pdf_url
                    logger.info(f"ğŸ”— OSFæ„é€ PDFé“¾æ¥: {osf_pdf_url}")
                    return metadata
            
            elif 'biorxiv.org' in url.lower() or 'medrxiv.org' in url.lower():
                # bioRxiv/medRxivç¦»çº¿PDFæ„é€  (å½“403/404æ—¶ä½¿ç”¨)
                # URLæ ¼å¼: https://www.biorxiv.org/content/10.1101/2025.02.19.639094v1
                doi_match = re.search(r'10\.1101/([0-9]{4}\.[0-9]{2}\.[0-9]{2}\.[0-9]+)', url)
                if doi_match:
                    doi_id = doi_match.group(1)
                    year, month, day = doi_id.split('.')[:3]
                    
                    # å°è¯•å¤šç§å¯èƒ½çš„PDF URLæ ¼å¼
                    if 'biorxiv.org' in url.lower():
                        possible_pdf_urls = [
                            f"https://www.biorxiv.org/content/10.1101/{doi_id}.full.pdf",
                            f"https://www.biorxiv.org/content/biorxiv/early/{year}/{month.zfill(2)}/{day.zfill(2)}/{doi_id}/{doi_id}.full.pdf",
                            f"https://www.biorxiv.org/content/early/{year}/{month.zfill(2)}/{day.zfill(2)}/{doi_id}.full.pdf"
                        ]
                        service = "bioRxiv"
                    else:
                        # ğŸ¯ ä¿®å¤ï¼šä»URLæå–å®Œæ•´IDï¼ˆå«ç‰ˆæœ¬å·ï¼‰
                        doc_id_match = re.search(r'/content/(?:10\.1101/)?([0-9]{4}\.[0-9]{2}\.[0-9]{2}\.[0-9]+v?\d*)', url)
                        full_doc_id = doc_id_match.group(1) if doc_id_match else doi_id
                        
                        possible_pdf_urls = [
                            f"https://www.medrxiv.org/content/10.1101/{full_doc_id}.full.pdf",
                            f"https://www.medrxiv.org/content/medrxiv/early/{year}/{month.zfill(2)}/{day.zfill(2)}/{full_doc_id}/{full_doc_id}.full.pdf",
                            f"https://www.medrxiv.org/content/early/{year}/{month.zfill(2)}/{day.zfill(2)}/{full_doc_id}.full.pdf"
                        ]
                        service = "medRxiv"
                    
                    # ä½¿ç”¨æœ€å¯èƒ½çš„æ ¼å¼ (æ ¹æ®ç»éªŒï¼Œç¬¬ä¸€ä¸ªæ ¼å¼æœ€å¸¸è§)
                    metadata['pdf_url'] = possible_pdf_urls[0]
                    logger.info(f"ğŸ”§ {service}ç¦»çº¿æ„é€ PDFé“¾æ¥: {possible_pdf_urls[0]}")
                    return metadata
            
            # é€šç”¨HTML PDFé“¾æ¥æœç´¢æ¨¡å¼
            pdf_patterns = [
                # ç›´æ¥é“¾æ¥åˆ°PDFæ–‡ä»¶
                r'href="([^"]*\.pdf[^"]*)"[^>]*>(?:[^<]*(?:download|pdf|full\s*text)[^<]*)</a>',
                r'href="([^"]*\.pdf[^"]*)"[^>]*>[^<]*(?:PDF|pdf)[^<]*</a>',
                r'href="([^"]*\.pdf[^"]*)"',
                
                # medRxiv/bioRxivç‰¹æœ‰çš„PDFé“¾æ¥
                r'href="([^"]*content[^"]*\.full\.pdf[^"]*)"',
                r'href="([^"]*early[^"]*\.full\.pdf[^"]*)"',
                
                # ä¸‹è½½é“¾æ¥
                r'href="([^"]*download[^"]*)"[^>]*>(?:[^<]*(?:PDF|pdf)[^<]*)</a>',
                r'href="([^"]*ndownloader[^"]*)"',  # ChemRxivç­‰
                
                # OSFç±»å‹çš„ä¸‹è½½é“¾æ¥
                r'href="(https://osf\.io/[a-z0-9]+/download)"',
                
                # å…¶ä»–å¯èƒ½çš„PDFé“¾æ¥æ ¼å¼
                r'href="([^"]*fulltext[^"]*\.pdf[^"]*)"',
                r'href="([^"]*manuscript[^"]*\.pdf[^"]*)"',
                
                # ğŸ”§ æ–°å¢ï¼šæ›´å¤šå¸¸è§çš„PDFé“¾æ¥æ¨¡å¼
                r'href="([^"]*)"[^>]*>(?:[^<]*(?:Full Text|å…¨æ–‡|PDFç‰ˆæœ¬)[^<]*)</a>',
                r'data-download-url="([^"]+)"',  # æ•°æ®å±æ€§ä¸­çš„ä¸‹è½½é“¾æ¥
                r'downloadUrl["\']?\s*[:=]\s*["\']([^"\']+)["\']',  # JavaScriptå˜é‡
            ]
            
            found_pdfs = []
            
            for pattern in pdf_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    if match not in found_pdfs:
                        found_pdfs.append(match)
            
            if found_pdfs:
                # é€‰æ‹©æœ€å¯èƒ½æ˜¯ä¸»æ–‡æ¡£PDFçš„é“¾æ¥
                main_pdf = self._select_main_pdf_link(found_pdfs, url)
                if main_pdf:
                    # ç¡®ä¿æ˜¯ç»å¯¹URL
                    if not main_pdf.startswith('http'):
                        from urllib.parse import urljoin
                        main_pdf = urljoin(url, main_pdf)
                    
                    metadata['pdf_url'] = main_pdf
                    logger.info(f"ğŸ” å¯å‘å¼æœç´¢æ‰¾åˆ°PDFé“¾æ¥: {main_pdf[:60]}...")
            
        except Exception as e:
            logger.warning(f"âš ï¸ PDFé“¾æ¥å¯å‘å¼æœç´¢å¤±è´¥: {e}")
        
        return metadata
    
    def _select_main_pdf_link(self, pdf_links: list, base_url: str) -> str:
        """ä»å¤šä¸ªPDFé“¾æ¥ä¸­é€‰æ‹©æœ€å¯èƒ½æ˜¯ä¸»æ–‡æ¡£çš„é“¾æ¥"""
        
        if not pdf_links:
            return None
        
        if len(pdf_links) == 1:
            return pdf_links[0]
        
        # æ’åºä¼˜å…ˆçº§è§„åˆ™
        def pdf_priority(pdf_url):
            score = 0
            pdf_lower = pdf_url.lower()
            
            # ä¼˜å…ˆé€‰æ‹©åŒ…å«è¿™äº›å…³é”®è¯çš„é“¾æ¥
            if 'full.pdf' in pdf_lower:
                score += 10
            if 'manuscript' in pdf_lower:
                score += 8  
            if 'main' in pdf_lower:
                score += 7
            if 'download' in pdf_lower and 'osf.io' in pdf_lower:
                score += 6  # OSFä¸‹è½½é“¾æ¥
            if 'ndownloader' in pdf_lower:
                score += 5  # ChemRxivä¸‹è½½
            
            # é™ä½ä¼˜å…ˆçº§çš„å†…å®¹
            if 'supplement' in pdf_lower:
                score -= 5
            if 'supporting' in pdf_lower:
                score -= 5  
            if 'appendix' in pdf_lower:
                score -= 3
            
            # ç»å¯¹URLä¼˜å…ˆ
            if pdf_url.startswith('http'):
                score += 2
                
            return score
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶è¿”å›æœ€é«˜åˆ†çš„
        sorted_pdfs = sorted(pdf_links, key=pdf_priority, reverse=True)
        return sorted_pdfs[0]
    
    def _clean_and_standardize(self, metadata: Dict) -> Dict:
        """æ¸…ç†å’Œæ ‡å‡†åŒ–å…ƒæ•°æ®"""
        # æ¸…ç†æ ‡é¢˜
        if metadata.get('title'):
            title = metadata['title']
            # ç§»é™¤å¤šä½™ç©ºç™½
            title = re.sub(r'\s+', ' ', title).strip()
            # ç§»é™¤å¸¸è§çš„æ— ç”¨åç¼€
            title = re.sub(r'\s*[\|\-]\s*(PLoS|PLOS|medRxiv|bioRxiv|Nature|Science).*$', '', title, re.IGNORECASE)
            metadata['title'] = title
        
        # æ¸…ç†ä½œè€…
        if metadata.get('authors'):
            authors = metadata['authors']
            # ç§»é™¤emailåœ°å€
            authors = re.sub(r'\s*\([^@]+@[^)]+\)', '', authors)
            # ç§»é™¤å¤šä½™ç©ºç™½
            authors = re.sub(r'\s+', ' ', authors).strip()
            metadata['authors'] = authors
        
        # æ¸…ç†æ‘˜è¦
        if metadata.get('abstract'):
            abstract = metadata['abstract']
            # é™åˆ¶é•¿åº¦
            if len(abstract) > 2000:
                abstract = abstract[:2000] + '...'
            metadata['abstract'] = abstract
        
        # ç¡®ä¿æœ‰PDFé“¾æ¥
        if metadata.get('pdf_url'):
            pdf_url = metadata['pdf_url']
            if not pdf_url.startswith('http'):
                # ç›¸å¯¹é“¾æ¥è½¬ç»å¯¹é“¾æ¥
                if metadata.get('url'):
                    base_url = metadata['url']
                    metadata['pdf_url'] = urljoin(base_url, pdf_url)
        
        # ğŸ†• ç¡®ä¿æœŸåˆŠç±»å’Œä¼šè®®è®ºæ–‡ä¹Ÿæœ‰åˆé€‚çš„publication title
        if not metadata.get('publicationTitle') and metadata.get('source'):
            source = metadata.get('source')
            item_type = metadata.get('itemType', '')
            
            if item_type == 'journalArticle':
                if 'PLOS' in source:
                    metadata['publicationTitle'] = 'PLOS ONE'
                elif 'Frontiers' in source:
                    metadata['publicationTitle'] = 'Frontiers'
                elif 'MDPI' in source:
                    metadata['publicationTitle'] = 'MDPI Journal'
                elif 'PMC' in source:
                    metadata['publicationTitle'] = 'PubMed Central'
                else:
                    metadata['publicationTitle'] = source
            elif item_type == 'conferencePaper':
                metadata['publicationTitle'] = source
        
        return metadata
    
    def test_access(self, test_url: str = None) -> bool:
        """æµ‹è¯•ç½‘ç«™è®¿é—®"""
        if not test_url:
            test_url = "https://www.medrxiv.org/"  # ä½¿ç”¨medRxivä½œä¸ºæµ‹è¯•
        
        try:
            response = self.session.get(test_url, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def get_supported_item_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¡ç›®ç±»å‹"""
        return ['journalArticle', 'conferencePaper', 'preprint'] 