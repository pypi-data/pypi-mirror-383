"""
æµè§ˆå™¨é©±åŠ¨çš„PDFæå–å™¨
ç”¨äºè§£å†³å¼€æºæ•°æ®åº“çš„åçˆ¬è™«é™åˆ¶ï¼Œå¦‚bioRxivã€OSFç³»åˆ—ç­‰
"""

import asyncio
import logging
from typing import Optional, Dict, Any, List
from urllib.parse import urljoin, urlparse
import re
import os

try:
    from playwright.async_api import async_playwright, Browser, Page, TimeoutError as PlaywrightTimeoutError
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    async_playwright = None

from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class BrowserExtractor(BaseExtractor):
    """æµè§ˆå™¨é©±åŠ¨çš„æå–å™¨ï¼Œç”¨äºå¤„ç†æœ‰åçˆ¬è™«æœºåˆ¶çš„å¼€æºæ•°æ®åº“"""
    
    # éœ€è¦ä½¿ç”¨æµè§ˆå™¨çš„åŸŸååˆ—è¡¨
    BROWSER_REQUIRED_DOMAINS = {
        'biorxiv.org': {'priority': 10, 'itemType': 'preprint', 'source': 'bioRxiv'},
        'medrxiv.org': {'priority': 10, 'itemType': 'preprint', 'source': 'medRxiv'},
        'chemrxiv.org': {'priority': 10, 'itemType': 'preprint', 'source': 'ChemRxiv'},
        'psyarxiv.com': {'priority': 10, 'itemType': 'preprint', 'source': 'PsyArXiv'},
        'osf.io': {'priority': 10, 'itemType': 'preprint', 'source': 'OSF'},
        'socarxiv.org': {'priority': 10, 'itemType': 'preprint', 'source': 'SocArXiv'},
    }
    
    def __init__(self):
        self.browser: Optional[Browser] = None
        self.context = None
    
    def get_database_name(self) -> str:
        """è¿”å›æ•°æ®åº“åç§°"""
        return "Browser-Driven"
    
    def requires_authentication(self) -> bool:
        """æ˜¯å¦éœ€è¦è®¤è¯"""
        return False
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError("Playwrightæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright && playwright install chromium")
        
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True,  # æ— å¤´æ¨¡å¼
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-setuid-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--disable-extensions',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-plugins',
                '--disable-default-apps',
                '--disable-background-timer-throttling',
                '--disable-renderer-backgrounding',
                '--disable-backgrounding-occluded-windows',
                '--disable-ipc-flooding-protection',
                '--single-process',
                '--no-zygote'
            ],
            ignore_default_args=['--enable-blink-features=IdleDetection'],
            timeout=30000  # 30ç§’å¯åŠ¨è¶…æ—¶
        )
        self.context = await self.browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            accept_downloads=True
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
    
    async def _download_biorxiv_with_mcp(self, browser_instance, pdf_url: str) -> Optional[bytes]:
        """
        ä½¿ç”¨MCPé«˜çº§æµè§ˆå™¨æŠ€æœ¯ä¸‹è½½bioRxiv PDF
        å·²éªŒè¯å¯ç»•è¿‡bioRxivåçˆ¬è™«æœºåˆ¶
        """
        try:
            logger.info("ğŸ§¬ å¯åŠ¨MCPé«˜çº§æµè§ˆå™¨ä¸‹è½½")
            
            # åˆ›å»ºæ–°çš„é«˜çº§é…ç½®é¡µé¢
            page = await browser_instance.context.new_page()
            
            # æ³¨å…¥é«˜çº§åæ£€æµ‹è„šæœ¬
            await page.add_init_script("""
                // ç§»é™¤webdriverå±æ€§
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                // ä¼ªé€ plugins
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [
                        {
                            name: 'Chrome PDF Plugin',
                            filename: 'internal-pdf-viewer',
                            description: 'Portable Document Format'
                        },
                        {
                            name: 'Chrome PDF Viewer', 
                            filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai',
                            description: ''
                        }
                    ],
                });
                
                // ä¼ªé€ languages
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['en-US', 'en'],
                });
                
                // ç§»é™¤automationæ ‡è®°
                delete Object.getPrototypeOf(navigator).webdriver;
            """)
            
            # å¤šæ­¥éª¤è®¿é—®ç­–ç•¥
            logger.info("  1. å…ˆè®¿é—®bioRxivä¸»é¡µå»ºç«‹ä¼šè¯...")
            # ä½¿ç”¨æ›´å®½æ¾çš„åŠ è½½ç­–ç•¥ï¼Œé¿å…networkidleåœ¨Cloudflareåœºæ™¯ä¸‹è¶…æ—¶
            try:
                await page.goto('https://www.biorxiv.org/', wait_until='domcontentloaded', timeout=15000)
            except Exception as e:
                logger.info(f"  â­ï¸ è·³è¿‡ä¸»é¡µç­‰å¾…ï¼ˆ{str(e)[:60]}ï¼‰")
            
            # æ¨¡æ‹Ÿäººç±»è¡Œä¸ºé—´éš”
            await asyncio.sleep(2)
            
            # ä¼˜å…ˆå°è¯•ç»è¿‡ä¼šè¯çš„å¤šç­–ç•¥ç›´æ¥ä¸‹è½½
            try:
                from typing import Optional as _Optional
                direct_pdf: _Optional[bytes] = await self._download_pdf_with_browser(page, pdf_url)
                if direct_pdf and direct_pdf.startswith(b'%PDF'):
                    await page.close()
                    logger.info(f"  âœ… MCPç›´æ¥è·å–PDFæˆåŠŸ: {len(direct_pdf):,} bytes")
                    return direct_pdf
            except Exception as e:
                logger.info(f"  âš ï¸ ç›´æ¥è·å–PDFå¤±è´¥ï¼Œå›é€€åˆ°ä¸‹è½½äº‹ä»¶: {e}")
            
            # è®¾ç½®ä¸‹è½½ç›‘å¬ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
            download_success = False
            pdf_content = None
            
            async def handle_biorxiv_download(download):
                nonlocal download_success, pdf_content
                try:
                    logger.info(f"  ğŸ¯ MCPæ£€æµ‹åˆ°ä¸‹è½½: {download.url}")
                    
                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                    import tempfile
                    temp_file = tempfile.NamedTemporaryFile(suffix='.pdf', delete=False)
                    temp_path = temp_file.name
                    temp_file.close()
                    
                    # ä¿å­˜ä¸‹è½½
                    await download.save_as(temp_path)
                    
                    # è¯»å–å†…å®¹
                    with open(temp_path, 'rb') as f:
                        pdf_content = f.read()
                    
                    # éªŒè¯PDFå¹¶æ¸…ç†
                    if pdf_content and pdf_content.startswith(b'%PDF'):
                        download_success = True
                        logger.info(f"  âœ… MCPä¸‹è½½æˆåŠŸ: {len(pdf_content):,} bytes")
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
                        
                except Exception as e:
                    logger.warning(f"  âš ï¸ MCPä¸‹è½½å¤„ç†å¼‚å¸¸: {e}")
            
            page.on("download", handle_biorxiv_download)
            
            # è§¦å‘ä¸‹è½½
            logger.info("  2. JavaScriptè§¦å‘PDFä¸‹è½½...")
            try:
                await page.evaluate(f"window.open('{pdf_url}', '_blank')")
            except:
                pass
            
            # ç­‰å¾…ä¸‹è½½
            logger.info("  3. ç­‰å¾…ä¸‹è½½å®Œæˆ...")
            for i in range(30):  # æœ€å¤šç­‰30ç§’
                if download_success:
                    break
                await asyncio.sleep(1)
            
            await page.close()
            
            if download_success and pdf_content:
                logger.info(f"ğŸ‰ MCP bioRxiv PDFä¸‹è½½æˆåŠŸ: {len(pdf_content):,} bytes")
                return pdf_content
            else:
                logger.warning("âš ï¸ MCP bioRxiv PDFä¸‹è½½è¶…æ—¶æˆ–å¤±è´¥")
                return None
                
        except Exception as e:
            logger.error(f"âŒ MCP bioRxivä¸‹è½½å¼‚å¸¸: {e}")
            return None
    
    def can_handle(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨æµè§ˆå™¨å¤„ç†"""
        if not PLAYWRIGHT_AVAILABLE:
            return False
            
        domain = urlparse(url).netloc.lower()
        for browser_domain in self.BROWSER_REQUIRED_DOMAINS:
            if browser_domain in domain:
                return True
        return False
    
    async def _download_pdf_content(self, pdf_url: str) -> Optional[bytes]:
        """
        ä½¿ç”¨æµè§ˆå™¨ç¯å¢ƒä¸‹è½½PDFå†…å®¹
        
        Args:
            pdf_url: PDFé“¾æ¥
            
        Returns:
            PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        if not self.browser:
            logger.error("æµè§ˆå™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¸‹è½½PDFå†…å®¹")
            return None
            
        try:
            # åˆ›å»ºæ–°é¡µé¢ç”¨äºä¸‹è½½
            page = await self.browser.new_page()
            
            # è®¾ç½®ä¸‹è½½ç›‘å¬
            download_info = None
            pdf_content = None
            
            async def handle_download(download):
                nonlocal download_info, pdf_content
                logger.info(f"ğŸ¯ æ£€æµ‹åˆ°ä¸‹è½½: {download.url}")
                download_info = download
                
                # è·å–ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                temp_path = await download.path()
                if temp_path:
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    with open(temp_path, 'rb') as f:
                        pdf_content = f.read()
                    logger.info(f"âœ… æˆåŠŸè¯»å–PDFå†…å®¹: {len(pdf_content)} bytes")
                else:
                    # å¦‚æœæ²¡æœ‰æ–‡ä»¶è·¯å¾„ï¼Œå°è¯•è·å–buffer
                    try:
                        pdf_content = await download.save_as(bytes)
                        logger.info(f"âœ… é€šè¿‡bufferè·å–PDFå†…å®¹: {len(pdf_content)} bytes")
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ— æ³•è·å–PDF buffer: {e}")
            
            # ç›‘å¬ä¸‹è½½äº‹ä»¶
            page.on("download", handle_download)
            
            # å¯¼èˆªåˆ°PDF URL
            logger.info(f"ğŸŒ æµè§ˆå™¨è®¿é—®PDF: {pdf_url}")
            
            try:
                # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´
                response = await page.goto(pdf_url, timeout=60000, wait_until='networkidle')
                
                if response and response.status == 200:
                    # æ£€æŸ¥Content-Typeæ˜¯å¦ä¸ºPDF
                    headers = await response.all_headers()
                    content_type = headers.get('content-type', '')
                    
                    if 'application/pdf' in content_type:
                        logger.info("ğŸ“„ æ£€æµ‹åˆ°PDFå“åº”ï¼Œå°è¯•è·å–å†…å®¹")
                        
                        # ç›´æ¥ä»å“åº”è·å–PDFå†…å®¹
                        pdf_content = await response.body()
                        logger.info(f"âœ… ç›´æ¥ä»å“åº”è·å–PDF: {len(pdf_content)} bytes")
                        
                        # éªŒè¯PDFå†…å®¹
                        if pdf_content and len(pdf_content) > 1024:  # è‡³å°‘1KB
                            if pdf_content.startswith(b'%PDF'):
                                await page.close()
                                return pdf_content
                            else:
                                logger.warning("âš ï¸ å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„PDFæ ¼å¼")
                        else:
                            logger.warning("âš ï¸ PDFå†…å®¹å¤ªå°æˆ–ä¸ºç©º")
                    else:
                        logger.warning(f"âš ï¸ å“åº”ä¸æ˜¯PDFç±»å‹: {content_type}")
                        
                        # å¦‚æœä¸æ˜¯ç›´æ¥çš„PDFå“åº”ï¼Œå¯èƒ½éœ€è¦è§¦å‘ä¸‹è½½
                        # ç­‰å¾…ä¸€æ®µæ—¶é—´çœ‹æ˜¯å¦æœ‰ä¸‹è½½äº‹ä»¶
                        await page.wait_for_timeout(5000)
                        
                        if pdf_content:
                            logger.info("âœ… é€šè¿‡ä¸‹è½½äº‹ä»¶è·å–PDFå†…å®¹")
                            await page.close()
                            return pdf_content
                
                else:
                    logger.error(f"âŒ PDFè®¿é—®å¤±è´¥: {response.status if response else 'No response'}")
                    
            except Exception as e:
                logger.error(f"âŒ PDFè®¿é—®å¼‚å¸¸: {e}")
                
            await page.close()
            return None
            
        except Exception as e:
            logger.error(f"âŒ PDFä¸‹è½½ä¸¥é‡å¼‚å¸¸: {e}")
            return None

    def extract_metadata(self, url: str) -> Dict[str, Any]:
        """
        æå–è®ºæ–‡å…ƒæ•°æ®ï¼Œå¯¹äºåçˆ¬è™«ç½‘ç«™è¿˜ä¼šä¸‹è½½PDFå†…å®¹
        
        Returns:
            åŒ…å«å…ƒæ•°æ®å’ŒPDFå†…å®¹çš„å­—å…¸
        """
        try:
            # ä½¿ç”¨å¼‚æ­¥åŒ…è£…å™¨
            return asyncio.run(self._async_extract_metadata_with_pdf(url))
        except Exception as e:
            logger.error(f"âŒ å…ƒæ•°æ®æå–å¼‚å¸¸: {e}")
            return {"error": f"æå–å¼‚å¸¸: {str(e)}"}

    async def _async_extract_metadata_with_pdf(self, url: str) -> Dict[str, Any]:
        """å¼‚æ­¥æå–å…ƒæ•°æ®å¹¶ä¸‹è½½PDFå†…å®¹ - ä¿®å¤ç‰ˆæœ¬"""
        if not self.can_handle(url):
            return {}
            
        logger.info(f"BrowserExtractor: ä½¿ç”¨æµè§ˆå™¨å¤„ç† {url}")
        
        try:
            # ğŸ”§ ä¿®å¤ï¼šåœ¨åŒä¸€ä¸ªæµè§ˆå™¨ä¼šè¯ä¸­å®Œæˆæ‰€æœ‰æ“ä½œ
            async with BrowserExtractor() as extractor_instance:
                page = await extractor_instance.context.new_page()
                
                # è®¾ç½®é¡µé¢å±æ€§ï¼Œæé«˜æˆåŠŸç‡
                await page.set_extra_http_headers({
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Upgrade-Insecure-Requests': '1'
                })
                
                # 1. é¦–å…ˆæå–å…ƒæ•°æ®
                result = await self._extract_metadata_from_page(page, url)
                
                if 'error' in result:
                    return result
                    
                # 2. åœ¨åŒä¸€ä¼šè¯ä¸­ä¸‹è½½PDF
                pdf_url = result.get('pdf_url')
                if pdf_url:
                    logger.info(f"ğŸ”„ å°è¯•ä¸‹è½½PDFå†…å®¹: {pdf_url}")
                    
                    # ğŸš€ MCPæµè§ˆå™¨PDFä¸‹è½½ - å·²éªŒè¯å¯è¡Œ
                    if 'biorxiv.org' in pdf_url.lower():
                        logger.info("ğŸ§¬ ä½¿ç”¨MCPé«˜çº§æµè§ˆå™¨ä¸‹è½½bioRxiv PDF")
                        pdf_content = await self._download_biorxiv_with_mcp(extractor_instance, pdf_url)
                    else:
                        logger.info("âš ï¸ ébioRxivç½‘ç«™ï¼Œè·³è¿‡æµè§ˆå™¨PDFä¸‹è½½")
                        pdf_content = None
                    
                    if pdf_content:
                        result['pdf_content'] = pdf_content
                        result['pdf_size'] = len(pdf_content)
                        logger.info(f"âœ… PDFå†…å®¹ä¸‹è½½æˆåŠŸ: {len(pdf_content)} bytes")
                    else:
                        logger.warning("âš ï¸ PDFå†…å®¹ä¸‹è½½å¤±è´¥ï¼Œä»…æä¾›é“¾æ¥")
                
                return result
                
        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨å…ƒæ•°æ®+PDFæå–å¼‚å¸¸: {e}")
            return {"error": f"æµè§ˆå™¨æå–å¼‚å¸¸: {str(e)}"}

    async def _async_extract_metadata(self, url: str) -> Dict[str, Any]:
        """ä½¿ç”¨æµè§ˆå™¨æå–å…ƒæ•°æ®å’ŒPDFé“¾æ¥"""
        if not self.can_handle(url):
            return {}
            
        logger.info(f"BrowserExtractor: ä½¿ç”¨æµè§ˆå™¨å¤„ç† {url}")
        
        try:
            # åˆ›å»ºä¸´æ—¶æµè§ˆå™¨å®ä¾‹
            async with BrowserExtractor() as extractor_instance:
                page = await extractor_instance.context.new_page()
                
                # è®¾ç½®é¡µé¢å±æ€§ï¼Œæé«˜æˆåŠŸç‡
                await page.set_extra_http_headers({
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Upgrade-Insecure-Requests': '1'
                })
                
                # ğŸš€ æ™ºèƒ½åçˆ¬è™«ç­–ç•¥
                await page.goto(url, wait_until='domcontentloaded', timeout=20000)
                
                # æ£€æŸ¥æ˜¯å¦é‡åˆ°"Just a moment"ç­‰å¾…é¡µé¢
                await asyncio.sleep(2)  # åˆå§‹ç­‰å¾…
                
                title = await page.title()
                if "just a moment" in title.lower() or "please wait" in title.lower():
                    logger.info("ğŸ”„ æ£€æµ‹åˆ°åçˆ¬è™«ç­‰å¾…é¡µé¢ï¼Œæ™ºèƒ½ç­‰å¾…ä¸­...")
                    
                    # æœ€å¤šç­‰å¾…30ç§’è®©é¡µé¢è‡ªç„¶åŠ è½½
                    for i in range(15):  # 15æ¬¡ï¼Œæ¯æ¬¡2ç§’
                        await asyncio.sleep(2)
                        new_title = await page.title()
                        if "just a moment" not in new_title.lower():
                            logger.info(f"âœ… åçˆ¬è™«é¡µé¢å·²é€šè¿‡ï¼Œæ–°æ ‡é¢˜: {new_title[:50]}...")
                            break
                        logger.info(f"â³ ç»§ç»­ç­‰å¾…åçˆ¬è™«æ£€æŸ¥... ({i+1}/15)")
                    
                    # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                    try:
                        # æ»šåŠ¨é¡µé¢
                        await page.evaluate("window.scrollTo(0, document.body.scrollHeight/2)")
                        await asyncio.sleep(1)
                        await page.evaluate("window.scrollTo(0, 0)")
                        await asyncio.sleep(1)
                    except:
                        pass
                
                # ç­‰å¾…æœ€ç»ˆçš„ç½‘ç»œç¨³å®š
                try:
                    await page.wait_for_load_state('networkidle', timeout=8000)
                except:
                    logger.info("ç½‘ç»œç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­å¤„ç†")
                
                # ğŸ”§ Windowså…¼å®¹æ€§ï¼šå¢å¼ºé¡µé¢ç¨³å®šæ€§æ£€æŸ¥
                try:
                    # æ£€æŸ¥é¡µé¢æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                    if page.is_closed():
                        logger.error("âš ï¸ é¡µé¢å·²å…³é—­ï¼Œæ— æ³•ç»§ç»­æå–")
                        return {"error": "é¡µé¢æ„å¤–å…³é—­"}
                        
                    # æœ€ç»ˆç¡®è®¤é¡µé¢æ ‡é¢˜
                    final_title = await page.title()
                    logger.info(f"ğŸ“„ æœ€ç»ˆé¡µé¢æ ‡é¢˜: {final_title[:70]}...")
                    
                    # æ‰§è¡ŒJavaScriptæ¥æå–å…ƒæ•°æ®å’ŒPDFé“¾æ¥
                    metadata = await page.evaluate(extractor_instance._get_extraction_script())
                except Exception as page_error:
                    logger.error(f"âš ï¸ é¡µé¢æ“ä½œå¤±è´¥: {page_error}")
                    # Windowsä¸Šå¸¸è§çš„é¡µé¢å…³é—­é”™è¯¯ï¼Œå°è¯•é‡æ–°è·å–
                    return {"error": f"é¡µé¢æ“ä½œå¤±è´¥: {page_error}"}
                
                # è¯†åˆ«åŸŸåç±»å‹
                domain_info = extractor_instance._identify_domain(url)
                metadata.update(domain_info)
                
                # æŸ¥æ‰¾PDFé“¾æ¥
                pdf_urls = await extractor_instance._find_pdf_links(page, url)
                if pdf_urls:
                    pdf_url = pdf_urls[0]
                    metadata['pdf_url'] = pdf_url
                    logger.info(f"BrowserExtractor: æ‰¾åˆ°PDFé“¾æ¥ {pdf_url}")
                
                # ç¡®ä¿é¡µé¢æ­£å¸¸å…³é—­
                try:
                    if not page.is_closed():
                        await page.close()
                except Exception:
                    pass  # å¿½ç•¥å…³é—­é”™è¯¯
                    
                return metadata
             
        except PlaywrightTimeoutError:
            logger.error(f"BrowserExtractor: é¡µé¢åŠ è½½è¶…æ—¶ {url}")
            return {}
        except Exception as e:
            logger.error(f"BrowserExtractor: æå–å¤±è´¥ {url} - {e}")
            return {}
    
    def _identify_domain(self, url: str) -> Dict[str, Any]:
        """è¯†åˆ«åŸŸåç±»å‹"""
        domain = urlparse(url).netloc.lower()
        for browser_domain, info in self.BROWSER_REQUIRED_DOMAINS.items():
            if browser_domain in domain:
                return {
                    'itemType': info['itemType'],
                    'source': info['source'],
                    'priority': info['priority']
                }
        return {}
    
    def _get_extraction_script(self) -> str:
        """è¿”å›åœ¨é¡µé¢ä¸­æ‰§è¡Œçš„JavaScriptä»£ç """
        return """
        () => {
            const metadata = {};
            
            // æå–åŸºæœ¬å…ƒæ•°æ®
            metadata.title = document.title || '';
            
            // æå–citationæ ‡ç­¾
            const citationFields = {
                'citation_title': 'title',
                'citation_author': 'authors',
                'citation_date': 'date',
                'citation_publication_date': 'date',
                'citation_online_date': 'date',
                'citation_doi': 'DOI',
                'citation_abstract_html_url': 'url',
                'citation_pdf_url': 'pdf_url',
                'citation_fulltext_pdf_url': 'pdf_url',
                'citation_preprint_server': 'publicationTitle',
                'citation_archive_id': 'archiveID'
            };
            
            for (const [citationField, metaField] of Object.entries(citationFields)) {
                const elements = document.querySelectorAll(`meta[name="${citationField}"]`);
                if (elements.length > 0) {
                    if (citationField === 'citation_author') {
                        metadata.authors = Array.from(elements).map(el => el.content).join('; ');
                    } else {
                        metadata[metaField] = elements[0].content;
                    }
                }
            }
            
            // æå–Dublin Coreæ ‡ç­¾
            const dcFields = {
                'DC.title': 'title',
                'DC.creator': 'authors',
                'DC.date': 'date',
                'DC.identifier': 'DOI'
            };
            
            for (const [dcField, metaField] of Object.entries(dcFields)) {
                const element = document.querySelector(`meta[name="${dcField}"]`);
                if (element && !metadata[metaField]) {
                    metadata[metaField] = element.content;
                }
            }
            
            // æå–JSON-LD
            const jsonLdScripts = document.querySelectorAll('script[type="application/ld+json"]');
            for (const script of jsonLdScripts) {
                try {
                    const data = JSON.parse(script.textContent);
                    if (data['@type'] === 'ScholarlyArticle' || data['@type'] === 'Article') {
                        if (data.name && !metadata.title) metadata.title = data.name;
                        if (data.headline && !metadata.title) metadata.title = data.headline;
                        if (data.datePublished && !metadata.date) metadata.date = data.datePublished;
                        if (data.identifier && !metadata.DOI) {
                            if (Array.isArray(data.identifier)) {
                                const doiIdentifier = data.identifier.find(id => id.propertyID === 'doi' || id['@type'] === 'PropertyValue');
                                if (doiIdentifier) metadata.DOI = doiIdentifier.value;
                            } else if (typeof data.identifier === 'string' && data.identifier.startsWith('10.')) {
                                metadata.DOI = data.identifier;
                            }
                        }
                        if (data.author && !metadata.authors) {
                            if (Array.isArray(data.author)) {
                                metadata.authors = data.author.map(a => a.name || a).join('; ');
                            }
                        }
                        // æ£€æŸ¥encodingå­—æ®µä¸­çš„PDF
                        if (data.encoding && Array.isArray(data.encoding)) {
                            for (const encoding of data.encoding) {
                                if (encoding.encodingFormat === 'application/pdf' && encoding.contentUrl) {
                                    metadata.pdf_url = encoding.contentUrl;
                                    break;
                                }
                            }
                        }
                    }
                } catch (e) {
                    // å¿½ç•¥JSONè§£æé”™è¯¯
                }
            }
            
            // ğŸ¯ å¢å¼ºæ ‡é¢˜æå–é€»è¾‘ - é’ˆå¯¹é¢„å°æœ¬ç½‘ç«™
            if (!metadata.title) {
                // é’ˆå¯¹bioRxiv/medRxivçš„ç‰¹æ®Šé€‰æ‹©å™¨
                const titleSelectors = [
                    'h1.highwire-cite-title',           // bioRxiv/medRxivä¸»æ ‡é¢˜
                    'h1#page-title',                    // é¡µé¢æ ‡é¢˜
                    'h1.article-title',                 // æ–‡ç« æ ‡é¢˜
                    '.article-title h1',                // æ–‡ç« æ ‡é¢˜å®¹å™¨å†…çš„h1
                    'h1.entry-title',                   // æ¡ç›®æ ‡é¢˜
                    '.paper-title h1',                  // è®ºæ–‡æ ‡é¢˜
                    '.title h1',                        // æ ‡é¢˜å®¹å™¨
                    'h1',                               // é€šç”¨h1
                    '.highwire-cite-title',             // é«˜çº¿å¼•ç”¨æ ‡é¢˜ï¼ˆéh1ï¼‰
                    '.article-title',                   // æ–‡ç« æ ‡é¢˜ï¼ˆéh1ï¼‰
                    '.paper-title'                      // è®ºæ–‡æ ‡é¢˜ï¼ˆéh1ï¼‰
                ];
                
                for (const selector of titleSelectors) {
                    const titleEl = document.querySelector(selector);
                    if (titleEl) {
                        let title = titleEl.textContent.trim();
                        // æ¸…ç†æ ‡é¢˜
                        title = title.replace(/\s+/g, ' ');
                        title = title.replace(/^\s*[-â€“]\s*/, ''); // ç§»é™¤å¼€å¤´ç ´æŠ˜å·
                        if (title && title.length > 10) {
                            metadata.title = title;
                            console.log('ğŸ¯ æµè§ˆå™¨æå–å™¨æ‰¾åˆ°æ ‡é¢˜:', title, 'ä½¿ç”¨é€‰æ‹©å™¨:', selector);
                            break;
                        }
                    }
                }
            }
            
            // ğŸ¯ é’ˆå¯¹chemRxivçš„ç‰¹æ®Šå¤„ç†
            if (!metadata.title && window.location.hostname.includes('chemrxiv')) {
                const chemSelectors = [
                    '.article-title',
                    '.paper-title', 
                    '.manuscript-title',
                    'h1[class*="title"]',
                    '.content-title'
                ];
                
                for (const selector of chemSelectors) {
                    const titleEl = document.querySelector(selector);
                    if (titleEl) {
                        let title = titleEl.textContent.trim();
                        if (title && title.length > 10) {
                            metadata.title = title;
                            console.log('ğŸ§ª ChemRxivæ ‡é¢˜æå–:', title);
                            break;
                        }
                    }
                }
            }
            
            return metadata;
        }
        """
    
    async def _find_pdf_links(self, page: Page, base_url: str) -> List[str]:
        """åœ¨é¡µé¢ä¸­æŸ¥æ‰¾PDFé“¾æ¥"""
        pdf_urls = []
        
        try:
            # æ–¹æ³•1: æŸ¥æ‰¾PDFé“¾æ¥å…ƒç´ 
            pdf_links = await page.eval_on_selector_all('a[href*=".pdf"], a[href*="pdf"], link[type="application/pdf"]', """
                elements => elements.map(el => el.href || el.getAttribute('href')).filter(href => href)
            """)
            
            for link in pdf_links:
                if link and not link.startswith('javascript:'):
                    full_url = urljoin(base_url, link)
                    pdf_urls.append(full_url)
            
            # æ–¹æ³•2: é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„PDFæŸ¥æ‰¾ç­–ç•¥
            domain = urlparse(base_url).netloc.lower()
            
            if 'biorxiv.org' in domain or 'medrxiv.org' in domain:
                # bioRxiv/medRxiv: å°è¯•ä»URLæ„é€ PDFé“¾æ¥
                doi_match = re.search(r'/content/(?:early/)?(?:\d{4}/\d{2}/\d{2}/)?(?:10\.1101/)?([^v/]+)', base_url)
                if doi_match:
                    doi = doi_match.group(1)
                    pdf_url = f"https://{urlparse(base_url).netloc}/content/10.1101/{doi}v1.full.pdf"
                    pdf_urls.append(pdf_url)
                
            elif 'chemrxiv.org' in domain:
                # ChemRxiv: æŸ¥æ‰¾ç‰¹å®šçš„ä¸‹è½½æŒ‰é’®æˆ–é“¾æ¥
                download_links = await page.eval_on_selector_all('a[href*="download"], button[onclick*="download"]', """
                    elements => elements.map(el => el.href || el.getAttribute('onclick')).filter(href => href)
                """)
                for link in download_links:
                    if 'pdf' in link.lower():
                        pdf_urls.append(link)
                        
            elif 'osf.io' in domain:
                # OSFç³»åˆ—: æŸ¥æ‰¾downloadé“¾æ¥
                download_links = await page.eval_on_selector_all('a[href*="/download"]', """
                    elements => elements.map(el => el.href).filter(href => href)
                """)
                pdf_urls.extend(download_links)
            
            # æ–¹æ³•3: æŸ¥æ‰¾é¡µé¢ä¸­æ‰€æœ‰å¯èƒ½çš„PDFç›¸å…³æŒ‰é’®
            button_links = await page.eval_on_selector_all('button, a.btn, .download-btn, .pdf-btn', """
                elements => {
                    const results = [];
                    elements.forEach(el => {
                        const text = el.textContent.toLowerCase();
                        const href = el.href;
                        const onclick = el.getAttribute('onclick') || '';
                        
                        if (text.includes('pdf') || text.includes('download') || onclick.includes('pdf')) {
                            if (href) results.push(href);
                            if (onclick && onclick.includes('http')) {
                                const urlMatch = onclick.match(/https?:\/\/[^'"\\s)]+/);
                                if (urlMatch) results.push(urlMatch[0]);
                            }
                        }
                    });
                    return results;
                }
            """)
            pdf_urls.extend(button_links)
            
            # å»é‡å¹¶è¿”å›
            unique_urls = []
            seen = set()
            for url in pdf_urls:
                if url and url not in seen:
                    seen.add(url)
                    unique_urls.append(url)
            
            logger.debug(f"BrowserExtractor: æ‰¾åˆ° {len(unique_urls)} ä¸ªPDFå€™é€‰é“¾æ¥")
            return unique_urls
            
        except Exception as e:
            logger.error(f"BrowserExtractor: PDFé“¾æ¥æŸ¥æ‰¾å¤±è´¥ - {e}")
            return []
    
    async def _download_pdf_with_browser(self, page, pdf_url: str) -> Optional[bytes]:
        """ä½¿ç”¨æµè§ˆå™¨ä¼šè¯ä¸‹è½½PDFå†…å®¹ - å¢å¼ºç‰ˆ"""
        try:
            logger.info(f"ğŸ”„ å¤šç­–ç•¥PDFä¸‹è½½: {pdf_url}")
            
            # ç­–ç•¥1: ä½¿ç”¨å·²éªŒè¯çš„æµè§ˆå™¨ä¼šè¯ï¼Œæ·»åŠ å®Œæ•´è¯·æ±‚å¤´
            try:
                # è·å–å½“å‰é¡µé¢çš„URLä½œä¸ºReferer
                referer = page.url
                
                # æ„å»ºå®Œæ•´çš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
                headers = {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Referer': referer,
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'same-origin',
                    'Sec-Fetch-User': '?1',
                    'Upgrade-Insecure-Requests': '1',
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
                
                logger.info(f"ğŸ”„ ç­–ç•¥1: ä½¿ç”¨å®Œæ•´è¯·æ±‚å¤´ä¸‹è½½PDF")
                response = await page.context.request.get(pdf_url, headers=headers, timeout=45000)
                
                if response.status == 200:
                    content = await response.body()
                    if content and len(content) > 1024 and content.startswith(b'%PDF'):
                        logger.info(f"âœ… ç­–ç•¥1æˆåŠŸ: å®Œæ•´è¯·æ±‚å¤´ä¸‹è½½ ({len(content)} bytes)")
                        return content
                    else:
                        logger.info(f"ç­–ç•¥1: å†…å®¹ä¸æ˜¯æœ‰æ•ˆPDF ({len(content) if content else 0} bytes)")
                else:
                    logger.info(f"ç­–ç•¥1å¤±è´¥: HTTP {response.status}")
                    
            except Exception as e:
                logger.info(f"ç­–ç•¥1å¼‚å¸¸: {e}")
            
            # ç­–ç•¥2: å°è¯•ä¸åŒçš„PDF URLæ ¼å¼
            alternative_urls = self._generate_alternative_pdf_urls(pdf_url)
            for alt_url in alternative_urls:
                try:
                    logger.info(f"ğŸ”„ æµ‹è¯•å¤‡ç”¨URL: {alt_url}")
                    response = await page.context.request.get(alt_url, timeout=15000)
                    if response.status == 200:
                        content = await response.body()
                        if content and content.startswith(b'%PDF'):
                            logger.info(f"âœ… ç­–ç•¥2æˆåŠŸ: å¤‡ç”¨URL ({len(content)} bytes)")
                            return content
                except Exception as e:
                    logger.debug(f"å¤‡ç”¨URLå¤±è´¥: {alt_url} - {e}")
            
            # ç­–ç•¥3: æ•è·æµè§ˆå™¨ä¸‹è½½äº‹ä»¶ï¼ˆå…³é”®çªç ´ï¼ï¼‰
            try:
                logger.info("ğŸ”„ ç­–ç•¥3: æ•è·æµè§ˆå™¨ä¸‹è½½äº‹ä»¶")
                pdf_page = await page.context.new_page()
                
                # è®¾ç½®ä¸‹è½½ç›‘å¬å™¨
                download_content = None
                download_event = asyncio.Event()
                
                def handle_download(download):
                    logger.info(f"âœ… æ•è·åˆ°ä¸‹è½½äº‹ä»¶: {download.url}")
                    # è¿™é‡Œæˆ‘ä»¬éœ€è¦å¼‚æ­¥å¤„ç†ä¸‹è½½
                    asyncio.create_task(save_download(download))
                
                async def save_download(download):
                    nonlocal download_content
                    try:
                        # ç­‰å¾…ä¸‹è½½å®Œæˆå¹¶è·å–å†…å®¹
                        await download.save_as('/tmp/temp_pdf.pdf')  # ä¸´æ—¶ä¿å­˜
                        
                        # è¯»å–ä¸‹è½½çš„å†…å®¹
                        with open('/tmp/temp_pdf.pdf', 'rb') as f:
                            download_content = f.read()
                        
                        logger.info(f"âœ… ä¸‹è½½å†…å®¹è·å–æˆåŠŸ: {len(download_content)} bytes")
                        download_event.set()
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        import os
                        os.remove('/tmp/temp_pdf.pdf')
                        
                    except Exception as e:
                        logger.warning(f"ä¸‹è½½å¤„ç†å¤±è´¥: {e}")
                        download_event.set()
                
                # æ³¨å†Œä¸‹è½½ç›‘å¬å™¨
                pdf_page.on('download', handle_download)
                
                # å¯¼èˆªåˆ°PDF URLï¼Œè¿™ä¼šè§¦å‘ä¸‹è½½
                try:
                    await pdf_page.goto(pdf_url, timeout=20000)
                except Exception as e:
                    # å¦‚æœå‡ºç°"Download is starting"é”™è¯¯ï¼Œè¿™å®é™…ä¸Šæ˜¯å¥½äº‹
                    if "download is starting" in str(e).lower():
                        logger.info("ğŸ¯ æ£€æµ‹åˆ°ä¸‹è½½å¼€å§‹ï¼Œç­‰å¾…ä¸‹è½½å®Œæˆ...")
                
                # ç­‰å¾…ä¸‹è½½äº‹ä»¶ï¼Œæœ€å¤š20ç§’
                try:
                    await asyncio.wait_for(download_event.wait(), timeout=20.0)
                    if download_content and download_content.startswith(b'%PDF'):
                        logger.info(f"ğŸ‰ ç­–ç•¥3æˆåŠŸ: ä¸‹è½½äº‹ä»¶æ•è· ({len(download_content)} bytes)")
                        await pdf_page.close()
                        return download_content
                except asyncio.TimeoutError:
                    logger.info("ç­–ç•¥3: ä¸‹è½½ç­‰å¾…è¶…æ—¶")
                
                await pdf_page.close()
                
            except Exception as e:
                logger.info(f"ç­–ç•¥3å¤±è´¥: {e}")
            
            # ç­–ç•¥4: å»¶è¿Ÿåé‡è¯•ç¬¬ä¸€ç§æ–¹æ³•ï¼ˆæœ‰æ—¶éœ€è¦æ—¶é—´ï¼‰
            try:
                logger.info("ğŸ”„ ç­–ç•¥4: å»¶è¿Ÿé‡è¯•")
                await asyncio.sleep(2)  # ç­‰å¾…2ç§’
                response = await page.context.request.get(pdf_url, timeout=20000)
                if response.status == 200:
                    content = await response.body()
                    if content and content.startswith(b'%PDF'):
                        logger.info(f"âœ… ç­–ç•¥4æˆåŠŸ: å»¶è¿Ÿé‡è¯• ({len(content)} bytes)")
                        return content
            except Exception as e:
                logger.info(f"ç­–ç•¥4å¤±è´¥: {e}")
            
            logger.warning(f"æ‰€æœ‰PDFä¸‹è½½ç­–ç•¥éƒ½å¤±è´¥äº†: {pdf_url}")
            return None
                
        except Exception as e:
            logger.error(f"æµè§ˆå™¨PDFä¸‹è½½ä¸¥é‡å¼‚å¸¸: {e}")
            return None
    
    def _generate_alternative_pdf_urls(self, original_url: str) -> List[str]:
        """ç”Ÿæˆå¯èƒ½çš„PDF URLå˜ä½“"""
        alternatives = []
        
        if 'biorxiv.org' in original_url:
            # bioRxivçš„å¤šç§URLæ ¼å¼
            import re
            
            # æå–DOI
            doi_match = re.search(r'(\d{4}\.\d{2}\.\d{2}\.\d{6})', original_url)
            if doi_match:
                doi = doi_match.group(1)
                alternatives.extend([
                    f"https://www.biorxiv.org/content/biorxiv/early/{doi[:4]}/{doi[5:7]}/{doi[8:10]}/{doi}.full.pdf",
                    f"https://www.biorxiv.org/content/early/{doi[:4]}/{doi[5:7]}/{doi[8:10]}/{doi}v1.full.pdf",
                    f"https://www.biorxiv.org/highwire/filestream/{doi}",
                    original_url.replace('.full.pdf', '.pdf'),
                    original_url.replace('v1.full.pdf', '.full.pdf')
                ])
        
        return alternatives 