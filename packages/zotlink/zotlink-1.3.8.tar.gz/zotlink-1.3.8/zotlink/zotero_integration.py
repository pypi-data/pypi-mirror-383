"""
ğŸ”— ZotLink Zoteroé›†æˆæ¨¡å—

æ‰©å±•ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šç§å­¦æœ¯æ•°æ®åº“ï¼š
- arXivï¼ˆæ— éœ€è®¤è¯ï¼‰
- Natureï¼ˆéœ€è¦cookiesï¼‰
- æ›´å¤šæ•°æ®åº“ï¼ˆå¯æ‰©å±•ï¼‰
"""

import requests
import json
import time
import re
import sqlite3
import tempfile
import shutil
import os
from pathlib import Path
from typing import Dict, List, Optional
import logging
import asyncio
from datetime import datetime

# å¯¼å…¥æå–å™¨ç®¡ç†å™¨
try:
    from .extractors.extractor_manager import ExtractorManager
    EXTRACTORS_AVAILABLE = True
except ImportError:
    try:
        # å¤‡ç”¨å¯¼å…¥è·¯å¾„
        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent))
        from extractors.extractor_manager import ExtractorManager
        EXTRACTORS_AVAILABLE = True
    except ImportError:
        EXTRACTORS_AVAILABLE = False
        logging.warning("âš ï¸ æå–å™¨ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä»…æ”¯æŒarXiv")

logger = logging.getLogger(__name__)


class ZoteroConnector:
    """ZotLinkçš„Zoteroè¿æ¥å™¨ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¿æ¥å™¨"""
        self.base_url = "http://127.0.0.1:23119"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Content-Type': 'application/json'
        })
        
        # åˆå§‹åŒ–é…ç½®ä¸æ•°æ®åº“è·¯å¾„
        self._zotero_storage_dir: Optional[Path] = None
        self._zotero_db_override: Optional[Path] = None
        self._load_config_overrides()
        self._zotero_db_path = self._find_zotero_database()
        
        # åˆå§‹åŒ–æå–å™¨ç®¡ç†å™¨
        if EXTRACTORS_AVAILABLE:
            self.extractor_manager = ExtractorManager()
            logger.info("âœ… æå–å™¨ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.extractor_manager = None
            logger.warning("âš ï¸ æå–å™¨ç®¡ç†å™¨ä¸å¯ç”¨")

    def _load_config_overrides(self) -> None:
        """ä»ç¯å¢ƒå˜é‡ä¸é…ç½®æ–‡ä»¶åŠ è½½Zoteroè·¯å¾„è¦†ç›–è®¾ç½®ã€‚
        ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > Claudeé…ç½® > æœ¬åœ°é…ç½®æ–‡ä»¶ > é»˜è®¤æ¢æµ‹
        æ”¯æŒï¼š
          - ç¯å¢ƒå˜é‡ ZOTLINK_ZOTERO_ROOT æŒ‡å®šZoteroæ ¹ç›®å½•ï¼ˆæ¨èï¼Œè‡ªåŠ¨æ¨å¯¼æ•°æ®åº“å’Œå­˜å‚¨è·¯å¾„ï¼‰
          - ç¯å¢ƒå˜é‡ ZOTLINK_ZOTERO_DB æŒ‡å®šæ•°æ®åº“å®Œæ•´è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
          - ç¯å¢ƒå˜é‡ ZOTLINK_ZOTERO_DIR æŒ‡å®šstorageç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
          - é€šè¿‡MCPç¯å¢ƒå˜é‡ä¼ é€’çš„é…ç½®
          - é…ç½®æ–‡ä»¶ ~/.zotlink/config.json ä¸­çš„ zotero.database_path / zotero.storage_dir
        """
        try:
            # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦è®¾ç½®äº†Zoteroæ ¹ç›®å½•ï¼ˆæ¨èæ–¹å¼ï¼‰
            env_root = os.environ.get('ZOTLINK_ZOTERO_ROOT', '').strip()
            if env_root:
                root_path = Path(os.path.expanduser(env_root))
                if root_path.exists():
                    # è‡ªåŠ¨æ¨å¯¼æ•°æ®åº“å’Œå­˜å‚¨è·¯å¾„
                    candidate_db = root_path / "zotero.sqlite"
                    candidate_storage = root_path / "storage"
                    
                    if candidate_db.exists():
                        self._zotero_db_override = candidate_db
                        logger.info(f"ğŸ”§ ä»Zoteroæ ¹ç›®å½•è‡ªåŠ¨æ¨å¯¼æ•°æ®åº“è·¯å¾„: {candidate_db}")
                    
                    if candidate_storage.exists():
                        self._zotero_storage_dir = candidate_storage
                        logger.info(f"ğŸ”§ ä»Zoteroæ ¹ç›®å½•è‡ªåŠ¨æ¨å¯¼å­˜å‚¨ç›®å½•: {candidate_storage}")
                    
                    if not candidate_db.exists() and not candidate_storage.exists():
                        logger.warning(f"âš ï¸ Zoteroæ ¹ç›®å½• {root_path} ä¸‹æœªæ‰¾åˆ°é¢„æœŸçš„æ•°æ®åº“æˆ–å­˜å‚¨ç›®å½•")
                else:
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_ROOTç›®å½•ä¸å­˜åœ¨: {root_path}")
            
            # 2. ç¯å¢ƒå˜é‡ä¼˜å…ˆï¼ˆå‘åå…¼å®¹ï¼Œä¼šè¦†ç›–æ ¹ç›®å½•æ¨å¯¼çš„ç»“æœï¼‰
            env_db = os.environ.get('ZOTLINK_ZOTERO_DB', '').strip()
            if env_db:
                candidate = Path(os.path.expanduser(env_db))
                if candidate.exists():
                    self._zotero_db_override = candidate
                    logger.info(f"ğŸ”§ ä½¿ç”¨ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DBè¦†ç›–Zoteroæ•°æ®åº“è·¯å¾„: {candidate}")
                else:
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DBè·¯å¾„ä¸å­˜åœ¨: {candidate}")
            
            env_storage = os.environ.get('ZOTLINK_ZOTERO_DIR', '').strip()
            if env_storage:
                storage_path = Path(os.path.expanduser(env_storage))
                if storage_path.exists():
                    self._zotero_storage_dir = storage_path
                    logger.info(f"ğŸ”§ ä½¿ç”¨ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DIRæŒ‡å®šstorageç›®å½•: {storage_path}")
                else:
                    logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ZOTLINK_ZOTERO_DIRç›®å½•ä¸å­˜åœ¨: {storage_path}")

            # Claudeé…ç½®æ–‡ä»¶ï¼ˆè‹¥æœªé€šè¿‡ç¯å¢ƒå˜é‡è®¾å®šï¼‰
            self._load_claude_config()

            # æœ¬åœ°é…ç½®æ–‡ä»¶ï¼ˆè‹¥å‰é¢æ–¹å¼éƒ½æœªè®¾å®šï¼‰
            config_file = Path.home() / '.zotlink' / 'config.json'
            if config_file.exists():
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        cfg = json.load(f)
                    zotero_cfg = cfg.get('zotero', {}) if isinstance(cfg, dict) else {}

                    if not self._zotero_db_override:
                        cfg_db = zotero_cfg.get('database_path', '').strip()
                        if cfg_db:
                            cfg_db_path = Path(os.path.expanduser(cfg_db))
                            if cfg_db_path.exists():
                                self._zotero_db_override = cfg_db_path
                                logger.info(f"ğŸ”§ ä½¿ç”¨é…ç½®æ–‡ä»¶è¦†ç›–Zoteroæ•°æ®åº“è·¯å¾„: {cfg_db_path}")
                            else:
                                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸­database_pathä¸å­˜åœ¨: {cfg_db_path}")

                    if not self._zotero_storage_dir:
                        cfg_storage = zotero_cfg.get('storage_dir', '').strip()
                        if cfg_storage:
                            cfg_storage_path = Path(os.path.expanduser(cfg_storage))
                            if cfg_storage_path.exists():
                                self._zotero_storage_dir = cfg_storage_path
                                logger.info(f"ğŸ”§ ä½¿ç”¨é…ç½®æ–‡ä»¶æŒ‡å®šstorageç›®å½•: {cfg_storage_path}")
                            else:
                                logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸­storage_dirä¸å­˜åœ¨: {cfg_storage_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½Zoteroè·¯å¾„è¦†ç›–è®¾ç½®å¤±è´¥: {e}")

    def _load_claude_config(self) -> None:
        """ä»Claudeé…ç½®æ–‡ä»¶åŠ è½½Zoteroè·¯å¾„è®¾ç½®ã€‚
        æ”¯æŒmacOS/Linuxå’ŒWindowsçš„Claudeé…ç½®è·¯å¾„ã€‚
        """
        try:
            # Claudeé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šå¹³å°ï¼‰
            claude_config_paths = [
                Path.home() / "Library" / "Application Support" / "Claude" / "claude_desktop_config.json",  # macOS
                Path.home() / ".config" / "claude" / "claude_desktop_config.json",                          # Linux
                Path.home() / "AppData" / "Roaming" / "Claude" / "claude_desktop_config.json"              # Windows
            ]
            
            for config_path in claude_config_paths:
                if config_path.exists():
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            claude_config = json.load(f)
                        
                        # æŸ¥æ‰¾zotlinkæœåŠ¡å™¨é…ç½®
                        mcp_servers = claude_config.get('mcpServers', {})
                        zotlink_config = mcp_servers.get('zotlink', {})
                        
                        # Claudeé…ç½®æ–‡ä»¶å­˜åœ¨ï¼Œè®°å½•ä½†ä¸å†è¯»å–éæ ‡å‡†MCPå­—æ®µ
                        # æ¨èä½¿ç”¨envç¯å¢ƒå˜é‡æ–¹å¼é…ç½®Zoteroè·¯å¾„
                        logger.debug(f"ğŸ“– æ‰¾åˆ°Claudeé…ç½®æ–‡ä»¶: {config_path}")
                        logger.info("ğŸ’¡ æ¨èåœ¨MCPé…ç½®ä¸­ä½¿ç”¨envç¯å¢ƒå˜é‡è®¾ç½®Zoteroè·¯å¾„")
                        break
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ è¯»å–Claudeé…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")
                        
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½Claudeé…ç½®å¤±è´¥: {e}")
    
    def _extract_arxiv_metadata(self, arxiv_url: str) -> Dict:
        """ä»arxiv URLæå–è¯¦ç»†çš„è®ºæ–‡å…ƒæ•°æ®"""
        try:
            # æå–arxiv ID
            arxiv_id_match = re.search(r'arxiv\.org/(abs|pdf)/([^/?]+)', arxiv_url)
            if not arxiv_id_match:
                return {"error": "æ— æ³•è§£æarxiv ID"}
            
            arxiv_id = arxiv_id_match.group(2)
            logger.info(f"æå–arxiv ID: {arxiv_id}")
            
            # è·å–arxivæ‘˜è¦é¡µé¢
            abs_url = f"https://arxiv.org/abs/{arxiv_id}"
            response = self.session.get(abs_url, timeout=10)
            
            if response.status_code != 200:
                return {"error": f"æ— æ³•è®¿é—®arxivé¡µé¢: {response.status_code}"}
            
            html_content = response.text
            
            # æå–è®ºæ–‡ä¿¡æ¯
            metadata = {
                'arxiv_id': arxiv_id,
                'abs_url': abs_url,
                'pdf_url': f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            }
            
            # æå–æ ‡é¢˜
            title_match = re.search(r'<meta name="citation_title" content="([^"]+)"', html_content)
            if title_match:
                metadata['title'] = title_match.group(1)
            else:
                # å¤‡é€‰æ–¹å¼
                title_match = re.search(r'<h1[^>]*class="title[^"]*"[^>]*>([^<]+)</h1>', html_content)
                if title_match:
                    metadata['title'] = title_match.group(1).replace('Title:', '').strip()
            
            # æå–ä½œè€… - æ”¹è¿›ç‰ˆæœ¬
            authors = []
            
            # æ–¹æ³•1: ä½¿ç”¨citation_authorå…ƒæ•°æ®ï¼ˆæœ€å‡†ç¡®ï¼‰
            author_matches = re.findall(r'<meta name="citation_author" content="([^"]+)"', html_content)
            if author_matches:
                authors = author_matches
            else:
                # æ–¹æ³•2: ä»ä½œè€…é“¾æ¥ä¸­æå–
                author_section = re.search(r'<div[^>]*class="[^"]*authors[^"]*"[^>]*>(.*?)</div>', html_content, re.DOTALL)
                if author_section:
                    # æå–æ‰€æœ‰ä½œè€…é“¾æ¥
                    author_links = re.findall(r'<a[^>]*href="/search/\?searchtype=author[^"]*">([^<]+)</a>', author_section.group(1))
                    if author_links:
                        authors = [author.strip() for author in author_links]
            
            # æ ¼å¼åŒ–ä½œè€…åˆ—è¡¨ - ç¡®ä¿æ­£ç¡®çš„å§“åæ ¼å¼
            if authors:
                formatted_authors = []
                for author in authors:
                    # å¦‚æœæ˜¯ "Last, First" æ ¼å¼ï¼Œä¿æŒä¸å˜
                    if ',' in author:
                        formatted_authors.append(author.strip())
                    else:
                        # å¦‚æœæ˜¯ "First Last" æ ¼å¼ï¼Œè½¬æ¢ä¸º "Last, First"
                        parts = author.strip().split()
                        if len(parts) >= 2:
                            last_name = parts[-1]
                            first_names = ' '.join(parts[:-1])
                            formatted_authors.append(f"{last_name}, {first_names}")
                        else:
                            formatted_authors.append(author.strip())
                
                metadata['authors'] = formatted_authors
                metadata['authors_string'] = '; '.join(formatted_authors)  # ä½¿ç”¨åˆ†å·åˆ†éš”ï¼Œæ›´æ ‡å‡†
            else:
                metadata['authors'] = []
                metadata['authors_string'] = ''
            
            # æå–æ‘˜è¦ - æ”¹è¿›ç‰ˆæœ¬
            abstract = None
            
            # å…ˆå°è¯•æ‰¾åˆ°æ‘˜è¦åŒºåŸŸ
            abstract_section = re.search(r'<blockquote[^>]*class="abstract[^"]*"[^>]*>(.*?)</blockquote>', html_content, re.DOTALL)
            if abstract_section:
                abstract_html = abstract_section.group(1)
                
                # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                abstract_text = re.sub(r'<[^>]+>', ' ', abstract_html)
                abstract_text = re.sub(r'\s+', ' ', abstract_text).strip()
                
                # ç§»é™¤"Abstract:"æ ‡è¯†ç¬¦
                if abstract_text.startswith('Abstract:'):
                    abstract_text = abstract_text[9:].strip()
                
                # è¿‡æ»¤æ‰arXivLabsç›¸å…³å†…å®¹ï¼ˆé€šå¸¸åœ¨æ‘˜è¦æœ€åï¼‰
                lines = abstract_text.split('.')
                filtered_lines = []
                
                for line in lines:
                    line = line.strip()
                    if not any(keyword in line.lower() for keyword in 
                             ['arxivlabs', 'framework that allows', 'collaborators to develop', 
                              'new arxiv features', 'directly on our website']):
                        filtered_lines.append(line)
                    else:
                        break  # é‡åˆ°arXivLabså†…å®¹å°±åœæ­¢
                
                if filtered_lines:
                    abstract = '. '.join(filtered_lines).strip()
                    if abstract.endswith('.'):
                        abstract = abstract[:-1]  # ç§»é™¤æœ€åå¤šä½™çš„å¥å·
                    abstract = abstract + '.'  # æ·»åŠ ç»“æŸå¥å·
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ‘˜è¦ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•
            if not abstract:
                # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ‘˜è¦æ ‡è®°
                alt_patterns = [
                    r'<div[^>]*class="abstract[^"]*"[^>]*>.*?<p[^>]*>(.*?)</p>',
                    r'<meta[^>]+name="description"[^>]+content="([^"]+)"'
                ]
                
                for pattern in alt_patterns:
                    alt_match = re.search(pattern, html_content, re.DOTALL)
                    if alt_match:
                        abstract_candidate = alt_match.group(1).strip()
                        abstract_candidate = re.sub(r'<[^>]+>', '', abstract_candidate)
                        abstract_candidate = re.sub(r'\s+', ' ', abstract_candidate).strip()
                        
                        if len(abstract_candidate) > 50:
                            abstract = abstract_candidate
                            break
            
            if abstract and len(abstract) > 20:
                metadata['abstract'] = abstract
            
            # æå–æ—¥æœŸ - æ”¹è¿›ç‰ˆæœ¬
            date_match = re.search(r'<meta name="citation_date" content="([^"]+)"', html_content)
            if date_match:
                metadata['date'] = date_match.group(1)
            else:
                # å¤‡é€‰æ–¹æ³•ï¼šä»æäº¤ä¿¡æ¯ä¸­æå–
                date_match = re.search(r'\[Submitted on ([^\]]+)\]', html_content)
                if date_match:
                    date_str = date_match.group(1).strip()
                    # è½¬æ¢æ—¥æœŸæ ¼å¼ä¸ºæ ‡å‡†æ ¼å¼
                    try:
                        import datetime
                        # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
                        for fmt in ['%d %b %Y', '%B %d, %Y', '%Y-%m-%d']:
                            try:
                                parsed_date = datetime.strptime(date_str, fmt)
                                metadata['date'] = parsed_date.strftime('%Y/%m/%d')
                                break
                            except ValueError:
                                continue
                        else:
                            metadata['date'] = date_str
                    except:
                        metadata['date'] = date_str
            
            # æå–è¯„è®ºä¿¡æ¯ï¼ˆé¡µæ•°ã€å›¾è¡¨ç­‰ï¼‰
            comment = None
            
            # æ–¹å¼1: æ ‡å‡†è¡¨æ ¼æ ¼å¼
            comment_match = re.search(r'<td class="comments">([^<]+)</td>', html_content)
            if comment_match:
                comment = comment_match.group(1).strip()
            
            # æ–¹å¼2: Commentsæ ‡ç­¾åçš„å†…å®¹
            if not comment:
                comment_match = re.search(r'Comments:\s*([^\n<]+)', html_content)
                if comment_match:
                    comment = comment_match.group(1).strip()
            
            # æ–¹å¼3: ç›´æ¥æœç´¢é¡µæ•°å’Œå›¾è¡¨ä¿¡æ¯
            if not comment:
                pages_figures = re.search(r'(\d+\s*pages?,?\s*\d*\s*figures?)', html_content, re.IGNORECASE)
                if pages_figures:
                    comment = pages_figures.group(1).strip()
            
            # æ–¹å¼4: æ›´å®½æ³›çš„é¡µæ•°æœç´¢
            if not comment:
                pages_match = re.search(r'(\d+\s*pages?[^<\n]{0,30})', html_content, re.IGNORECASE)
                if pages_match:
                    comment = pages_match.group(1).strip()
            
            if comment:
                metadata['comment'] = comment
            
            # æå–å­¦ç§‘åˆ†ç±»
            subjects_matches = re.findall(r'<span class="primary-subject">([^<]+)</span>', html_content)
            if subjects_matches:
                metadata['subjects'] = subjects_matches
            else:
                # å¤‡é€‰æ–¹å¼
                subjects_matches = re.findall(r'class="[^"]*subject-class[^"]*">([^<]+)</span>', html_content)
                if subjects_matches:
                    metadata['subjects'] = subjects_matches
            
            # æå–DOIï¼ˆå¦‚æœæœ‰ï¼‰
            doi_match = re.search(r'<meta name="citation_doi" content="([^"]+)"', html_content)
            if doi_match:
                metadata['doi'] = doi_match.group(1)
            
            # æå–æœŸåˆŠä¿¡æ¯ï¼ˆå¦‚æœå·²å‘è¡¨ï¼‰
            journal_match = re.search(r'<meta name="citation_journal_title" content="([^"]+)"', html_content)
            if journal_match:
                metadata['published_journal'] = journal_match.group(1)
            
            # è®¾ç½®é»˜è®¤å€¼
            metadata.setdefault('title', 'Unknown arXiv Paper')
            metadata.setdefault('authors_string', 'Unknown Authors')
            metadata.setdefault('date', time.strftime('%Y'))
            metadata.setdefault('abstract', '')
            
            logger.info(f"æˆåŠŸæå–arxivå…ƒæ•°æ®: {metadata.get('title', 'Unknown')}")
            return metadata
            
        except Exception as e:
            logger.error(f"æå–arxivå…ƒæ•°æ®å¤±è´¥: {e}")
            return {"error": f"å…ƒæ•°æ®æå–å¤±è´¥: {e}"}
    
    def _enhance_paper_info_for_arxiv(self, paper_info: Dict) -> Dict:
        """ä¸ºarxivè®ºæ–‡å¢å¼ºå…ƒæ•°æ®"""
        url = paper_info.get('url', '')
        
        if 'arxiv.org' in url:
            logger.info("æ£€æµ‹åˆ°arxivè®ºæ–‡ï¼Œå¼€å§‹å¢å¼ºå…ƒæ•°æ®...")
            arxiv_metadata = self._extract_arxiv_metadata(url)
            
            if 'error' not in arxiv_metadata:
                # åˆå¹¶å…ƒæ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨arxivæå–çš„ä¿¡æ¯
                enhanced_info = paper_info.copy()
                enhanced_info.update({
                    'title': arxiv_metadata.get('title', paper_info.get('title', '')),
                    'authors': arxiv_metadata.get('authors_string', paper_info.get('authors', '')),
                    'abstract': arxiv_metadata.get('abstract', paper_info.get('abstract', '')),
                    'date': arxiv_metadata.get('date', paper_info.get('date', '')),
                    'journal': 'arXiv',
                    'itemType': 'preprint',
                    'url': arxiv_metadata.get('abs_url', url),
                    'arxiv_id': arxiv_metadata.get('arxiv_id', ''),
                    'pdf_url': arxiv_metadata.get('pdf_url', ''),
                    'comment': arxiv_metadata.get('comment', ''),  # æ·»åŠ commentä¿¡æ¯
                    'subjects': arxiv_metadata.get('subjects', []),  # æ·»åŠ å­¦ç§‘ä¿¡æ¯
                    'doi': arxiv_metadata.get('doi', ''),  # æ·»åŠ DOI
                    'published_journal': arxiv_metadata.get('published_journal', ''),  # æ·»åŠ å‘è¡¨æœŸåˆŠ
                })
                
                logger.info(f"arxivå…ƒæ•°æ®å¢å¼ºå®Œæˆ: {enhanced_info.get('title', 'Unknown')}")
                return enhanced_info
            else:
                logger.warning(f"arxivå…ƒæ•°æ®å¢å¼ºå¤±è´¥: {arxiv_metadata.get('error', 'Unknown')}")
        
        return paper_info

    def _find_zotero_database(self) -> Optional[Path]:
        """æŸ¥æ‰¾Zoteroæ•°æ®åº“æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨è¦†ç›–è·¯å¾„ã€‚"""
        # è¦†ç›–ä¼˜å…ˆ
        if self._zotero_db_override and Path(self._zotero_db_override).exists():
            logger.info(f"æ‰¾åˆ°Zoteroæ•°æ®åº“(è¦†ç›–): {self._zotero_db_override}")
            return self._zotero_db_override

        # æŒ‰ç³»ç»Ÿé»˜è®¤è·¯å¾„æ¢æµ‹
        possible_paths: List[Path] = []
        platform = os.name  # 'posix' / 'nt'

        # é€šç”¨è·¯å¾„
        possible_paths.append(Path.home() / 'Zotero' / 'zotero.sqlite')

        # macOS
        possible_paths.append(Path.home() / 'Library' / 'Application Support' / 'Zotero' / 'zotero.sqlite')
        profiles_base_mac = Path.home() / 'Library' / 'Application Support' / 'Zotero' / 'Profiles'
        if profiles_base_mac.exists():
            for profile_dir in profiles_base_mac.iterdir():
                if profile_dir.is_dir():
                    possible_paths.append(profile_dir / 'zotero.sqlite')

        # Windowsï¼ˆAPPDATA ä¸‹çš„Profilesï¼‰
        appdata = os.environ.get('APPDATA')
        if appdata:
            profiles_base_win = Path(appdata) / 'Zotero' / 'Zotero' / 'Profiles'
            if profiles_base_win.exists():
                for profile_dir in profiles_base_win.iterdir():
                    if profile_dir.is_dir():
                        possible_paths.append(profile_dir / 'zotero.sqlite')

        # Linux å¸¸è§è·¯å¾„ï¼ˆè‹¥ç”¨æˆ·å°†Zoteroæ”¾åœ¨å®¶ç›®å½•ï¼‰
        possible_paths.append(Path.home() / '.zotero' / 'zotero.sqlite')

        for path in possible_paths:
            try:
                if path.exists():
                    logger.info(f"æ‰¾åˆ°Zoteroæ•°æ®åº“: {path}")
                    return path
            except Exception:
                continue
        
        logger.warning("æœªæ‰¾åˆ°Zoteroæ•°æ®åº“æ–‡ä»¶")
        return None

    def _read_collections_from_db(self) -> List[Dict]:
        """ç›´æ¥ä»æ•°æ®åº“è¯»å–é›†åˆä¿¡æ¯"""
        if not self._zotero_db_path or not self._zotero_db_path.exists():
            logger.error("Zoteroæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return []
        
        try:
            # åˆ›å»ºä¸´æ—¶å‰¯æœ¬ä»¥é¿å…é”å®šé—®é¢˜
            with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as temp_file:
                shutil.copy2(self._zotero_db_path, temp_file.name)
                temp_db_path = temp_file.name
            
            try:
                conn = sqlite3.connect(temp_db_path)
                cursor = conn.cursor()
                
                # æŸ¥è¯¢é›†åˆä¿¡æ¯
                query = """
                SELECT 
                    c.collectionID,
                    c.collectionName,
                    c.parentCollectionID,
                    c.key
                FROM collections c
                ORDER BY c.collectionName
                """
                
                cursor.execute(query)
                rows = cursor.fetchall()
                
                collections = []
                for row in rows:
                    collection_data = {
                        'id': row[0],
                        'name': row[1],
                        'parentCollection': row[2] if row[2] else None,
                        'key': row[3] if row[3] else f"collection_{row[0]}"
                    }
                    collections.append(collection_data)
                
                conn.close()
                logger.info(f"ä»æ•°æ®åº“æˆåŠŸè¯»å– {len(collections)} ä¸ªé›†åˆ")
                return collections
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    Path(temp_db_path).unlink()
                except:
                    pass
                    
        except Exception as e:
            logger.error(f"è¯»å–æ•°æ®åº“é›†åˆå¤±è´¥: {e}")
            return []
    
    def is_running(self) -> bool:
        """æ£€æŸ¥Zoteroæ˜¯å¦åœ¨è¿è¡Œ"""
        try:
            response = self.session.get(f"{self.base_url}/connector/ping", timeout=2)
            return response.status_code == 200
        except Exception as e:
            logger.debug(f"Zoteroæœªè¿è¡Œæˆ–æ— æ³•è¿æ¥: {e}")
            return False
    
    def get_version(self) -> Optional[str]:
        """è·å–Zoteroç‰ˆæœ¬ä¿¡æ¯"""
        try:
            if not self.is_running():
                return None
            
            response = self.session.get(f"{self.base_url}/connector/ping", timeout=5)
            if response.status_code == 200:
                # Zotero pingè¿”å›HTMLï¼Œä¸æ˜¯JSON
                if "Zotero is running" in response.text:
                    return "Zotero Desktop (Unknown version)"
                else:
                    return "unknown"
        except Exception as e:
            logger.debug(f"è·å–Zoteroç‰ˆæœ¬å¤±è´¥: {e}")
            return "unknown"
    
    def get_collections(self) -> List[Dict]:
        """è·å–æ‰€æœ‰é›†åˆ
        ä¼˜å…ˆå°è¯•ç›´æ¥è¯»å–æ•°æ®åº“ï¼Œå¤‡é€‰APIæ–¹å¼
        """
        try:
            if not self.is_running():
                return []
            
            # é¦–å…ˆå°è¯•ç›´æ¥ä»æ•°æ®åº“è¯»å–ï¼ˆæ–°çš„è§£å†³æ–¹æ¡ˆï¼ï¼‰
            logger.info("å°è¯•ç›´æ¥ä»æ•°æ®åº“è¯»å–é›†åˆ...")
            db_collections = self._read_collections_from_db()
            
            if db_collections:
                logger.info(f"âœ… æˆåŠŸä»æ•°æ®åº“è·å– {len(db_collections)} ä¸ªé›†åˆ")
                return db_collections
            
            # å¦‚æœæ•°æ®åº“è¯»å–å¤±è´¥ï¼Œå›é€€åˆ°APIæ–¹å¼
            logger.info("æ•°æ®åº“è¯»å–å¤±è´¥ï¼Œå°è¯•APIæ–¹å¼...")
            api_endpoints = [
                "/api/users/local/collections",  # Zotero 7 æœ¬åœ°API
                "/connector/collections",        # å¯èƒ½çš„Connector API
                "/api/collections"               # å¦ä¸€ç§å¯èƒ½çš„ç«¯ç‚¹
            ]
            
            for endpoint in api_endpoints:
                try:
                    response = self.session.get(f"{self.base_url}{endpoint}", timeout=5)
                    if response.status_code == 200:
                        try:
                            data = response.json()
                            if isinstance(data, list):
                                logger.info(f"æˆåŠŸä»ç«¯ç‚¹è·å–é›†åˆ: {endpoint}")
                                return data
                            elif isinstance(data, dict) and 'collections' in data:
                                return data['collections']
                        except json.JSONDecodeError:
                            continue
                except Exception as e:
                    logger.debug(f"æµ‹è¯•ç«¯ç‚¹{endpoint}å¤±è´¥: {e}")
                    continue
            
            logger.warning("æ— æ³•é€šè¿‡APIæˆ–æ•°æ®åº“è·å–é›†åˆåˆ—è¡¨")
            return []
                
        except Exception as e:
            logger.error(f"è·å–Zoteroé›†åˆå¤±è´¥: {e}")
            return []
    
    def save_item_to_zotero(self, paper_info: Dict, pdf_path: Optional[str] = None, 
                           collection_key: Optional[str] = None) -> Dict:
        """
        ä¿å­˜è®ºæ–‡åˆ°Zotero
        
        Args:
            paper_info: è®ºæ–‡ä¿¡æ¯å­—å…¸
            pdf_path: PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            collection_key: ç›®æ ‡é›†åˆkeyï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: ä¿å­˜ç»“æœ
        """
        try:
            if not self.is_running():
                return {
                    "success": False,
                    "message": "Zoteroæœªè¿è¡Œï¼Œè¯·å¯åŠ¨Zoteroæ¡Œé¢åº”ç”¨"
                }
            
            # ğŸ¯ å…³é”®æ‰©å±•ï¼šä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¢å¼ºå…ƒæ•°æ®
            enhanced_paper_info = self._enhance_paper_metadata(paper_info)
            
            # å¦‚æœå¢å¼ºå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹ä¿¡æ¯
            if 'error' in enhanced_paper_info:
                logger.warning(f"âš ï¸ å…ƒæ•°æ®å¢å¼ºå¤±è´¥: {enhanced_paper_info['error']}")
                enhanced_paper_info = paper_info

            # æ„å»ºZoteroé¡¹ç›®æ•°æ®
            zotero_item = self._convert_to_zotero_format(enhanced_paper_info)
            
            # ä¿å­˜åˆ°Zotero
            result = self._save_via_connector(zotero_item, pdf_path, collection_key)
            
            # æ·»åŠ æ‰©å±•ä¿¡æ¯åˆ°ç»“æœ
            if result["success"]:
                result["database"] = enhanced_paper_info.get('extractor', 'arXiv')
                result["enhanced"] = 'extractor' in enhanced_paper_info
            
            # å¯¹äºarxivè®ºæ–‡ï¼Œåœ¨å…ƒæ•°æ®ä¿å­˜æˆåŠŸåå¤„ç†PDF
            if result["success"] and 'arxiv.org' in enhanced_paper_info.get('url', '') and enhanced_paper_info.get('arxiv_id'):
                logger.info("å…ƒæ•°æ®ä¿å­˜æˆåŠŸï¼Œç°åœ¨å¤„ç†PDF...")
                
                # åœ¨Extraå­—æ®µä¸­æ·»åŠ PDFä¿¡æ¯ï¼Œç”¨æˆ·å¯ä»¥æ‰‹åŠ¨ä¸‹è½½
                pdf_url = f"https://arxiv.org/pdf/{enhanced_paper_info['arxiv_id']}.pdf"
                result["pdf_url"] = pdf_url
                result["pdf_info"] = f"PDFå¯ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½: {pdf_url}"
                result["message"] += f"\nğŸ“¥ PDFé“¾æ¥: {pdf_url}"
                
                logger.info(f"âœ… PDFé“¾æ¥å·²æ·»åŠ åˆ°æ¡ç›®ä¿¡æ¯ä¸­: {pdf_url}")
            
            if result["success"]:
                logger.info(f"æˆåŠŸä¿å­˜åˆ°Zotero: {enhanced_paper_info.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨è¿”å›ç»“æœä¸­æ·»åŠ æ­£ç¡®çš„æ ‡é¢˜ä¿¡æ¯
                result["title"] = enhanced_paper_info.get('title', '')
                result["paper_info"] = enhanced_paper_info
            
            return result
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°Zoteroå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"ä¿å­˜åˆ°Zoteroå¤±è´¥: {e}"
            }
    
    def _split_comma_authors(self, authors_str: str) -> list:
        """
        æ™ºèƒ½åˆ†å‰²é€—å·åˆ†éš”çš„ä½œè€…
        
        æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. "First Last, First Last" - é€—å·åˆ†éš”ä¸åŒä½œè€…
        2. "Last, First, Last, First" - è¿ç»­çš„"å§“, å"æ ¼å¼
        """
        parts = [p.strip() for p in authors_str.split(',')]
        
        # å¦‚æœåªæœ‰1-2ä¸ªéƒ¨åˆ†
        if len(parts) <= 2:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ "First Last, First Last" æ ¼å¼
            # å¯å‘å¼è§„åˆ™ï¼šå¦‚æœç¬¬ä¸€éƒ¨åˆ†å’Œç¬¬äºŒéƒ¨åˆ†éƒ½åŒ…å«ç©ºæ ¼ï¼Œå¯èƒ½æ˜¯ä¸¤ä¸ªç‹¬ç«‹ä½œè€…
            if len(parts) == 2 and ' ' in parts[0] and ' ' in parts[1]:
                # "John Smith, Jane Doe" -> ä¸¤ä¸ªä½œè€…
                return parts
            else:
                # "Smith, John" -> ä¸€ä¸ªä½œè€…
                return [authors_str]
        
        # å¤šä¸ªéƒ¨åˆ†çš„æƒ…å†µ
        # å¯å‘å¼è§„åˆ™1ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰éƒ¨åˆ†éƒ½åŒ…å«ç©ºæ ¼ï¼ˆè¡¨ç¤º "First Last" æ ¼å¼ï¼‰
        all_have_spaces = all(' ' in part for part in parts)
        if all_have_spaces:
            # "John Smith, Jane Doe, Bob Chen" -> ä¸‰ä¸ªç‹¬ç«‹ä½œè€…
            return parts
        
        # å¯å‘å¼è§„åˆ™2ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„"å§“, å"æ ¼å¼
        # å¦‚æœéƒ¨åˆ†æ•°é‡æ˜¯å¶æ•°ï¼Œä¸”äº¤æ›¿å‡ºç°"æ— ç©ºæ ¼"å’Œ"å¯èƒ½æœ‰ç©ºæ ¼"çš„æ¨¡å¼
        if len(parts) % 2 == 0:
            # æ£€æŸ¥å¥‡æ•°ç´¢å¼•ï¼ˆå§“ï¼‰æ˜¯å¦é€šå¸¸ä¸å«ç©ºæ ¼
            odd_indices_no_space = sum(1 for i in range(0, len(parts), 2) if ' ' not in parts[i])
            if odd_indices_no_space > len(parts) // 4:  # è‡³å°‘25%çš„"å§“"ä¸å«ç©ºæ ¼
                # å¾ˆå¯èƒ½æ˜¯ "Last, First, Last, First" æ ¼å¼
                author_names = []
                for i in range(0, len(parts), 2):
                    if i + 1 < len(parts):
                        author_names.append(f"{parts[i]}, {parts[i+1]}")
                return author_names
        
        # é»˜è®¤ï¼šå¦‚æœæœ‰å¤šä¸ªé€—å·ä½†æ— æ³•ç¡®å®šï¼Œå°è¯•æŒ‰ç©ºæ ¼æ•°åˆ¤æ–­
        # å¦‚æœå¤§éƒ¨åˆ†éƒ¨åˆ†éƒ½æœ‰ç©ºæ ¼ï¼Œå¯èƒ½æ˜¯ç‹¬ç«‹ä½œè€…
        parts_with_space = sum(1 for part in parts if ' ' in part)
        if parts_with_space > len(parts) * 0.6:  # è¶…è¿‡60%æœ‰ç©ºæ ¼
            return parts
        
        # æ— æ³•ç¡®å®šï¼Œä¿æŒåŸæ ·
        return [authors_str]
    
    def _convert_to_zotero_format(self, paper_info: Dict) -> Dict:
        """å°†è®ºæ–‡ä¿¡æ¯è½¬æ¢ä¸ºZoteroæ ¼å¼"""
        
        # è§£æä½œè€… - æ”¹è¿›çš„é€»è¾‘æ”¯æŒå¤šç§æ ¼å¼
        authors = []
        
        # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨å·²ç»æ ¼å¼åŒ–çš„ creatorsï¼ˆZoteroæ ¼å¼æ•°ç»„ï¼‰
        # éƒ¨åˆ†æå–å™¨ï¼ˆå¦‚ PreprintExtractor, BioRxivDirectExtractorï¼‰ç›´æ¥è¿”å› Zotero æ ¼å¼
        if paper_info.get('creators') and isinstance(paper_info['creators'], list):
            logger.debug("âœ… æ£€æµ‹åˆ° creators å­—æ®µï¼ˆZoteroæ ¼å¼ï¼‰ï¼Œç›´æ¥ä½¿ç”¨")
            authors = paper_info['creators'][:15]  # é™åˆ¶ä½œè€…æ•°é‡
        
        # å¦åˆ™è§£æ authors å­—ç¬¦ä¸²æ ¼å¼ï¼ˆarXiv, Generic ç­‰æå–å™¨ä½¿ç”¨ï¼‰
        elif paper_info.get('authors'):
            authors_str = paper_info['authors']
            
            # ğŸ”§ ä¿®å¤: æ­£ç¡®åˆ†å‰²ä½œè€…åˆ—è¡¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼
            if ';' in authors_str:
                # æ ‡å‡†æ ¼å¼ï¼šä½¿ç”¨åˆ†å·åˆ†éš”
                author_names = authors_str.split(';')
            elif ' and ' in authors_str:
                # ä½¿ç”¨ "and" è¿æ¥çš„æ ¼å¼
                author_names = [a.strip() for a in authors_str.split(' and ')]
            else:
                # å¤„ç†é€—å·åˆ†éš”çš„æƒ…å†µ - æ™ºèƒ½åˆ¤æ–­æ ¼å¼
                author_names = self._split_comma_authors(authors_str)
            
            for author_name in author_names[:15]:  # é™åˆ¶ä½œè€…æ•°é‡
                author_name = author_name.strip()
                if not author_name or author_name == 'æœªçŸ¥ä½œè€…':
                    continue
                
                # è§£æ"å§“, å"æ ¼å¼
                if ',' in author_name:
                    parts = author_name.split(',', 1)  # åªåˆ†å‰²ç¬¬ä¸€ä¸ªé€—å·
                    lastName = parts[0].strip()
                    firstName = parts[1].strip()
                else:
                    # å¤„ç†"å å§“"æ ¼å¼
                    parts = author_name.split()
                    if len(parts) >= 2:
                        firstName = ' '.join(parts[:-1])
                        lastName = parts[-1]
                    else:
                        firstName = ""
                        lastName = author_name
                
                # ç¡®ä¿ä¸ä¸ºç©º
                if firstName or lastName:
                    authors.append({
                        "creatorType": "author",
                        "firstName": firstName,
                        "lastName": lastName
                    })
        
        # è§£ææ—¥æœŸ
        date = paper_info.get('date', '')
        if date and date != 'æœªçŸ¥æ—¥æœŸ':
            # å°è¯•æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            try:
                # å¤„ç†arxivå’Œå…¶ä»–å¸¸è§çš„æ—¥æœŸæ ¼å¼
                # æ ¼å¼1: "12 Jun 2017"
                date_match = re.search(r'(\d{1,2})\s+(\w+)\s+(\d{4})', date)
                if date_match:
                    day, month_name, year = date_match.groups()
                    months = {
                        'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
                        'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',
                        'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
                    }
                    month = months.get(month_name[:3], '01')
                    date = f"{year}-{month}-{day.zfill(2)}"
                # æ ¼å¼2: "2017/06/12" æˆ– "2017-06-12"
                elif re.search(r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})', date):
                    # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œä¿æŒä¸å˜
                    pass
                # æ ¼å¼3: åªæœ‰å¹´ä»½ "2017"
                elif re.search(r'^\d{4}$', date):
                    date = f"{date}-01-01"
            except:
                pass
        
        # ç¡®å®šé¡¹ç›®ç±»å‹ 
        item_type = paper_info.get('itemType', 'journalArticle')
        if 'arxiv.org' in paper_info.get('url', ''):
            item_type = 'preprint'  # arxivè®ºæ–‡ä½¿ç”¨preprintç±»å‹
        
        # æ„å»ºZoteroé¡¹ç›®
        zotero_item = {
            "itemType": item_type,
            "title": paper_info.get('title', ''),
            "creators": authors,
            "abstractNote": paper_info.get('abstract', ''),
            "publicationTitle": self._get_default_publication_title(paper_info),
            "url": paper_info.get('url', ''),
            "date": date
        }
        
        # ğŸ†• ä¸ºé¢„å°æœ¬æ·»åŠ å®˜æ–¹Zotero Connectorå…¼å®¹çš„å­—æ®µ
        if item_type == 'preprint':
            if 'arxiv.org' in paper_info.get('url', '') and paper_info.get('arxiv_id'):
                # arXivç‰¹æ®Šå¤„ç†
                zotero_item["repository"] = "arXiv"
                zotero_item["archiveID"] = f"arXiv:{paper_info['arxiv_id']}"
                zotero_item["libraryCatalog"] = "arXiv.org"
                
                # ç¾å¼æ—¥æœŸæ—¶é—´æ ¼å¼
                import datetime
                now = datetime.datetime.now()
                month = now.month
                day = now.day
                year = now.year
                hour = now.hour
                minute = now.minute
                second = now.second
                
                if hour == 0:
                    hour_12 = 12
                    am_pm = "AM"
                elif hour < 12:
                    hour_12 = hour
                    am_pm = "AM"
                elif hour == 12:
                    hour_12 = 12
                    am_pm = "PM"
                else:
                    hour_12 = hour - 12
                    am_pm = "PM"
                
                us_format = f"{month}/{day}/{year}, {hour_12}:{minute:02d}:{second:02d} {am_pm}"
                zotero_item["accessDate"] = us_format
            else:
                # ğŸ†• å…¶ä»–é¢„å°æœ¬æœåŠ¡å™¨çš„é€šç”¨å¤„ç†
                if paper_info.get('repository'):
                    zotero_item["repository"] = paper_info['repository']
                
                if paper_info.get('archiveID'):
                    zotero_item["archiveID"] = paper_info['archiveID']
                elif paper_info.get('DOI'):
                    # å¦‚æœæ²¡æœ‰ä¸“é—¨çš„archiveIDï¼Œä½¿ç”¨DOI
                    zotero_item["archiveID"] = paper_info['DOI']
                
                if paper_info.get('libraryCatalog'):
                    zotero_item["libraryCatalog"] = paper_info['libraryCatalog']
                
                # æ ‡å‡†è®¿é—®æ—¥æœŸæ ¼å¼
                if paper_info.get('accessDate'):
                    zotero_item["accessDate"] = paper_info['accessDate']
                else:
                    zotero_item["accessDate"] = time.strftime('%Y-%m-%d')
        else:
            # éé¢„å°æœ¬ä½¿ç”¨æ ‡å‡†æ ¼å¼
            zotero_item["accessDate"] = time.strftime('%Y-%m-%d')
        
        # ğŸš¨ ä¿®å¤ï¼šä¸ºarxivè®ºæ–‡æ·»åŠ PDF URLï¼ˆä¾›_save_via_connectorä½¿ç”¨ï¼‰
        if paper_info.get('arxiv_id') and paper_info.get('pdf_url'):
            zotero_item["pdf_url"] = paper_info['pdf_url']  # å…³é”®ï¼šæ·»åŠ pdf_urlå­—æ®µ
        
        # ğŸš€ å…³é”®ä¿®å¤ï¼šä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹ï¼ˆarXivè·¯å¾„ï¼‰
        if paper_info.get('pdf_content'):
            zotero_item["pdf_content"] = paper_info['pdf_content']
            logger.info(f"âœ… ä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹: {len(paper_info['pdf_content'])} bytes")
        
        # æ·»åŠ arxivç‰¹æ®Šå­—æ®µå’Œå¢å¼ºä¿¡æ¯
        if paper_info.get('arxiv_id'):
            # ğŸ†• ä½¿ç”¨å®˜æ–¹Zotero Connectorå…¼å®¹çš„Extraæ ¼å¼: "arXiv:ID [å­¦ç§‘]"
            arxiv_id = paper_info['arxiv_id']
            extra_parts = [f"arXiv:{arxiv_id}"]
            
            # æ·»åŠ ä¸»è¦å­¦ç§‘åˆ†ç±»çš„ç¼©å†™ (å¦‚ [cs] è¡¨ç¤º Computer Science)
            if paper_info.get('subjects'):
                # æå–ç¬¬ä¸€ä¸ªå­¦ç§‘çš„ç¼©å†™
                first_subject = paper_info['subjects'][0]
                # ä»"Computation and Language (cs.CL)"ä¸­æå–"cs"
                subject_match = re.search(r'\(([^.]+)', first_subject)
                if subject_match:
                    subject_abbr = subject_match.group(1)
                    extra_parts.append(f"[{subject_abbr}]")
            
            # æ„å»ºç®€æ´çš„Extraä¿¡æ¯ (ä¸å®˜æ–¹æ’ä»¶æ ¼å¼ä¸€è‡´)
            zotero_item["extra"] = " ".join(extra_parts)
            
            # æ·»åŠ DOIå­—æ®µï¼ˆå¦‚æœæœ‰ï¼‰
            if paper_info.get('doi'):
                zotero_item["DOI"] = paper_info['doi']
            
            # å¦‚æœå·²å‘è¡¨åˆ°æœŸåˆŠï¼Œæ›´æ–°æœŸåˆŠåç§°
            if paper_info.get('published_journal'):
                zotero_item["publicationTitle"] = paper_info['published_journal']
        else:
            # å¤„ç†å…¶ä»–æ•°æ®åº“çš„å…ƒæ•°æ®
            extra_info = f"ä¸‹è½½æ¥æº: ZotLink\n"
            
            if paper_info.get('extractor'):
                extra_info += f"æ•°æ®åº“: {paper_info['extractor']}\n"
            
            # æ·»åŠ DOIå­—æ®µ
            if paper_info.get('DOI'):
                zotero_item["DOI"] = paper_info['DOI']
                extra_info += f"DOI: {paper_info['DOI']}\n"
            elif paper_info.get('doi'):
                zotero_item["DOI"] = paper_info['doi']
                extra_info += f"DOI: {paper_info['doi']}\n"
            
            if paper_info.get('comment'):
                extra_info += f"Comment: {paper_info['comment']}\n"
            
            if paper_info.get('pdf_url'):
                extra_info += f"PDFé“¾æ¥: {paper_info['pdf_url']}\n"
                zotero_item["pdf_url"] = paper_info['pdf_url']

            # ğŸš€ å…³é”®ä¿®å¤ï¼šä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹ï¼ˆéarXivè·¯å¾„ï¼‰
            if paper_info.get('pdf_content'):
                zotero_item["pdf_content"] = paper_info['pdf_content']
                logger.info(f"âœ… ä¼ é€’æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹: {len(paper_info['pdf_content'])} bytes")
            
            zotero_item["extra"] = extra_info
        
        # ğŸ”‘ æ·»åŠ PDFé™„ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if paper_info.get('pdf_url'):
            zotero_item["attachments"] = [{
                "title": "Full Text PDF",
                "url": paper_info['pdf_url'],
                "mimeType": "application/pdf",
                "snapshot": False  # é“¾æ¥é™„ä»¶ï¼Œä¸ä¸‹è½½å†…å®¹
            }]
        
        # ç§»é™¤ç©ºå€¼
        zotero_item = {k: v for k, v in zotero_item.items() if v}
        
        return zotero_item
    
    def _download_pdf_content(self, pdf_url: str) -> Optional[bytes]:
        """
        å°è¯•ä¸‹è½½PDFå†…å®¹
        
        Args:
            pdf_url: PDFé“¾æ¥
            
        Returns:
            PDFæ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            import requests
            
            # ğŸ§¬ ç‰¹æ®Šå¤„ç†ï¼šbioRxivä½¿ç”¨MCPé«˜çº§æµè§ˆå™¨ä¸‹è½½
            if 'biorxiv.org' in pdf_url.lower():
                logger.info("ğŸ§¬ æ£€æµ‹åˆ°bioRxiv - å¯åŠ¨MCPé«˜çº§æµè§ˆå™¨ä¸‹è½½")
                try:
                    # ä½¿ç”¨äº‹ä»¶å¾ªç¯å…¼å®¹çš„å¼‚æ­¥è°ƒç”¨
                    import asyncio
                    # ä½¿ç”¨åŒ…å†…ç›¸å¯¹å¯¼å…¥ï¼Œé¿å…åœ¨è¿è¡Œç¯å¢ƒä¸­æ‰¾ä¸åˆ°é¡¶çº§æ¨¡å—
                    from .extractors.browser_extractor import BrowserExtractor
                    
                    async def download_biorxiv_mcp():
                        async with BrowserExtractor() as extractor:
                            return await extractor._download_biorxiv_with_mcp(extractor, pdf_url)
                    
                    # åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯æ‰§è¡Œå¼‚æ­¥ä»»åŠ¡
                    import concurrent.futures
                    import threading
                    
                    def run_in_thread():
                        # åœ¨æ–°çº¿ç¨‹ä¸­åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(download_biorxiv_mcp())
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_in_thread)
                        pdf_content = future.result(timeout=120)  # æ”¾å®½åˆ°120ç§’
                    
                    if pdf_content:
                        logger.info(f"âœ… MCPæµè§ˆå™¨ä¸‹è½½bioRxiv PDFæˆåŠŸ: {len(pdf_content):,} bytes")
                        return pdf_content
                    else:
                        logger.warning("âš ï¸ MCPæµè§ˆå™¨ä¸‹è½½bioRxiv PDFå¤±è´¥ï¼Œå°è¯•å¤‡ç”¨åçˆ¬è™«ä¸‹è½½å™¨")
                        # å›é€€ï¼šä½¿ç”¨é€šç”¨åçˆ¬è™«ä¸‹è½½å™¨
                        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è°ƒç”¨å¼‚æ­¥ä¸‹è½½å™¨ï¼Œé¿å…äº‹ä»¶å¾ªç¯å†²çª
                        try:
                            import concurrent.futures
                            import asyncio
                            
                            def run_fallback_thread():
                                new_loop = asyncio.new_event_loop()
                                asyncio.set_event_loop(new_loop)
                                try:
                                    from .tools.anti_crawler_pdf_downloader import download_anti_crawler_pdf_async
                                    return new_loop.run_until_complete(download_anti_crawler_pdf_async(pdf_url))
                                finally:
                                    new_loop.close()
                            
                            with concurrent.futures.ThreadPoolExecutor() as executor:
                                future = executor.submit(run_fallback_thread)
                                fallback_content = future.result(timeout=120)
                        except Exception:
                            fallback_content = None
                        if fallback_content:
                            logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½å™¨æˆåŠŸè·å–PDF: {len(fallback_content):,} bytes")
                            return fallback_content
                        return None
                        
                except Exception as e:
                    logger.error(f"âŒ MCPæµè§ˆå™¨ä¸‹è½½å¼‚å¸¸: {e}")
                    # å¼‚å¸¸ä¹Ÿå°è¯•å¤‡ç”¨ä¸‹è½½å™¨
                    # å¼‚å¸¸è·¯å¾„åŒæ ·åœ¨çº¿ç¨‹ä¸­è°ƒç”¨å¼‚æ­¥ä¸‹è½½å™¨
                    try:
                        import concurrent.futures
                        import asyncio
                        
                        def run_fallback_thread():
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                from .tools.anti_crawler_pdf_downloader import download_anti_crawler_pdf_async
                                return new_loop.run_until_complete(download_anti_crawler_pdf_async(pdf_url))
                            finally:
                                new_loop.close()
                        
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(run_fallback_thread)
                            fallback_content = future.result(timeout=120)
                    except Exception:
                        fallback_content = None
                    if fallback_content:
                        logger.info(f"âœ… å¤‡ç”¨ä¸‹è½½å™¨æˆåŠŸè·å–PDF: {len(fallback_content):,} bytes")
                        return fallback_content
                    return None
            else:
                # å¯¹äºæ™®é€šç½‘ç«™ï¼Œä½¿ç”¨HTTPè¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                logger.info("ğŸ“¥ ä½¿ç”¨HTTPè¯·æ±‚ä¸‹è½½PDF")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/pdf,*/*',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive'
                }
                
                # ğŸ¯ v1.3.6: æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œè§£å†³ç½‘ç»œä¸­æ–­å¯¼è‡´çš„ä¸‹è½½å¤±è´¥
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = requests.get(pdf_url, headers=headers, timeout=30, stream=True)
                        
                        if response.status_code == 200:
                            content = response.content
                            
                            # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆPDF
                            if content and content.startswith(b'%PDF'):
                                logger.info(f"âœ… HTTPä¸‹è½½æˆåŠŸ: {len(content):,} bytes")
                                return content
                            else:
                                logger.warning("âš ï¸ ä¸‹è½½çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆPDF")
                                return None
                        else:
                            logger.warning(f"âš ï¸ HTTPä¸‹è½½å¤±è´¥: {response.status_code}")
                            return None
                            
                    except (requests.exceptions.ConnectionError, 
                            requests.exceptions.ChunkedEncodingError,
                            requests.exceptions.Timeout) as e:
                        if attempt < max_retries - 1:
                            wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿ï¼š1s, 2s, 4s
                            logger.warning(f"âš ï¸ PDFä¸‹è½½ä¸­æ–­: {type(e).__name__}ï¼Œ{wait_time}ç§’åé‡è¯• (ç¬¬{attempt+1}/{max_retries}æ¬¡)")
                            import time
                            time.sleep(wait_time)
                            continue
                        else:
                            logger.error(f"âŒ PDFä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {e}")
                            return None
                    
        except Exception as e:
            logger.error(f"âŒ PDFä¸‹è½½å¼‚å¸¸: {e}")
            return None
    
    def _get_default_publication_title(self, paper_info: Dict) -> str:
        """æ ¹æ®è®ºæ–‡ä¿¡æ¯æ™ºèƒ½ç¡®å®šé»˜è®¤çš„æœŸåˆŠ/ä¼šè®®åç§°"""
        
        # ä¼˜å…ˆä½¿ç”¨å·²æå–çš„æœŸåˆŠä¿¡æ¯
        if paper_info.get('journal'):
            return paper_info['journal']
        
        if paper_info.get('publicationTitle'):
            return paper_info['publicationTitle']
        
        if paper_info.get('proceedingsTitle'):
            return paper_info['proceedingsTitle']
        
        # æ ¹æ®URLå’Œæå–å™¨ç±»å‹ç¡®å®šé»˜è®¤å€¼
        url = paper_info.get('url', '')
        extractor = paper_info.get('extractor', '')
        
        # arXivè®ºæ–‡
        if 'arxiv.org' in url:
            return 'arXiv'
        
        # ğŸ†• å…¶ä»–é¢„å°æœ¬æœåŠ¡å™¨
        if 'medrxiv.org' in url:
            return 'medRxiv'
        elif 'biorxiv.org' in url:
            return 'bioRxiv'
        elif 'chemrxiv.org' in url:
            return 'ChemRxiv'
        elif 'psyarxiv.com' in url:
            return 'PsyArXiv'
        elif 'socarxiv.org' in url:
            return 'SocArXiv'
        
        # CVFè®ºæ–‡
        if 'thecvf.com' in url or extractor.upper() == 'CVF':
            # ä»URLæ¨æ–­ä¼šè®®åç§°
            if '/ICCV' in url:
                return 'IEEE International Conference on Computer Vision (ICCV)'
            elif '/CVPR' in url:
                return 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR)'
            elif '/WACV' in url:
                return 'IEEE Winter Conference on Applications of Computer Vision (WACV)'
            else:
                return 'IEEE Computer Vision Conference'
        
        # Natureè®ºæ–‡
        if 'nature.com' in url or extractor.upper() == 'NATURE':
            return 'Nature'
        
        # æ ¹æ®æ¡ç›®ç±»å‹ç¡®å®šé»˜è®¤å€¼
        item_type = paper_info.get('itemType', '')
        if item_type == 'conferencePaper':
            return 'Conference Proceedings'
        elif item_type == 'preprint':
            return 'Preprint Server'
        
        # æœ€ç»ˆé»˜è®¤å€¼
        return 'Unknown Journal'
    
    def _save_via_connector(self, zotero_item: Dict, pdf_path: Optional[str] = None, 
                           collection_key: Optional[str] = None) -> Dict:
        """é€šè¿‡Connector APIä¿å­˜é¡¹ç›® - å®ç”¨è§£å†³æ–¹æ¡ˆ"""
        try:
            import time
            import json
            import requests
            
            session_id = f"success-test-{int(time.time() * 1000)}"
            
            # ğŸ¯ æŒ‰ç…§å®˜æ–¹æ’ä»¶æ–¹æ³•ï¼šç”ŸæˆéšæœºID
            import random
            import string
            
            # ç”Ÿæˆ8ä½éšæœºå­—ç¬¦ä¸²IDï¼ˆæ¨¡ä»¿å®˜æ–¹æ’ä»¶ï¼‰
            random_item_id = ''.join(random.choices(string.ascii_letters + string.digits, k=8))
            
            clean_item = {
                "itemType": zotero_item.get("itemType", "journalArticle"),
                "title": zotero_item.get("title", ""),
                "url": zotero_item.get("url", ""),
                "id": random_item_id,  # å…³é”®ï¼šæ·»åŠ éšæœºID
                "tags": [],
                "notes": [],
                "seeAlso": [],
                "attachments": []
            }
            
            # æ·»åŠ å®Œæ•´å…ƒæ•°æ® - ç¡®ä¿Commentä¿¡æ¯åœ¨Extraå­—æ®µä¸­
            if zotero_item.get("creators"):
                clean_item["creators"] = zotero_item["creators"]
            if zotero_item.get("abstractNote"):
                clean_item["abstractNote"] = zotero_item["abstractNote"]
            if zotero_item.get("date"):
                clean_item["date"] = zotero_item["date"]
            if zotero_item.get("publicationTitle"):
                clean_item["publicationTitle"] = zotero_item["publicationTitle"]
            if zotero_item.get("DOI"):
                clean_item["DOI"] = zotero_item["DOI"]
            
            # ğŸ¯ å…³é”®ï¼šç¡®ä¿Extraå­—æ®µï¼ˆåŒ…å«Commentï¼‰è¢«æ­£ç¡®ä¿å­˜
            if zotero_item.get("extra"):
                clean_item["extra"] = zotero_item["extra"]
                logger.info(f"âœ… Extraå­—æ®µï¼ˆåŒ…å«Commentï¼‰: {len(clean_item['extra'])} characters")
                # æ˜¾ç¤ºcommenté¢„è§ˆ
                if 'Comment:' in clean_item['extra']:
                    comment_line = [line for line in clean_item['extra'].split('\n') if 'Comment:' in line][0]
                    logger.info(f"ğŸ“ Commenté¢„è§ˆ: {comment_line}")
            
            # ç”Ÿæˆitem_idå’Œheadersï¼ˆéœ€è¦åœ¨PDFå¤„ç†å‰å®šä¹‰ï¼‰
            item_id = f"item_{int(time.time() * 1000)}"
            clean_item["id"] = item_id
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Content-Type': 'application/json',
                'X-Zotero-Version': '5.0.97',
                'X-Zotero-Connector-API-Version': '3'
            }
            
            session = requests.Session()
            session.headers.update(headers)
            
            # ğŸ¯ æœ€ç»ˆç­–ç•¥ï¼šä¸åœ¨saveItemsä¸­åŒ…å«é™„ä»¶ï¼Œç¨åæ‰‹åŠ¨è§¦å‘ä¸‹è½½
            pdf_url = zotero_item.get('pdf_url')
            
            if pdf_url:
                logger.info(f"ğŸ” å‘ç°PDFé“¾æ¥: {pdf_url}")
                logger.info("ğŸ“ å°†åœ¨ä¿å­˜åæ‰‹åŠ¨è§¦å‘PDFä¸‹è½½")
            
            # ä¸ºitemç”ŸæˆéšæœºID
            import random
            import string
            item_id = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(8))
            clean_item["id"] = item_id
            
            # æ·»åŠ é“¾æ¥é™„ä»¶ï¼ˆä¸ä¼šè¢«ä¸‹è½½çš„ï¼‰
            if pdf_url:
                if not clean_item.get("attachments"):
                    clean_item["attachments"] = []
                clean_item["attachments"].append({
                    "title": f"{clean_item.get('repository', 'Online')} Snapshot",
                    "url": clean_item.get('url', pdf_url),
                    "snapshot": False
                })
            
            # æ„å»ºä¿å­˜payload
            payload = {
                "sessionID": session_id,
                "uri": zotero_item.get("url", ""),
                "items": [clean_item]
            }
            
            # è®¾ç½®ç›®æ ‡é›†åˆ
            if collection_key:
                tree_view_id = self._get_collection_tree_view_id(collection_key)
                if tree_view_id:
                    payload["target"] = tree_view_id
                    logger.info(f"ğŸ¯ ä½¿ç”¨treeViewID: {tree_view_id}")
            
            # headerså’Œsessionå·²ç»åœ¨ä¸Šé¢å®šä¹‰äº†
            
            # ä¿å­˜é¡¹ç›®
            response = session.post(f"{self.base_url}/connector/saveItems", json=payload, timeout=30)
            
            if response.status_code not in [200, 201]:
                return {
                    "success": False,
                    "message": f"ä¿å­˜å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
                }
            
            logger.info("âœ… é¡¹ç›®ä¿å­˜æˆåŠŸ")
            
            # ğŸ¯ æ­£ç¡®çš„é™„ä»¶å¤„ç†ï¼šè°ƒç”¨saveAttachment APIä¿å­˜PDF
            pdf_attachment_success = False
            
            if pdf_url:
                logger.info(f"ğŸ” å‘ç°PDFé“¾æ¥: {pdf_url}")
                
                # ğŸš€ å…³é”®ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹
                try:
                    if zotero_item.get('pdf_content'):
                        logger.info("âœ… ä½¿ç”¨æµè§ˆå™¨é¢„ä¸‹è½½çš„PDFå†…å®¹ï¼Œè·³è¿‡HTTPä¸‹è½½")
                        pdf_content = zotero_item['pdf_content']
                    else:
                        logger.info("ğŸ“¥ å¼€å§‹ä¸‹è½½PDFå†…å®¹...")
                        pdf_content = self._download_pdf_content(pdf_url)
                    
                    if pdf_content:
                        # ğŸ” è¯Šæ–­ï¼šæ£€æŸ¥ä¸‹è½½å†…å®¹çš„å®é™…ç±»å‹
                        logger.info(f"ğŸ“Š PDFå†…å®¹å¤§å°: {len(pdf_content)} bytes")
                        
                        # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯PDFï¼ˆå‰å‡ ä¸ªå­—èŠ‚åº”è¯¥æ˜¯%PDFï¼‰
                        if pdf_content[:4] != b'%PDF':
                            logger.error(f"âŒ ä¸‹è½½çš„å†…å®¹ä¸æ˜¯PDFï¼å‰20å­—èŠ‚: {pdf_content[:20]}")
                            logger.warning("âš ï¸ å¯èƒ½ä¸‹è½½äº†HTMLé”™è¯¯é¡µé¢ï¼Œè·³è¿‡PDFä¿å­˜")
                        else:
                            logger.info(f"âœ… ç¡®è®¤æ˜¯PDFæ–‡ä»¶ï¼Œç‰ˆæœ¬æ ‡è¯†: {pdf_content[:8]}")
                        
                        # å‡†å¤‡é™„ä»¶å…ƒæ•°æ®
                        import random
                        import string
                        attachment_id = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(8))
                        
                        attachment_metadata = {
                            "id": attachment_id,
                            "url": pdf_url,
                            "contentType": "application/pdf",
                            "parentItemID": clean_item.get("id", ""),  # ä½¿ç”¨itemçš„ID
                            "title": "Full Text PDF"
                        }
                        
                        # è°ƒç”¨saveAttachment API
                        attachment_headers = {
                            "Content-Type": "application/pdf",
                            "X-Metadata": json.dumps(attachment_metadata)
                        }
                        
                        # ğŸ”§ Windowså…¼å®¹æ€§ï¼šå¢åŠ è¶…æ—¶æ—¶é—´ï¼Œå¯¹å¤§æ–‡ä»¶æ›´å®½å®¹
                        timeout_value = 60 if len(pdf_content) > 500000 else 30
                        logger.info(f"â±ï¸ ä½¿ç”¨è¶…æ—¶æ—¶é—´: {timeout_value}ç§’")
                        
                        attachment_response = session.post(
                            f"{self.base_url}/connector/saveAttachment?sessionID={session_id}",
                            data=pdf_content,
                            headers=attachment_headers,
                            timeout=timeout_value
                        )
                        
                        if attachment_response.status_code in [200, 201]:
                            pdf_attachment_success = True
                            logger.info("âœ… PDFé™„ä»¶ä¿å­˜æˆåŠŸï¼")
                        else:
                            logger.warning(f"âš ï¸ PDFé™„ä»¶ä¿å­˜å¤±è´¥: {attachment_response.status_code}")
                            logger.warning(f"âš ï¸ å®Œæ•´å“åº”å†…å®¹: {attachment_response.text}")
                            logger.warning(f"âš ï¸ å“åº”Headers: {dict(attachment_response.headers)}")
                            
                            # ğŸ” é¢å¤–è¯Šæ–­ä¿¡æ¯
                            logger.info(f"ğŸ” è¯·æ±‚URL: {self.base_url}/connector/saveAttachment?sessionID={session_id}")
                            logger.info(f"ğŸ” è¯·æ±‚Headers: {attachment_headers}")
                            logger.info(f"ğŸ” PDFå¤§å°: {len(pdf_content)} bytes")
                            logger.info(f"ğŸ” PDFå‰8å­—èŠ‚: {pdf_content[:8]}")
                            
                            # ğŸ”§ Windowså…¼å®¹æ€§ï¼šå°è¯•å¤‡ç”¨æ–¹æ³•
                            if attachment_response.status_code == 500:
                                logger.info("ğŸ”„ å°è¯•å¤‡ç”¨PDFä¿å­˜æ–¹æ³•...")
                                try:
                                    # æ–¹æ³•2ï¼šä½¿ç”¨åŸºç¡€çš„æ–‡ä»¶ä¸Šä¼ æ–¹å¼
                                    files = {
                                        'file': ('document.pdf', pdf_content, 'application/pdf')
                                    }
                                    backup_response = session.post(
                                        f"{self.base_url}/connector/saveAttachment?sessionID={session_id}",
                                        files=files,
                                        timeout=30
                                    )
                                    if backup_response.status_code in [200, 201]:
                                        pdf_attachment_success = True
                                        logger.info("âœ… å¤‡ç”¨æ–¹æ³•PDFä¿å­˜æˆåŠŸï¼")
                                    else:
                                        logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {backup_response.status_code}")
                                        logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•å“åº”: {backup_response.text}")
                                        logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•Headers: {dict(backup_response.headers)}")
                                except Exception as backup_e:
                                    logger.warning(f"âš ï¸ å¤‡ç”¨æ–¹æ³•å¼‚å¸¸: {backup_e}")
                    else:
                        logger.warning("âš ï¸ PDFå†…å®¹ä¸‹è½½å¤±è´¥")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ PDFå¤„ç†å¼‚å¸¸: {e}")
                

            
            # ç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ
            collection_move_success = False
            if collection_key:
                tree_view_id = self._get_collection_tree_view_id(collection_key)
                if tree_view_id:
                    try:
                        update_data = {"sessionID": session_id, "target": tree_view_id}
                        update_response = session.post(f"{self.base_url}/connector/updateSession", json=update_data, timeout=30)
                        if update_response.status_code in [200, 201]:
                            collection_move_success = True
                            logger.info("âœ… æˆåŠŸç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ é›†åˆç§»åŠ¨å¤±è´¥: {e}")
            
            # æ„å»ºç»“æœ
            result = {
                "success": True,
                "message": "è®ºæ–‡å·²æˆåŠŸä¿å­˜" + ("ï¼ŒPDFé™„ä»¶å·²æ·»åŠ " if pdf_attachment_success else ""),
                "details": {
                    "metadata_saved": True,
                    "collection_moved": collection_move_success,
                    "pdf_downloaded": pdf_attachment_success,
                    "pdf_error": None if pdf_attachment_success else "PDFé™„ä»¶ä¿å­˜å¤±è´¥" if pdf_url else None,
                    "pdf_method": "attachment" if pdf_attachment_success else "failed" if pdf_url else "none"
                }
            }
            
            return result
                        
        except Exception as e:
            logger.error(f"âŒ å®ç”¨æ–¹æ¡ˆä¿å­˜å¼‚å¸¸: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "success": False,
                "message": f"ä¿å­˜å¤±è´¥: {e}"
            }
    
    def _download_arxiv_pdf(self, arxiv_id: str, title: str) -> Optional[str]:
        """ä¸‹è½½arxiv PDFåˆ°ä¸´æ—¶ç›®å½•"""
        try:
            import tempfile
            import urllib.request
            from urllib.parse import quote
            
            # åˆ›å»ºä¸´æ—¶ä¸‹è½½ç›®å½•
            temp_dir = Path(tempfile.gettempdir()) / "zotero_pdfs"
            temp_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()
            safe_title = safe_title[:50] if len(safe_title) > 50 else safe_title
            pdf_filename = f"{arxiv_id}_{safe_title}.pdf"
            pdf_path = temp_dir / pdf_filename
            
            # ä¸‹è½½PDF
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            logger.info(f"ä¸‹è½½PDF: {pdf_url}")
            
            urllib.request.urlretrieve(pdf_url, pdf_path)
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸‹è½½æˆåŠŸä¸”æ˜¯PDF
            if pdf_path.exists() and pdf_path.stat().st_size > 1024:  # è‡³å°‘1KB
                logger.info(f"PDFä¸‹è½½æˆåŠŸ: {pdf_path}")
                return str(pdf_path)
            else:
                logger.warning("PDFä¸‹è½½å¤±è´¥æˆ–æ–‡ä»¶å¤ªå°")
                return None
                
        except Exception as e:
            logger.error(f"ä¸‹è½½PDFå¤±è´¥: {e}")
            return None
    
    def _attach_pdf_to_item(self, item_key: str, pdf_path: str, title: str) -> bool:
        """å°†PDFé™„åŠ åˆ°Zoteroæ¡ç›®"""
        try:
            if not Path(pdf_path).exists():
                logger.error(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                return False
            
            # å‡†å¤‡é™„ä»¶æ•°æ®
            attachment_data = {
                "itemType": "attachment",
                "parentItem": item_key,
                "linkMode": "imported_file",
                "title": f"{title} - PDF",
                "filename": Path(pdf_path).name,
                "path": pdf_path,
                "contentType": "application/pdf"
            }
            
            # å°è¯•ä¸åŒçš„é™„ä»¶ä¸Šä¼ ç«¯ç‚¹
            attachment_endpoints = [
                "/connector/attachments",
                "/connector/saveItems",
                "/attachments"
            ]
            
            for endpoint in attachment_endpoints:
                try:
                    # ä½¿ç”¨multipart/form-dataä¸Šä¼ æ–‡ä»¶
                    import requests
                    files = {
                        'file': (Path(pdf_path).name, open(pdf_path, 'rb'), 'application/pdf')
                    }
                    data = {
                        'data': json.dumps(attachment_data)
                    }
                    
                    response = self.session.post(
                        f"{self.base_url}{endpoint}",
                        files=files,
                        data=data,
                        timeout=60
                    )
                    
                    files['file'][1].close()  # å…³é—­æ–‡ä»¶
                    
                    if response.status_code in [200, 201]:
                        logger.info(f"PDFé™„ä»¶ä¸Šä¼ æˆåŠŸ: {endpoint}")
                        return True
                        
                except Exception as e:
                    logger.debug(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}ä¸Šä¼ é™„ä»¶å¤±è´¥: {e}")
                    continue
            
            logger.warning("æ‰€æœ‰é™„ä»¶ä¸Šä¼ ç«¯ç‚¹éƒ½å¤±è´¥äº†")
            return False
            
        except Exception as e:
            logger.error(f"é™„ä»¶ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def create_collection(self, name: str, parent_key: Optional[str] = None) -> Dict:
        """åˆ›å»ºæ–°é›†åˆ"""
        try:
            if not self.is_running():
                return {
                    "success": False,
                    "message": "Zoteroæœªè¿è¡Œï¼Œè¯·å¯åŠ¨Zoteroæ¡Œé¢åº”ç”¨"
                }
            
            collection_data = {
                "name": name,
                "parentCollection": parent_key if parent_key else False
            }
            
            # å°è¯•ä¸åŒçš„åˆ›å»ºç«¯ç‚¹
            create_endpoints = [
                "/api/users/local/collections",
                "/connector/createCollection", 
                "/api/collections"
            ]
            
            for endpoint in create_endpoints:
                try:
                    response = self.session.post(
                        f"{self.base_url}{endpoint}",
                        json=collection_data,
                        timeout=15
                    )
                    
                    if response.status_code in [200, 201]:
                        try:
                            result = response.json()
                            collection_key = result.get('key', '')
                            logger.info(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}æˆåŠŸåˆ›å»ºé›†åˆ: {name}")
                            
                            return {
                                "success": True,
                                "message": f"æˆåŠŸåˆ›å»ºé›†åˆ: {name}",
                                "collection_key": collection_key,
                                "collection_name": name
                            }
                        except json.JSONDecodeError:
                            # å³ä½¿æ²¡æœ‰JSONå“åº”ï¼Œå¦‚æœçŠ¶æ€ç æ­£ç¡®ä¹Ÿè®¤ä¸ºæˆåŠŸ
                            logger.info(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}åˆ›å»ºé›†åˆæˆåŠŸï¼ˆæ— JSONå“åº”ï¼‰")
                            return {
                                "success": True,
                                "message": f"æˆåŠŸåˆ›å»ºé›†åˆ: {name}",
                                "collection_key": "",
                                "collection_name": name
                            }
                    
                    elif response.status_code == 404:
                        logger.debug(f"åˆ›å»ºç«¯ç‚¹ä¸å­˜åœ¨: {endpoint}")
                        continue
                    else:
                        logger.debug(f"ç«¯ç‚¹{endpoint}è¿”å›çŠ¶æ€ç : {response.status_code}")
                        continue
                        
                except Exception as e:
                    logger.debug(f"ä½¿ç”¨ç«¯ç‚¹{endpoint}åˆ›å»ºå¤±è´¥: {e}")
                    continue
            
            # å¦‚æœæ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥äº†
            return {
                "success": False,
                "message": "æ‰€æœ‰åˆ›å»ºç«¯ç‚¹éƒ½ä¸å¯ç”¨ï¼Œå¯èƒ½éœ€è¦æ›´æ–°çš„Zoteroç‰ˆæœ¬æˆ–æ‰‹åŠ¨åœ¨Zoteroä¸­åˆ›å»ºé›†åˆ"
            }
                
        except Exception as e:
            logger.error(f"åˆ›å»ºZoteroé›†åˆå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"åˆ›å»ºé›†åˆå¤±è´¥: {e}"
            }

    def _get_collection_tree_view_id(self, collection_key: str) -> Optional[str]:
        """æ ¹æ®collection keyè·å–treeViewIDæ ¼å¼"""
        try:
            # ä»æ•°æ®åº“ä¸­æŸ¥æ‰¾collection ID
            if not self._zotero_db_path or not self._zotero_db_path.exists():
                return None
                
            import tempfile
            import shutil
            import sqlite3
            
            with tempfile.NamedTemporaryFile(suffix='.sqlite', delete=False) as temp_file:
                shutil.copy2(self._zotero_db_path, temp_file.name)
                temp_db_path = temp_file.name
                
            try:
                conn = sqlite3.connect(temp_db_path)
                cursor = conn.cursor()
                
                # æ ¹æ®keyæŸ¥æ‰¾collectionID
                cursor.execute(
                    'SELECT collectionID FROM collections WHERE key = ?', 
                    (collection_key,)
                )
                
                result = cursor.fetchone()
                if result:
                    collection_id = result[0]
                    tree_view_id = f"C{collection_id}"
                    logger.info(f"ğŸ¯ è½¬æ¢: {collection_key} â†’ {tree_view_id}")
                    return tree_view_id
                else:
                    logger.warning(f"âš ï¸ æ‰¾ä¸åˆ°collection key: {collection_key}")
                    return None
                    
                conn.close()
                
            finally:
                try:
                    Path(temp_db_path).unlink()
                except:
                    pass
                    
        except Exception as e:
            logger.error(f"âŒ è·å–treeViewIDå¤±è´¥: {e}")
            return None
    
    def set_database_cookies(self, database_name: str, cookies: str) -> bool:
        """
        ä¸ºç‰¹å®šæ•°æ®åº“è®¾ç½®cookies
        
        Args:
            database_name: æ•°æ®åº“åç§°ï¼ˆå¦‚"Nature"ï¼‰
            cookies: cookieå­—ç¬¦ä¸²
            
        Returns:
            bool: è®¾ç½®æˆåŠŸè¿”å›True
        """
        if not self.extractor_manager:
            logger.error("âŒ æå–å™¨ç®¡ç†å™¨ä¸å¯ç”¨")
            return False
        
        return self.extractor_manager.set_database_cookies(database_name, cookies)
    
    def get_supported_databases(self) -> List[Dict]:
        """è·å–æ”¯æŒçš„æ•°æ®åº“åˆ—è¡¨"""
        if not self.extractor_manager:
            return [{'name': 'arXiv', 'requires_auth': False, 'has_cookies': False}]
        
        databases = self.extractor_manager.get_supported_databases()
        
        # æ·»åŠ å†…ç½®çš„arXivæ”¯æŒ
        databases.insert(0, {
            'name': 'arXiv',
            'requires_auth': False,
            'has_cookies': False,
            'supported_types': ['preprint']
        })
        
        return databases
    
    def test_database_access(self, database_name: str) -> Dict:
        """æµ‹è¯•æ•°æ®åº“è®¿é—®çŠ¶æ€"""
        if database_name.lower() == 'arxiv':
            return {
                'database': 'arXiv',
                'status': 'success',
                'message': 'arXivæ— éœ€è®¤è¯ï¼Œè®¿é—®æ­£å¸¸'
            }
        
        if not self.extractor_manager:
            return {
                'database': database_name,
                'status': 'not_supported',
                'message': 'æå–å™¨ç®¡ç†å™¨ä¸å¯ç”¨'
            }
        
        return self.extractor_manager.test_database_access(database_name)
    
    def _quick_validate_pdf_link(self, pdf_url: str) -> bool:
        """å¿«é€ŸéªŒè¯PDFé“¾æ¥æ˜¯å¦å¯ç”¨"""
        
        if not pdf_url:
            return False
        
        try:
            import requests
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/pdf,*/*;q=0.8',
            })
            
            # ä½¿ç”¨HEADè¯·æ±‚å¿«é€Ÿæ£€æŸ¥ï¼Œè¶…æ—¶5ç§’
            response = session.head(pdf_url, timeout=5, allow_redirects=True)
            
            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code == 200:
                # æ£€æŸ¥Content-Typeï¼ˆå…è®¸OSFçš„octet-streamï¼‰
                content_type = response.headers.get('Content-Type', '').lower()
                if 'pdf' in content_type or content_type == 'application/octet-stream':
                    logger.info(f"ğŸ” PDFé“¾æ¥éªŒè¯é€šè¿‡: {response.status_code}, {content_type}")
                    return True
                else:
                    logger.warning(f"âš ï¸ PDFé“¾æ¥Content-Typeå¼‚å¸¸: {content_type}")
                    return False
            elif response.status_code == 403:
                logger.warning(f"âš ï¸ PDFé“¾æ¥è¢«403é˜»æ­¢: {pdf_url[:60]}...")
                return False
            elif response.status_code == 404:
                logger.warning(f"âš ï¸ PDFé“¾æ¥ä¸å­˜åœ¨: {pdf_url[:60]}...")
                return False
            else:
                logger.warning(f"âš ï¸ PDFé“¾æ¥çŠ¶æ€å¼‚å¸¸: {response.status_code}")
                return False
                
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ PDFé“¾æ¥éªŒè¯è¶…æ—¶")
            return False
        except requests.exceptions.ConnectionError:
            logger.warning(f"âš ï¸ PDFé“¾æ¥è¿æ¥å¤±è´¥")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ PDFé“¾æ¥éªŒè¯å¼‚å¸¸: {e}")
            return False

    def _enhance_paper_metadata(self, paper_info: Dict) -> Dict:
        """
        ä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¢å¼ºè®ºæ–‡å…ƒæ•°æ®ï¼ˆæ”¯æŒå¼‚æ­¥æµè§ˆå™¨æ¨¡å¼ï¼‰
        
        Args:
            paper_info: åŸºæœ¬è®ºæ–‡ä¿¡æ¯
            
        Returns:
            Dict: å¢å¼ºåçš„è®ºæ–‡ä¿¡æ¯
        """
        url = paper_info.get('url', '')
        
        # ğŸ”§ ä¿®å¤ï¼šç²¾ç¡®æ£€æŸ¥arXivï¼Œé¿å…è¯¯åŒ¹é…SocArXivç­‰
        if re.search(r'(?<!soc)(?<!med)(?<!bio)arxiv\.org', url):
            return self._enhance_paper_info_for_arxiv(paper_info)
        
        # ä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¤„ç†å…¶ä»–æ•°æ®åº“
        if self.extractor_manager:
            logger.info("ğŸ”„ ä½¿ç”¨æå–å™¨ç®¡ç†å™¨å¢å¼ºå…ƒæ•°æ®...")
            
            # ğŸš€ å…³é”®ä¿®å¤ï¼šæ”¯æŒå¼‚æ­¥æµè§ˆå™¨æ¨¡å¼
            try:
                # æ£€æŸ¥æ˜¯å¦å·²åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
                try:
                    loop = asyncio.get_running_loop()
                    logger.info("ğŸ”„ åœ¨ç°æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æå–...")
                    # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œéœ€è¦ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
                    enhanced_metadata = self._run_async_extraction(url)
                except RuntimeError:
                    # æ²¡æœ‰è¿è¡Œçš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                    logger.info("ğŸ”„ åˆ›å»ºæ–°äº‹ä»¶å¾ªç¯è¿è¡Œå¼‚æ­¥æå–...")
                    enhanced_metadata = asyncio.run(self.extractor_manager.extract_metadata(url))
            except Exception as e:
                logger.error(f"âŒ å¼‚æ­¥æå–å¤±è´¥: {e}")
                enhanced_metadata = {'error': f'å¼‚æ­¥æå–å¤±è´¥: {e}'}
            
            if 'error' not in enhanced_metadata:
                # åˆå¹¶åŸå§‹ä¿¡æ¯å’Œå¢å¼ºä¿¡æ¯
                enhanced_info = paper_info.copy()
                enhanced_info.update(enhanced_metadata)
                
                logger.info(f"âœ… å…ƒæ•°æ®å¢å¼ºæˆåŠŸ: {enhanced_info.get('title', 'Unknown')}")
                return enhanced_info
            else:
                logger.warning(f"âš ï¸ æå–å™¨å¤„ç†å¤±è´¥: {enhanced_metadata['error']}")
                
                # ğŸ”§ ç½‘ç»œå¤±è´¥æ—¶çš„æ™ºèƒ½å›é€€ï¼šå°è¯•åŸºäºURLçš„æ¨¡å¼æå–
                logger.info("ğŸ”„ å°è¯•åŸºäºURLçš„ç¦»çº¿æ¨¡å¼æå–...")
                
                try:
                    # è·å–é€šç”¨æå–å™¨è¿›è¡ŒURLæ¨¡å¼æå–
                    for extractor in self.extractor_manager.extractors:
                        if extractor.__class__.__name__ == 'GenericOpenAccessExtractor':
                            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†è¿™ä¸ªURL
                            if extractor.can_handle(url):
                                logger.info("ğŸ“ æ‰¾åˆ°é€šç”¨æå–å™¨ï¼Œæ‰§è¡ŒURLæ¨¡å¼æå–")
                                
                                # åº”ç”¨URLæ¨¡å¼æå–
                                url_metadata = extractor._extract_from_url_patterns({}, url)
                                
                                # ğŸ”§ æ–°å¢ï¼šå°è¯•PDFé“¾æ¥æ„é€ 
                                if not url_metadata.get('pdf_url'):
                                    url_metadata = extractor._search_pdf_links_in_html("", url, url_metadata)
                                
                                if url_metadata.get('pdf_url'):
                                    logger.info(f"âœ… ç¦»çº¿æ¨¡å¼æå–æˆåŠŸï¼Œæ‰¾åˆ°PDFé“¾æ¥")
                                    
                                    # ğŸ”§ æ–°å¢ï¼šéªŒè¯PDFé“¾æ¥çš„å®é™…å¯ç”¨æ€§
                                    pdf_url = url_metadata.get('pdf_url')
                                    pdf_valid = self._quick_validate_pdf_link(pdf_url)
                                    
                                    if pdf_valid:
                                        logger.info(f"âœ… PDFé“¾æ¥éªŒè¯é€šè¿‡")
                                        
                                        # è¯†åˆ«åŸŸåä¿¡æ¯
                                        domain_info = extractor._identify_domain(url)
                                        
                                        # æ„å»ºåŸºç¡€å…ƒæ•°æ®
                                        fallback_info = paper_info.copy()
                                        fallback_info.update(url_metadata)
                                        fallback_info.update({
                                            'itemType': domain_info['type'],
                                            'source': domain_info['source'],
                                            'extractor': f"Generic-{domain_info['source']}",
                                            'url': url
                                        })
                                        
                                        # å¢å¼ºé¢„å°æœ¬å­—æ®µ
                                        fallback_info = extractor._enhance_preprint_fields(fallback_info, url)
                                        
                                        logger.info(f"ğŸ¯ ç¦»çº¿å›é€€æˆåŠŸ: {fallback_info.get('repository', 'Unknown')} - PDFé“¾æ¥å·²éªŒè¯")
                                        return fallback_info
                                    else:
                                        logger.warning(f"âš ï¸ PDFé“¾æ¥éªŒè¯å¤±è´¥ï¼Œç»§ç»­å›é€€æµç¨‹")
                                        # ç§»é™¤æ— æ•ˆçš„PDFé“¾æ¥ï¼Œé¿å…è¯¯å¯¼ç”¨æˆ·
                                        url_metadata['pdf_url'] = None
                                else:
                                    logger.warning(f"âš ï¸ ç¦»çº¿æ¨¡å¼æå–æœªæ‰¾åˆ°PDFé“¾æ¥")
                                break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„æå–å™¨æˆ–æå–å¤±è´¥ï¼Œè¿”å›åŸå§‹é”™è¯¯
                    logger.warning(f"âŒ ç¦»çº¿å›é€€ä¹Ÿå¤±è´¥ï¼Œè¿”å›åŸå§‹é”™è¯¯")
                    return enhanced_metadata
                    
                except Exception as e:
                    logger.error(f"âŒ ç¦»çº¿å›é€€è¿‡ç¨‹å‡ºé”™: {e}")
                    return enhanced_metadata
        
        # å¦‚æœæ²¡æœ‰æå–å™¨ç®¡ç†å™¨ï¼Œè¿”å›åŸå§‹ä¿¡æ¯
        logger.info("â„¹ï¸ ä½¿ç”¨åŸºæœ¬è®ºæ–‡ä¿¡æ¯")
        return paper_info

    def _run_async_extraction(self, url: str) -> Dict:
        """
        åœ¨ç°æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æå–çš„è¾…åŠ©æ–¹æ³•
        """
        import concurrent.futures
        import threading
        import asyncio
        
        try:
            # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ
            def run_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        self.extractor_manager.extract_metadata(url)
                    )
                finally:
                    new_loop.close()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_in_thread)
                return future.result(timeout=180)  # å¢åŠ åˆ°180ç§’è¶…æ—¶ï¼Œç»™æµè§ˆå™¨è¶³å¤Ÿæ—¶é—´
                
        except concurrent.futures.TimeoutError:
            logger.error("âŒ æµè§ˆå™¨æ¨¡å¼è¶…æ—¶ï¼ˆè¶…è¿‡3åˆ†é’Ÿï¼‰")
            return {'error': 'æµè§ˆå™¨æ¨¡å¼è¶…æ—¶ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–åçˆ¬è™«æœºåˆ¶å‡çº§'}
        except Exception as e:
            logger.error(f"âŒ çº¿ç¨‹æ± æ‰§è¡Œå¼‚å¸¸: {e}")
            return {'error': f'çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {e}'}

    def _validate_pdf_content(self, pdf_data: bytes, headers: dict, pdf_url: str) -> dict:
        """
        éªŒè¯ä¸‹è½½çš„PDFå†…å®¹æ˜¯å¦æœ‰æ•ˆå’Œå®Œæ•´
        
        Args:
            pdf_data: PDFæ–‡ä»¶äºŒè¿›åˆ¶æ•°æ®
            headers: HTTPå“åº”å¤´
            pdf_url: PDFä¸‹è½½URL
            
        Returns:
            dict: éªŒè¯ç»“æœ {"is_valid": bool, "reason": str, "details": dict}
        """
        try:
            pdf_size = len(pdf_data)
            content_type = headers.get('Content-Type', '').lower()
            
            logger.info(f"ğŸ” PDFéªŒè¯å¼€å§‹: {pdf_size} bytes, Content-Type: {content_type}")
            
            # æ£€æŸ¥1: åŸºæœ¬å¤§å°éªŒè¯
            if pdf_size < 1024:  # å°äº1KBè‚¯å®šæœ‰é—®é¢˜
                return {
                    "is_valid": False,
                    "reason": f"æ–‡ä»¶å¤ªå° ({pdf_size} bytes)ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢",
                    "details": {"size": pdf_size, "content_preview": pdf_data[:200].decode('utf-8', errors='ignore')[:100]}
                }
            
            # æ£€æŸ¥2: Content-TypeéªŒè¯ (ğŸ”§ ä¿®å¤ï¼šå…è®¸OSFçš„octet-streamæ ¼å¼)
            if content_type and 'pdf' not in content_type:
                # ğŸ”§ ç‰¹æ®Šå¤„ç†ï¼šapplication/octet-streamå¯èƒ½æ˜¯æœ‰æ•ˆPDFï¼ˆå¦‚OSFï¼‰
                if content_type != 'application/octet-stream':
                    return {
                        "is_valid": False,
                        "reason": f"Content-Typeä¸æ˜¯PDF: {content_type}",
                        "details": {"content_type": content_type, "size": pdf_size}
                    }
                else:
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ°octet-streamç±»å‹ï¼Œå°†é€šè¿‡PDFé­”æœ¯å­—èŠ‚éªŒè¯")
            
            # æ£€æŸ¥3: PDFé­”æœ¯å­—èŠ‚
            if not pdf_data.startswith(b'%PDF'):
                return {
                    "is_valid": False,
                    "reason": "æ–‡ä»¶ä¸ä»¥PDFé­”æœ¯å­—èŠ‚å¼€å¤´",
                    "details": {"size": pdf_size, "start_bytes": pdf_data[:20].hex()}
                }
            
            # æ£€æŸ¥4: HTMLå†…å®¹æ£€æµ‹ï¼ˆæœ‰äº›æœåŠ¡å™¨è¿”å›HTMLé¡µé¢ä½†ä¼ªé€ PDFå¤´ï¼‰
            pdf_text = pdf_data[:2048].decode('utf-8', errors='ignore').lower()
            html_indicators = ['<html', '<body', '<div', '<!doctype', '<title>']
            found_html = [indicator for indicator in html_indicators if indicator in pdf_text]
            
            if found_html:
                return {
                    "is_valid": False,
                    "reason": f"æ–‡ä»¶åŒ…å«HTMLå†…å®¹ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢: {found_html}",
                    "details": {"size": pdf_size, "html_indicators": found_html}
                }
            
            # æ£€æŸ¥5: Natureç‰¹å®šçš„å¤§å°éªŒè¯
            if 'nature.com' in pdf_url.lower():
                if pdf_size < 500000:  # Nature PDFé€šå¸¸è‡³å°‘500KB
                    logger.warning(f"âš ï¸ Nature PDFå¤§å°å¼‚å¸¸: {pdf_size} bytes (é€šå¸¸åº”è¯¥>500KB)")
                    return {
                        "is_valid": False,
                        "reason": f"Nature PDFå¤§å°å¼‚å¸¸: {pdf_size/1024:.1f}KB (é€šå¸¸åº”è¯¥>500KB)",
                        "details": {"size": pdf_size, "expected_min_size": 500000, "url": pdf_url}
                    }
            
            # æ£€æŸ¥6: PDFç»“æ„åŸºæœ¬éªŒè¯
            if b'%%EOF' not in pdf_data[-1024:]:  # PDFæ–‡ä»¶åº”è¯¥ä»¥%%EOFç»“å°¾
                logger.warning("âš ï¸ PDFæ–‡ä»¶å¯èƒ½ä¸å®Œæ•´ï¼ˆç¼ºå°‘EOFæ ‡è®°ï¼‰")
                return {
                    "is_valid": False,
                    "reason": "PDFæ–‡ä»¶ä¸å®Œæ•´ï¼ˆç¼ºå°‘ç»“å°¾æ ‡è®°ï¼‰",
                    "details": {"size": pdf_size, "has_eof": False}
                }
            
            # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
            logger.info(f"âœ… PDFéªŒè¯é€šè¿‡: {pdf_size} bytes ({pdf_size/1024:.1f}KB)")
            return {
                "is_valid": True,
                "reason": "PDFéªŒè¯é€šè¿‡",
                "details": {
                    "size": pdf_size,
                    "size_kb": round(pdf_size/1024, 1),
                    "content_type": content_type,
                    "has_pdf_header": True,
                    "has_eof": True
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ PDFéªŒè¯è¿‡ç¨‹å¼‚å¸¸: {e}")
            return {
                "is_valid": False,
                "reason": f"PDFéªŒè¯å¼‚å¸¸: {e}",
                "details": {"exception": str(e)}
            }

    def _analyze_pdf_status(self, pdf_success: bool, pdf_attempts: int, pdf_errors: list) -> dict:
        """
        åˆ†æPDFä¸‹è½½å’Œä¿å­˜çŠ¶æ€
        
        Args:
            pdf_success: PDFæ˜¯å¦æˆåŠŸ
            pdf_attempts: PDFå°è¯•æ¬¡æ•°  
            pdf_errors: PDFé”™è¯¯åˆ—è¡¨
            
        Returns:
            dict: PDFçŠ¶æ€åˆ†æç»“æœ
        """
        if pdf_attempts == 0:
            return {
                "status": "none",
                "message": "æœªå‘ç°PDFä¸‹è½½é“¾æ¥",
                "success": False,
                "details": "è®ºæ–‡å¯èƒ½ä¸åŒ…å«å¯ä¸‹è½½çš„PDFï¼Œæˆ–éœ€è¦ç‰¹æ®Šæƒé™"
            }
        
        if pdf_success:
            return {
                "status": "success", 
                "message": "PDFé™„ä»¶ä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ",
                "success": True,
                "details": f"æˆåŠŸå¤„ç† {pdf_attempts} ä¸ªPDFé™„ä»¶"
            }
        else:
            error_summary = "; ".join(pdf_errors) if pdf_errors else "æœªçŸ¥é”™è¯¯"
            return {
                "status": "failed",
                "message": "PDFä¸‹è½½å¤±è´¥", 
                "success": False,
                "details": error_summary,
                "suggestion": self._get_pdf_error_suggestion(pdf_errors)
            }
    
    def _get_pdf_error_suggestion(self, pdf_errors: list) -> str:
        """æ ¹æ®PDFé”™è¯¯æä¾›è§£å†³å»ºè®®"""
        if not pdf_errors:
            return "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒPDFé“¾æ¥æœ‰æ•ˆæ€§"
        
        error_text = " ".join(pdf_errors).lower()
        
        if "403" in error_text or "è®¤è¯" in error_text:
            return "è¯·åœ¨Claude Desktopä¸­è®¾ç½®æœ‰æ•ˆçš„æ•°æ®åº“è®¤è¯cookies"
        elif "404" in error_text:
            return "PDFé“¾æ¥å¯èƒ½å·²å¤±æ•ˆï¼Œè¯·å°è¯•å…¶ä»–ä¸‹è½½æº"
        elif "html" in error_text:
            return "ä¸‹è½½åˆ°ç™»å½•é¡µé¢ï¼Œéœ€è¦æ›´æ–°è®¤è¯ä¿¡æ¯"
        else:
            return "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¨åé‡è¯•"
    
    def _generate_save_message(self, pdf_status: dict, collection_moved: bool) -> str:
        """
        ç”Ÿæˆä¿å­˜ç»“æœçš„ç”¨æˆ·å‹å¥½æ¶ˆæ¯
        
        Args:
            pdf_status: PDFçŠ¶æ€ä¿¡æ¯
            collection_moved: æ˜¯å¦ç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ
            
        Returns:
            str: ç”¨æˆ·æ¶ˆæ¯
        """
        base_msg = "âœ… è®ºæ–‡åŸºæœ¬ä¿¡æ¯å·²ä¿å­˜åˆ°Zotero"
        
        if pdf_status.get("success", False):
            base_msg += "\nâœ… PDFé™„ä»¶ä¸‹è½½å¹¶ä¿å­˜æˆåŠŸ"
        elif pdf_status.get("status") == "none":
            base_msg += "\nâ„¹ï¸ æœªå‘ç°å¯ä¸‹è½½çš„PDFé“¾æ¥"
        else:
            base_msg += f"\nâš ï¸ PDFä¸‹è½½å¤±è´¥: {pdf_status.get('details', 'æœªçŸ¥åŸå› ')}"
            if pdf_status.get("suggestion"):
                base_msg += f"\nğŸ’¡ å»ºè®®: {pdf_status['suggestion']}"
        
        if collection_moved:
            base_msg += "\nâœ… å·²ç§»åŠ¨åˆ°æŒ‡å®šé›†åˆ"
        
        return base_msg
    
    def load_cookies_from_files(self) -> Dict[str, bool]:
        """
        ä»æ–‡ä»¶åŠ è½½æ‰€æœ‰å¯ç”¨çš„cookies
        æ”¯æŒå¤šç§æ ¼å¼å’Œä½ç½®ï¼š
        1. ~/.zotlink/cookies.json (æ¨èä½ç½®ï¼Œå¤šæ•°æ®åº“)
        2. é¡¹ç›®æ ¹ç›®å½•/cookies.json (å‘åå…¼å®¹)
        3. shared_cookies_*.json (ä¹¦ç­¾åŒæ­¥)
        4. ~/.zotlink/nature_cookies.txt (å‘åå…¼å®¹)
        
        Returns:
            Dict[str, bool]: æ¯ä¸ªæ•°æ®åº“çš„åŠ è½½çŠ¶æ€
        """
        import os
        from pathlib import Path
        import time
        from datetime import datetime, timezone
        
        results = {}
        # ä¼˜å…ˆçº§ï¼šç”¨æˆ·é…ç½®ç›®å½• > é¡¹ç›®æ ¹ç›®å½•
        user_config_dir = Path.home() / '.zotlink'
        project_root = Path(__file__).parent.parent
        
        # ç¡®ä¿ç”¨æˆ·é…ç½®ç›®å½•å­˜åœ¨
        user_config_dir.mkdir(exist_ok=True)
        
        logger.info("ğŸ” æ­£åœ¨æ‰«æcookieæ–‡ä»¶...")
        
        # 1. ä¼˜å…ˆåŠ è½½cookies.jsonï¼ˆä¸»é…ç½®æ–‡ä»¶ï¼‰- ä¼˜å…ˆä»ç”¨æˆ·é…ç½®ç›®å½•åŠ è½½
        json_config_paths = [
            user_config_dir / "cookies.json",  # æ¨èä½ç½®
            project_root / "cookies.json"      # å‘åå…¼å®¹
        ]
        
        json_config_file = None
        for path in json_config_paths:
            if path.exists():
                json_config_file = path
                break
        
        if json_config_file:
            logger.info(f"ğŸ“ æ‰¾åˆ°ä¸»Cookieé…ç½®æ–‡ä»¶: {json_config_file}")
            try:
                with open(json_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                databases = config.get('databases', {})
                loaded_count = 0
                
                for db_key, db_config in databases.items():
                    if db_config.get('status') == 'active' and db_config.get('cookies'):
                        cookies_str = db_config['cookies']
                        cookie_count = db_config.get('cookie_count', len(cookies_str.split(';')))
                        db_name = db_config.get('name', db_key)
                        
                        # è®¾ç½®åˆ°å¯¹åº”æ•°æ®åº“
                        success = self.set_database_cookies(db_key, cookies_str)
                        if success:
                            logger.info(f"âœ… ä»JSONåŠ è½½ {db_name} cookiesæˆåŠŸï¼š{cookie_count}ä¸ªcookies")
                            loaded_count += 1
                        else:
                            logger.error(f"âŒ è®¾ç½® {db_name} cookieså¤±è´¥")
                        results[db_key] = success
                    else:
                        logger.info(f"â¸ï¸ {db_config.get('name', db_key)}: æœªæ¿€æ´»æˆ–æ— cookies")
                        results[db_key] = False
                
                if loaded_count > 0:
                    logger.info(f"ğŸ¯ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæ•°æ®åº“çš„cookies")
                    return results
                else:
                    logger.warning("âš ï¸ cookies.jsonä¸­æ²¡æœ‰æ¿€æ´»çš„æ•°æ®åº“cookies")
                    
            except Exception as e:
                logger.error(f"âŒ è¯»å–cookies.jsonå¤±è´¥ï¼š{e}")
                results['json_config'] = False
        
        # 2. å…¼å®¹æ€§æ”¯æŒï¼šæ£€æŸ¥nature_cookies.txtæ–‡ä»¶ - ä¼˜å…ˆä»ç”¨æˆ·é…ç½®ç›®å½•åŠ è½½
        txt_cookie_paths = [
            user_config_dir / "nature_cookies.txt",  # æ¨èä½ç½®
            project_root / "nature_cookies.txt"      # å‘åå…¼å®¹
        ]
        
        txt_cookie_file = None
        for path in txt_cookie_paths:
            if path.exists():
                txt_cookie_file = path
                break
                
        if txt_cookie_file:
            logger.info(f"ğŸ“ æ‰¾åˆ°å…¼å®¹æ€§TXTæ–‡ä»¶: {txt_cookie_file}")
            try:
                with open(txt_cookie_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                # è¿‡æ»¤æ³¨é‡Šå’Œç©ºè¡Œ
                lines = [line.strip() for line in content.split('\n') 
                        if line.strip() and not line.strip().startswith('#')]
                
                if lines:
                    cookies_str = ' '.join(lines).strip()
                    cookie_count = len(cookies_str.split(';'))
                    
                    # è®¾ç½®åˆ°Natureæ•°æ®åº“
                    success = self.set_database_cookies('nature', cookies_str)
                    if success:
                        logger.info(f"âœ… ä»TXTæ–‡ä»¶åŠ è½½Nature cookiesæˆåŠŸï¼š{cookie_count}ä¸ªcookies")
                        logger.warning("ğŸ’¡ å»ºè®®è¿ç§»åˆ°cookies.jsonæ ¼å¼ä»¥æ”¯æŒå¤šæ•°æ®åº“")
                    else:
                        logger.error("âŒ è®¾ç½®Nature TXT cookieså¤±è´¥")
                    results['nature_txt'] = success
                else:
                    logger.warning("âš ï¸ nature_cookies.txtæ–‡ä»¶ä¸ºç©ºæˆ–åªåŒ…å«æ³¨é‡Š")
                    results['nature_txt'] = False
                    
            except Exception as e:
                logger.error(f"âŒ è¯»å–nature_cookies.txtå¤±è´¥ï¼š{e}")
                results['nature_txt'] = False
        
        # 3. æŸ¥æ‰¾æ‰€æœ‰shared_cookies_*.jsonæ–‡ä»¶ï¼ˆä¹¦ç­¾åŒæ­¥æ ¼å¼ï¼‰
        cookie_files = list(project_root.glob("shared_cookies_*.json"))
        if not cookie_files:
            if not results:
                logger.info("ğŸ“„ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•cookieæ–‡ä»¶")
            return results
        
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(cookie_files)} ä¸ªcookieæ–‡ä»¶")
        
        for file_path in cookie_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    cookie_data = json.load(f)
                
                site_name = cookie_data.get('siteName', 'Unknown')
                cookies = cookie_data.get('cookies', '')
                timestamp = cookie_data.get('timestamp', '')
                cookies_count = cookie_data.get('cookies_count', 0)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                last_updated = cookie_data.get('last_updated', 0)
                if time.time() - last_updated > 24 * 3600:
                    logger.warning(f"âš ï¸ {site_name} cookieså·²è¿‡æœŸï¼ˆ{timestamp}ï¼‰")
                    results[site_name] = False
                    continue
                
                # æ ¹æ®ç«™ç‚¹åæ˜ å°„åˆ°æ•°æ®åº“å
                database_name = self._map_site_to_database(site_name)
                if database_name:
                    success = self.set_database_cookies(database_name, cookies)
                    if success:
                        logger.info(f"âœ… ä»æ–‡ä»¶åŠ è½½ {site_name} cookiesæˆåŠŸï¼š{cookies_count}ä¸ªcookiesï¼ˆ{timestamp}ï¼‰")
                    else:
                        logger.error(f"âŒ è®¾ç½® {site_name} cookieså¤±è´¥")
                    results[site_name] = success
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥ç«™ç‚¹ï¼š{site_name}")
                    results[site_name] = False
                    
            except Exception as e:
                logger.error(f"âŒ è¯»å–cookieæ–‡ä»¶ {file_path} å¤±è´¥ï¼š{e}")
                results[file_path.stem] = False
        
        return results
    
    def _map_site_to_database(self, site_name: str) -> str:
        """å°†ç«™ç‚¹åæ˜ å°„åˆ°æ•°æ®åº“å"""
        mapping = {
            'www.nature.com': 'nature',
            'nature.com': 'nature', 
            'www.science.org': 'science',
            'science.org': 'science',
            'ieeexplore.ieee.org': 'ieee',
            'link.springer.com': 'springer'
        }
        return mapping.get(site_name.lower(), '')
    
    def update_database_cookies(self, database_key: str, cookies_str: str) -> bool:
        """
        æ›´æ–°æŒ‡å®šæ•°æ®åº“çš„cookiesåˆ°cookies.jsonæ–‡ä»¶
        
        Args:
            database_key: æ•°æ®åº“æ ‡è¯† (å¦‚ 'nature', 'science')
            cookies_str: Cookieå­—ç¬¦ä¸²
            
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        import json
        from pathlib import Path
        from datetime import datetime, timezone
        
        # ä¼˜å…ˆçº§ï¼šç”¨æˆ·é…ç½®ç›®å½• > é¡¹ç›®æ ¹ç›®å½•
        user_config_dir = Path.home() / '.zotlink'
        project_root = Path(__file__).parent.parent
        
        json_config_paths = [
            user_config_dir / "cookies.json",  # æ¨èä½ç½®
            project_root / "cookies.json"      # å‘åå…¼å®¹
        ]
        
        json_config_file = None
        for path in json_config_paths:
            if path.exists():
                json_config_file = path
                break
        
        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨èä½ç½®åˆ›å»ºæ–°æ–‡ä»¶
        if not json_config_file:
            user_config_dir.mkdir(exist_ok=True)
            json_config_file = user_config_dir / "cookies.json"
        
        try:
            # è¯»å–ç°æœ‰é…ç½®
            if json_config_file.exists():
                with open(json_config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
            else:
                logger.error("âŒ cookies.jsonæ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            if database_key not in config.get('databases', {}):
                logger.error(f"âŒ æœªçŸ¥æ•°æ®åº“: {database_key}")
                return False
            
            # æ›´æ–°Cookieä¿¡æ¯
            current_time = datetime.now(timezone.utc).isoformat()
            cookie_count = len(cookies_str.split(';')) if cookies_str else 0
            
            config['last_updated'] = current_time
            config['databases'][database_key].update({
                'cookies': cookies_str,
                'last_updated': current_time,
                'cookie_count': cookie_count,
                'status': 'active' if cookies_str else 'inactive'
            })
            
            # ä¿å­˜æ›´æ–°çš„é…ç½®
            with open(json_config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            # åŒæ—¶è®¾ç½®åˆ°ExtractorManager
            success = self.set_database_cookies(database_key, cookies_str)
            
            db_name = config['databases'][database_key].get('name', database_key)
            if success:
                logger.info(f"âœ… æ›´æ–° {db_name} cookiesæˆåŠŸï¼š{cookie_count}ä¸ªcookies")
            else:
                logger.error(f"âŒ è®¾ç½® {db_name} cookieså¤±è´¥")
                
            return success
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ•°æ®åº“cookieså¤±è´¥ï¼š{e}")
            return False
    
    def get_databases_status(self) -> Dict[str, Dict]:
        """
        è·å–æ‰€æœ‰æ•°æ®åº“çš„çŠ¶æ€ä¿¡æ¯
        
        Returns:
            Dict[str, Dict]: æ•°æ®åº“çŠ¶æ€ä¿¡æ¯
        """
        import json
        from pathlib import Path
        
        # ä¼˜å…ˆçº§ï¼šç”¨æˆ·é…ç½®ç›®å½• > é¡¹ç›®æ ¹ç›®å½•
        user_config_dir = Path.home() / '.zotlink'
        project_root = Path(__file__).parent.parent
        
        json_config_paths = [
            user_config_dir / "cookies.json",  # æ¨èä½ç½®
            project_root / "cookies.json"      # å‘åå…¼å®¹
        ]
        
        json_config_file = None
        for path in json_config_paths:
            if path.exists():
                json_config_file = path
                break
        
        # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ¨èä½ç½®åˆ›å»ºæ–°æ–‡ä»¶
        if not json_config_file:
            user_config_dir.mkdir(exist_ok=True)
            json_config_file = user_config_dir / "cookies.json"
        
        try:
            if not json_config_file.exists():
                return {}
                
            with open(json_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            databases = config.get('databases', {})
            status_info = {}
            
            for db_key, db_config in databases.items():
                status_info[db_key] = {
                    'name': db_config.get('name', db_key),
                    'status': db_config.get('status', 'inactive'),
                    'cookie_count': db_config.get('cookie_count', 0),
                    'last_updated': db_config.get('last_updated'),
                    'domains': db_config.get('domains', []),
                    'description': db_config.get('description', ''),
                    'login_url': db_config.get('login_url', ''),
                    'test_url': db_config.get('test_url', '')
                }
            
            return status_info
            
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ•°æ®åº“çŠ¶æ€å¤±è´¥ï¼š{e}")
            return {}


def test_zotero_connection():
    """æµ‹è¯•Zoteroè¿æ¥"""
    print("ğŸ§ª æµ‹è¯•Zoteroè¿æ¥...")
    
    connector = ZoteroConnector()
    
    if connector.is_running():
        version = connector.get_version()
        if version:
            print(f"âœ… Zoteroè¿æ¥æˆåŠŸï¼Œç‰ˆæœ¬: {version}")
            
            # æµ‹è¯•é›†åˆè·å–
            collections = connector.get_zotero_collections()
            print(f"ğŸ“š æ‰¾åˆ° {len(collections.get('collections', []))} ä¸ªé›†åˆ")
        else:
            print("âš ï¸ Zoteroè¿æ¥æˆåŠŸï¼Œä½†æ— æ³•è·å–ç‰ˆæœ¬ä¿¡æ¯")
    else:
        print("âŒ Zoteroæœªè¿è¡Œæˆ–è¿æ¥å¤±è´¥")


# åœ¨ZoteroConnectorç±»ä¸­æ·»åŠ æ–°æ–¹æ³•ï¼ˆè¿™é‡Œæ˜¯æ–‡ä»¶æœ«å°¾ï¼Œéœ€è¦æ‰¾åˆ°ç±»çš„æ­£ç¡®ä½ç½®ï¼‰
# æ³¨æ„ï¼šè¿™äº›æ–¹æ³•åº”è¯¥æ·»åŠ åˆ°ZoteroConnectorç±»å†…éƒ¨

if __name__ == "__main__":
    test_zotero_connection() 