#!/usr/bin/env python3
"""
ğŸ”— CVF (Computer Vision Foundation) è®ºæ–‡æå–å™¨

å¤„ç† openaccess.thecvf.com ç½‘ç«™çš„è®ºæ–‡å…ƒæ•°æ®æå–
"""

import re
import requests
import logging
from typing import Dict, List
from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class CVFExtractor(BaseExtractor):
    """CVFè®ºæ–‡æå–å™¨"""
    
    def __init__(self, session: requests.Session = None):
        """åˆå§‹åŒ–CVFæå–å™¨"""
        super().__init__(session)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
    
    def can_handle(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†æ­¤URL"""
        cvf_domains = [
            'openaccess.thecvf.com',
            'thecvf.com'
        ]
        return any(domain in url.lower() for domain in cvf_domains)
    
    def requires_authentication(self) -> bool:
        """CVFæ˜¯å¼€æ”¾è·å–ï¼Œä¸éœ€è¦è®¤è¯"""
        return False
    
    def get_database_name(self) -> str:
        """è·å–æ•°æ®åº“åç§°"""
        return "CVF"
    
    def extract_metadata(self, url: str) -> Dict:
        """ä»CVF URLæå–è®ºæ–‡å…ƒæ•°æ®"""
        try:
            # ä»URLæå–åŸºæœ¬ä¿¡æ¯
            metadata = self._extract_from_url(url)
            logger.info(f"ğŸ” ä»URLæå–çš„å…ƒæ•°æ®: title='{metadata.get('title', 'None')}'")
            
            # å°è¯•è·å–å¯¹åº”çš„HTMLé¡µé¢æ¥æå–æ›´å¤šä¿¡æ¯
            html_url = self._get_html_url_from_pdf(url)
            if html_url:
                html_metadata = self._extract_from_html_page(html_url)
                logger.info(f"ğŸŒ ä»HTMLæå–çš„å…ƒæ•°æ®: title='{html_metadata.get('title', 'None')}'")
                
                # ä¼˜å…ˆä½¿ç”¨HTMLé¡µé¢çš„ä¿¡æ¯ï¼Œä½†ä¿æŠ¤é‡è¦å­—æ®µ
                for key, value in html_metadata.items():
                    if value:  # åªæœ‰å½“HTMLé¡µé¢çš„å€¼ä¸ä¸ºç©ºä¸”éNoneæ—¶æ‰è¦†ç›–
                        # ğŸ¯ ä¿®å¤ï¼šå¯¹äºtitleå­—æ®µï¼Œå¦‚æœHTMLæå–å¤±è´¥ï¼Œä¸è¦è¦†ç›–URLæå–çš„æ ‡é¢˜
                        if key == 'title' and metadata.get('title') and not value.strip():
                            logger.warning(f"âš ï¸ HTMLæ ‡é¢˜ä¸ºç©ºï¼Œä¿ç•™URLæå–çš„æ ‡é¢˜: {metadata.get('title')}")
                            continue
                        metadata[key] = value
            
            # ğŸ¯ ç¡®ä¿æ ‡é¢˜ä¸ä¸ºç©º
            if not metadata.get('title') or not metadata.get('title').strip():
                logger.warning("âš ï¸ æ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»URLé‡æ–°æå–")
                url_metadata = self._extract_from_url(url)
                if url_metadata.get('title'):
                    metadata['title'] = url_metadata['title']
                    logger.info(f"âœ… é‡æ–°è®¾ç½®æ ‡é¢˜: {metadata['title']}")
            
            # ç”ŸæˆTLDR
            if metadata.get('abstract'):
                metadata['tldr'] = self._generate_tldr(metadata['abstract'])
            
            # è®¾ç½®CVFç‰¹æœ‰å­—æ®µ
            metadata.update({
                'extractor': self.get_database_name(),
                'itemType': 'conferencePaper',
                'pdf_url': url if url.endswith('.pdf') else None
            })
            
            logger.info(f"âœ… CVFæœ€ç»ˆå…ƒæ•°æ®: title='{metadata.get('title', 'None')}'")
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ CVFå…ƒæ•°æ®æå–å¤±è´¥: {e}")
            return {
                'error': f'CVFå…ƒæ•°æ®æå–å¤±è´¥: {str(e)}',
                'url': url
            }
    
    def _extract_from_url(self, url: str) -> Dict:
        """ä»URLè·¯å¾„æå–åŸºæœ¬ä¿¡æ¯"""
        metadata = {}
        
        # ä»URLè·¯å¾„æå–ä¼šè®®ä¿¡æ¯
        # ä¾‹å¦‚: /content/ICCV2023/papers/...
        path_match = re.search(r'/content/([A-Z]+)(\d{4})/', url)
        if path_match:
            conference_abbr = path_match.group(1)  # ICCV, CVPR, WACV
            year = path_match.group(2)
            
            # æ˜ å°„ä¼šè®®ç¼©å†™åˆ°å…¨å
            conference_names = {
                'ICCV': 'International Conference on Computer Vision',
                'CVPR': 'Conference on Computer Vision and Pattern Recognition', 
                'WACV': 'Winter Conference on Applications of Computer Vision'
            }
            
            full_conference = conference_names.get(conference_abbr, conference_abbr)
            conference_full_name = f"{year} IEEE/CVF {full_conference} ({conference_abbr})"
            
            metadata.update({
                'conference': conference_abbr,
                'conferenceName': conference_full_name,
                'proceedingsTitle': conference_full_name,
                'publicationTitle': conference_full_name,
                'date': f"{year}-10-01",  # ICCVé€šå¸¸åœ¨10æœˆ
                'year': year
            })
        
        # æ”¹è¿›çš„æ–‡ä»¶åè§£æ
        # ä¾‹å¦‚: Fang_Visible-Infrared_Person_Re-Identification_via_Semantic_Alignment_and_Affinity_Inference_ICCV_2023_paper.pdf
        filename_match = re.search(r'/([^/_]+)_(.+)_([A-Z]+)_(\d{4})_paper\.pdf$', url)
        if filename_match:
            first_author_lastname = filename_match.group(1)
            title_part = filename_match.group(2)
            conf_abbr = filename_match.group(3)
            
            # é‡æ„æ ‡é¢˜ï¼šå°†ä¸‹åˆ’çº¿è½¬æ¢ä¸ºç©ºæ ¼ï¼Œè¿å­—ç¬¦ä¿ç•™
            title = title_part.replace('_', ' ')
            
            metadata.update({
                'title': title,
                'first_author_lastname': first_author_lastname
            })
        
        # å¦‚æœæ— æ³•ä»æ–‡ä»¶åå®Œæ•´è§£æï¼Œå°è¯•æ›´ç®€å•çš„æ¨¡å¼
        elif not metadata.get('title'):
            simple_match = re.search(r'/([^/]+)_paper\.pdf$', url)
            if simple_match:
                filename = simple_match.group(1)
                # å°è¯•åˆ†å‰²å‡ºä½œè€…å’Œæ ‡é¢˜
                parts = filename.split('_')
                if len(parts) >= 3:
                    first_author_lastname = parts[0]
                    title = ' '.join(parts[1:]).replace('-', ' ')
                    
                    metadata.update({
                        'title': title,
                        'first_author_lastname': first_author_lastname
                    })
        
        return metadata
    
    def _get_html_url_from_pdf(self, pdf_url: str) -> str:
        """ä»PDF URLè·å–å¯¹åº”çš„HTMLé¡µé¢URL"""
        if not pdf_url.endswith('.pdf'):
            return pdf_url
        
        # è½¬æ¢PDF URLä¸ºHTML URL
        # ä¾‹å¦‚: .../paper.pdf -> .../html/...html
        html_url = pdf_url.replace('.pdf', '.html').replace('/papers/', '/html/')
        
        # ä¹Ÿå°è¯•ç›´æ¥å»æ‰.pdfåç¼€çš„ç‰ˆæœ¬
        simple_url = pdf_url.replace('_paper.pdf', '')
        
        # ä¼˜å…ˆå°è¯•HTMLç‰ˆæœ¬
        for candidate_url in [html_url, simple_url]:
            try:
                response = self.session.head(candidate_url, timeout=5)
                if response.status_code == 200:
                    logger.info(f"âœ… æ‰¾åˆ°HTMLé¡µé¢: {candidate_url}")
                    return candidate_url
            except:
                continue
        
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°HTMLé¡µé¢ï¼Œå°†å°è¯•ç›´æ¥è§£æPDF")
        return None
    
    def _extract_from_html_page(self, html_url: str) -> Dict:
        """ä»HTMLé¡µé¢æå–è¯¦ç»†å…ƒæ•°æ®"""
        try:
            response = self.session.get(html_url, timeout=10)
            if response.status_code != 200:
                return {}
            
            html_content = response.text
            metadata = {}
            
            # æå–æ ‡é¢˜ - ä¼˜å…ˆä½¿ç”¨citationå…ƒæ•°æ®ï¼Œç„¶åä½¿ç”¨é¡µé¢å†…å®¹
            # é¦–å…ˆå°è¯•ä»citationå…ƒæ•°æ®æå–æ ‡é¢˜
            citation_title_match = re.search(r'<meta name="citation_title" content="([^"]+)"', html_content, re.IGNORECASE)
            if citation_title_match:
                title = citation_title_match.group(1).strip()
                metadata['title'] = title
                logger.info(f"âœ… ä»citationå…ƒæ•°æ®æå–åˆ°æ ‡é¢˜: {title}")
            else:
                # å¦‚æœæ²¡æœ‰citationå…ƒæ•°æ®ï¼Œå°è¯•ä»é¡µé¢å†…å®¹æå–
                title_patterns = [
                    r'<div id="papertitle">\s*([^<]+(?:\n[^<]*)*?)</div>',
                    r'<div class="ptitle">([^<]+)</div>',
                    r'<div class="papertitle">([^<]+)</div>',
                    r'<h1[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</h1>',
                    r'<h1[^>]*>([^<]+)</h1>'
                ]
                
                for pattern in title_patterns:
                    match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
                    if match:
                        title = match.group(1).strip()
                        # æ¸…ç†æ ‡é¢˜
                        title = re.sub(r'\s+', ' ', title)
                        title = re.sub(r'^\s*[-â€“]\s*', '', title)  # ç§»é™¤å¼€å¤´çš„ç ´æŠ˜å·
                        if title and len(title) > 10:  # ç¡®ä¿æ˜¯æœ‰æ„ä¹‰çš„æ ‡é¢˜
                            metadata['title'] = title
                            logger.info(f"âœ… ä»é¡µé¢å†…å®¹æå–åˆ°æ ‡é¢˜: {title}")
                            break
            
            # æå–ä½œè€… - ä¼˜å…ˆä½¿ç”¨citationå…ƒæ•°æ®ï¼Œç„¶åä½¿ç”¨é¡µé¢å†…å®¹
            authors = []
            
            # é¦–å…ˆå°è¯•ä»citationå…ƒæ•°æ®æå–ä½œè€…
            citation_authors = re.findall(r'<meta name="citation_author" content="([^"]+)"', html_content, re.IGNORECASE)
            if citation_authors:
                # citation_authorå·²ç»æ˜¯"Last, First"æ ¼å¼
                metadata['authors'] = '; '.join(citation_authors)
                logger.info(f"âœ… ä»citationå…ƒæ•°æ®æå–åˆ°ä½œè€…: {metadata['authors']}")
            else:
                # å¦‚æœæ²¡æœ‰citationå…ƒæ•°æ®ï¼Œå°è¯•ä»é¡µé¢å†…å®¹æå–
                author_patterns = [
                    r'<div id="authors">[^<]*<b><i>([^<]+)</i></b>',
                    r'<div class="pauthors">([^<]+(?:<[^>]*>[^<]*</[^>]*>[^<]*)*)</div>',
                    r'<div class="authors?">([^<]+(?:<[^>]*>[^<]*</[^>]*>[^<]*)*)</div>',
                    r'<span class="author[^"]*">([^<]+)</span>',
                    r'<div class="author[^"]*">([^<]+)</div>',
                    r'<p class="author[^"]*">([^<]+)</p>'
                ]
                
                for pattern in author_patterns:
                    match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
                    if match:
                        author_block = match.group(1).strip()
                        # æ¸…ç†HTMLæ ‡ç­¾
                        clean_authors = re.sub(r'<[^>]+>', '', author_block)
                        # åˆ†å‰²å¤šä¸ªä½œè€…ï¼ˆå¯èƒ½ç”¨é€—å·ã€åˆ†å·æˆ–å…¶ä»–åˆ†éš”ç¬¦ï¼‰
                        author_list = re.split(r'[,;]|\s+and\s+', clean_authors)
                        
                        formatted_authors = []
                        for author in author_list:
                            author = author.strip()
                            if author and len(author) > 2:
                                # å¤„ç†"First Middle Last"æ ¼å¼ï¼Œè½¬æ¢ä¸º"Last, First Middle"
                                parts = author.split()
                                if len(parts) >= 2:
                                    first_names = ' '.join(parts[:-1])
                                    last_name = parts[-1]
                                    formatted_authors.append(f"{last_name}, {first_names}")
                                else:
                                    formatted_authors.append(author)
                        
                        if formatted_authors:
                            metadata['authors'] = '; '.join(formatted_authors)
                            logger.info(f"âœ… ä»é¡µé¢å†…å®¹æå–åˆ°ä½œè€…: {metadata['authors']}")
                            break
            
            # æå–æ‘˜è¦ - ä½¿ç”¨æ­£ç¡®çš„CVFé¡µé¢ç»“æ„
            abstract_patterns = [
                r'<div id="abstract">\s*(.*?)\s*</div>',
                r'<div class="pabstract">([^<]+(?:<[^>]*>[^<]*</[^>]*>[^<]*)*)</div>',
                r'<div class="abstract">([^<]+(?:<[^>]*>[^<]*</[^>]*>[^<]*)*)</div>',
                r'<p class="abstract[^"]*">([^<]+(?:<[^>]*>[^<]*</[^>]*>[^<]*)*)</p>',
                r'<div[^>]*class="[^"]*abstract[^"]*"[^>]*>([^<]+(?:<[^>]*>[^<]*</[^>]*>[^<]*)*)</div>'
            ]
            
            for pattern in abstract_patterns:
                match = re.search(pattern, html_content, re.DOTALL | re.IGNORECASE)
                if match:
                    abstract = match.group(1).strip()
                    # æ¸…ç†HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºç™½
                    abstract = re.sub(r'<[^>]+>', '', abstract)
                    abstract = re.sub(r'\s+', ' ', abstract).strip()
                    if len(abstract) > 100:  # ç¡®ä¿æ˜¯çœŸæ­£çš„æ‘˜è¦
                        metadata['abstract'] = abstract
                        logger.info(f"âœ… æå–åˆ°æ‘˜è¦: {abstract[:100]}...")
                        break
            
            # æå–é¡µç èŒƒå›´ - ä»citationå…ƒæ•°æ®
            first_page = re.search(r'<meta name="citation_firstpage" content="([^"]+)"', html_content, re.IGNORECASE)
            last_page = re.search(r'<meta name="citation_lastpage" content="([^"]+)"', html_content, re.IGNORECASE)
            if first_page and last_page:
                pages = f"{first_page.group(1)}-{last_page.group(1)}"
                metadata['pages'] = pages
                logger.info(f"âœ… æå–åˆ°é¡µç : {pages}")
            elif first_page:
                metadata['pages'] = first_page.group(1)
                logger.info(f"âœ… æå–åˆ°èµ·å§‹é¡µ: {first_page.group(1)}")
            
            # æå–å‘å¸ƒæ—¥æœŸ - æ›´ç²¾ç¡®çš„æ—¥æœŸ
            pub_date_match = re.search(r'<meta name="citation_publication_date" content="([^"]+)"', html_content, re.IGNORECASE)
            if pub_date_match:
                year = pub_date_match.group(1).strip()
                # æ ¹æ®ä¼šè®®ç±»å‹è®¾å®šæ›´ç²¾ç¡®çš„æ—¥æœŸï¼Œè¦†ç›–URLè§£æçš„æ—¥æœŸ
                if metadata.get('conference') == 'ICCV':
                    metadata['date'] = f"{year}-10-01"  # ICCVé€šå¸¸åœ¨10æœˆ
                elif metadata.get('conference') == 'CVPR':
                    metadata['date'] = f"{year}-06-01"  # CVPRé€šå¸¸åœ¨6æœˆ
                elif metadata.get('conference') == 'WACV':
                    metadata['date'] = f"{year}-01-01"  # WACVé€šå¸¸åœ¨1æœˆ
                else:
                    metadata['date'] = f"{year}-01-01"
                logger.info(f"âœ… æ›´æ–°æ—¥æœŸ: {metadata['date']}")
            
            # è®¾ç½®å‡ºç‰ˆå•†ä¿¡æ¯
            metadata['publisher'] = 'IEEE'
            
            # è®¾ç½®è¯­è¨€
            metadata['language'] = 'en'
            
            # æå–æˆ–è®¾ç½®ä¼šè®®åœ°ç‚¹ï¼ˆå¦‚æœHTMLä¸­æœ‰çš„è¯ï¼‰
            place_patterns = [
                r'<meta name="citation_conference_place" content="([^"]+)"',
                r'<div class="conference-location">([^<]+)</div>',
                r'(\w+,\s*\w+)\s*\d{4}'  # åŒ¹é…"City, Country 2023"æ ¼å¼
            ]
            
            for pattern in place_patterns:
                place_match = re.search(pattern, html_content, re.IGNORECASE)
                if place_match:
                    place = place_match.group(1).strip()
                    if place and len(place) > 2:
                        metadata['place'] = place
                        logger.info(f"âœ… æå–åˆ°ä¼šè®®åœ°ç‚¹: {place}")
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åœ°ç‚¹ï¼Œæ ¹æ®ä¼šè®®å’Œå¹´ä»½è®¾ç½®é»˜è®¤åœ°ç‚¹
            if not metadata.get('place') and metadata.get('conference') and metadata.get('year'):
                conf = metadata['conference']
                year = metadata['year']
                # ä¸€äº›å¸¸è§çš„CVFä¼šè®®åœ°ç‚¹ï¼ˆè¿™äº›æ˜¯å†å²æ•°æ®ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥æ›´æ–°ï¼‰
                default_places = {
                    'ICCV': {
                        '2023': 'Paris, France',
                        '2021': 'Virtual',
                        '2019': 'Seoul, Korea'
                    },
                    'CVPR': {
                        '2023': 'Vancouver, Canada', 
                        '2022': 'New Orleans, USA',
                        '2021': 'Virtual'
                    },
                    'WACV': {
                        '2023': 'Waikoloa, Hawaii',
                        '2022': 'Waikoloa, Hawaii'
                    }
                }
                if conf in default_places and year in default_places[conf]:
                    metadata['place'] = default_places[conf][year]
                    logger.info(f"âœ… è®¾ç½®é»˜è®¤ä¼šè®®åœ°ç‚¹: {metadata['place']}")
            
            # æŸ¥æ‰¾DOIï¼ˆè™½ç„¶CVFçš„å¼€æ”¾è·å–è®ºæ–‡é€šå¸¸æ²¡æœ‰å•ç‹¬çš„DOIï¼‰
            doi_patterns = [
                r'<meta name="citation_doi" content="([^"]+)"',
                r'<meta name="DC\.identifier" content="doi:([^"]+)"',
                r'doi:\s*([0-9]{2}\.[0-9]{4}/[^\s<>]+)',
                r'DOI:\s*([0-9]{2}\.[0-9]{4}/[^\s<>]+)'
            ]
            
            for pattern in doi_patterns:
                doi_match = re.search(pattern, html_content, re.IGNORECASE)
                if doi_match:
                    doi = doi_match.group(1).strip()
                    if doi.startswith('10.'):
                        metadata['DOI'] = doi
                        metadata['url'] = f'https://doi.org/{doi}'  # DOI URLä¼˜å…ˆäºPDF URL
                        logger.info(f"âœ… æå–åˆ°DOI: {doi}")
                        break
            
            # æŸ¥æ‰¾ISBNï¼ˆä¼šè®®è®ºæ–‡é›†é€šå¸¸æœ‰ISBNï¼‰
            isbn_patterns = [
                r'<meta name="citation_isbn" content="([^"]+)"',
                r'ISBN[:\s]*([0-9-]{10,17})',
                r'ISBN[:\s]*([0-9]{10,13})'
            ]
            
            for pattern in isbn_patterns:
                isbn_match = re.search(pattern, html_content, re.IGNORECASE)
                if isbn_match:
                    isbn = isbn_match.group(1).strip()
                    # ç®€å•éªŒè¯ISBNæ ¼å¼
                    isbn_clean = re.sub(r'[^0-9X]', '', isbn.upper())
                    if len(isbn_clean) in [10, 13]:
                        metadata['ISBN'] = isbn
                        logger.info(f"âœ… æå–åˆ°ISBN: {isbn}")
                        break
            
            # æå–æ›´ç²¾ç¡®çš„ä¼šè®®ä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨citationå…ƒæ•°æ®
            citation_conference_match = re.search(r'<meta name="citation_conference_title" content="([^"]+)"', html_content, re.IGNORECASE)
            if citation_conference_match:
                conference_title = citation_conference_match.group(1).strip()
                metadata['proceedingsTitle'] = conference_title
                metadata['publicationTitle'] = conference_title
                logger.info(f"âœ… ä»citationå…ƒæ•°æ®æå–åˆ°ä¼šè®®æ ‡é¢˜: {conference_title}")
            else:
                # å¤‡é€‰æ–¹æ¡ˆï¼šä»é¡µé¢å†…å®¹æå–
                conference_patterns = [
                    r'<div class="pconf">([^<]+)</div>',
                    r'Proceedings of the ([^<\n]+)',
                    r'(\d{4} IEEE/CVF [^<\n]+Conference[^<\n]*)'
                ]
                
                for pattern in conference_patterns:
                    match = re.search(pattern, html_content, re.IGNORECASE)
                    if match:
                        conference_info = match.group(1).strip()
                        metadata['proceedingsTitle'] = conference_info
                        metadata['publicationTitle'] = conference_info
                        logger.info(f"âœ… ä»é¡µé¢å†…å®¹æå–åˆ°ä¼šè®®æ ‡é¢˜: {conference_info}")
                        break
            
            # è®¾ç½®å®Œæ•´çš„URLï¼ˆå¦‚æœæ²¡æœ‰DOIçš„è¯ï¼‰
            if not metadata.get('DOI'):
                metadata['url'] = html_url
            
            return metadata
            
        except Exception as e:
            logger.warning(f"âš ï¸ HTMLé¡µé¢å…ƒæ•°æ®æå–å¤±è´¥: {e}")
            return {}
    
    def _generate_tldr(self, abstract: str) -> str:
        """ä»æ‘˜è¦ç”ŸæˆTLDR"""
        if not abstract or len(abstract) < 50:
            return ""
        
        # æå–ç¬¬ä¸€å¥è¯æˆ–å‰100ä¸ªå­—ç¬¦ä½œä¸ºTLDR
        sentences = re.split(r'[.!?]\s+', abstract)
        if sentences:
            first_sentence = sentences[0].strip()
            if len(first_sentence) > 20:
                # å¦‚æœç¬¬ä¸€å¥è¯åˆç†é•¿åº¦ï¼Œä½¿ç”¨ç¬¬ä¸€å¥è¯
                tldr = first_sentence
                if not tldr.endswith('.'):
                    tldr += '.'
                return tldr
        
        # å¦åˆ™ä½¿ç”¨å‰150ä¸ªå­—ç¬¦
        tldr = abstract[:150].strip()
        if len(tldr) == 150:
            # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
            last_space = tldr.rfind(' ')
            if last_space > 100:
                tldr = tldr[:last_space]
            tldr += '...'
        
        return tldr
    
    def test_access(self, test_url: str = None) -> bool:
        """æµ‹è¯•CVFç½‘ç«™è®¿é—®"""
        if not test_url:
            test_url = "https://openaccess.thecvf.com/"
        
        try:
            response = self.session.get(test_url, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def get_supported_item_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¡ç›®ç±»å‹"""
        return ['conferencePaper'] 