#!/usr/bin/env python3
"""
åŸºäºZotero Connectoræ¶æ„çš„å¢å¼ºç‰ˆé€šç”¨æå–å™¨
è§£å†³é‡å®šå‘é—®é¢˜å’Œæ”¹è¿›PDFæ£€æµ‹é€»è¾‘
"""

import requests
import re
import time
import logging
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup # Added for _process_successful_response

from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class EnhancedGenericExtractor(BaseExtractor):
    """
    åŸºäºZotero Connectoré‡æ–°è®¾è®¡çš„å¢å¼ºç‰ˆé€šç”¨æå–å™¨
    """
    
    # ä¸»è¦é™„ä»¶ç±»å‹ (ç±»ä¼¼Zoteroçš„PRIMARY_ATTACHMENT_TYPES)
    PRIMARY_ATTACHMENT_TYPES = {
        'application/pdf',
        'application/epub+zip'
    }
    
    # æ”¯æŒçš„åŸŸåé…ç½®
    DOMAIN_CONFIGS = {
        # arXivç³»åˆ—
        'arxiv.org': {
            'type': 'preprint',
            'source': 'arXiv',
            'priority': 1,
            'pdf_patterns': [
                r'href="([^"]*\.pdf)"',
                r'href="(/pdf/[^"]+)"',
                r'<meta[^>]*name="citation_pdf_url"[^>]*content="([^"]+)"',
            ]
        },
        # medRxivç³»åˆ— 
        'medrxiv.org': {
            'type': 'preprint', 
            'source': 'medRxiv',
            'priority': 1,
            'pdf_patterns': [
                r'<meta[^>]*name="citation_pdf_url"[^>]*content="([^"]+)"',
                r'href="([^"]*\.full\.pdf[^"]*)"',
            ]
        },
        'biorxiv.org': {
            'type': 'preprint',
            'source': 'bioRxiv', 
            'priority': 1,
            'pdf_patterns': [
                r'<meta[^>]*name="citation_pdf_url"[^>]*content="([^"]+)"',
                r'href="([^"]*\.full\.pdf[^"]*)"',
            ]
        },
        # ChemRxiv
        'chemrxiv.org': {
            'type': 'preprint',
            'source': 'ChemRxiv',
            'priority': 1,
            'pdf_patterns': [
                r'href="([^"]*ndownloader[^"]*)"',
                r'href="([^"]*download[^"]*)"[^>]*>(?:[^<]*(?:PDF|pdf)[^<]*)</a>',
            ]
        },
        # OSFç³»åˆ—
        'osf.io/preprints/psyarxiv': {
            'type': 'preprint',
            'source': 'PsyArXiv', 
            'priority': 1,
            'pdf_patterns': []  # å°†ä½¿ç”¨OSFçš„ç‰¹æ®Šå¤„ç†
        },
        'osf.io/preprints/socarxiv': {
            'type': 'preprint',
            'source': 'SocArXiv',
            'priority': 1, 
            'pdf_patterns': []  # å°†ä½¿ç”¨OSFçš„ç‰¹æ®Šå¤„ç†
        }
    }
    
    def __init__(self, session=None):
        """
        åˆå§‹åŒ–å¢å¼ºé€šç”¨æå–å™¨
        
        Args:
            session: requestsä¼šè¯å¯¹è±¡
        """
        super().__init__(session=session)
        self.session = requests.Session()
        
        # åçˆ¬è™«ç½‘ç«™åˆ—è¡¨ - éœ€è¦å¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼
        self.anti_crawler_sites = {
            'biorxiv.org', 'medrxiv.org', 'chemrxiv.org', 
            'psyarxiv.com', 'socarxiv.org', 'osf.io',
            'researchsquare.com', 'authorea.com'
        }
        
        # é…ç½®ä¼šè¯headers
        self._setup_session()

    def requires_authentication(self) -> bool:
        """æ£€æŸ¥æ­¤æå–å™¨æ˜¯å¦éœ€è¦è®¤è¯"""
        return False
    
    def get_database_name(self) -> str:
        """è·å–æ•°æ®åº“åç§°"""
        return "Enhanced Generic Extractor"
    
    def can_handle(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†è¯¥URL"""
        # å¢å¼ºæå–å™¨å¯ä»¥å¤„ç†ä»»ä½•URLï¼Œä½†ä¼šæ ¹æ®åŸŸåä¼˜åŒ–ç­–ç•¥
        return True
        url_lower = url.lower()
        for domain_pattern in self.DOMAIN_CONFIGS.keys():
            if domain_pattern in url_lower:
                return True
        return False
    
    def extract_metadata(self, url: str) -> Dict:
        """
        æå–è®ºæ–‡å…ƒæ•°æ®çš„ä¸»å…¥å£ï¼ˆè¦†ç›–åŸºç±»æ–¹æ³•ï¼‰
        
        Returns:
            åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœé‡åˆ°403é”™è¯¯ä¼šè‡ªåŠ¨ä½¿ç”¨åçˆ¬è™«å¤„ç†ç­–ç•¥
        """
        logger.info(f"ğŸ” å¼€å§‹æå–è®ºæ–‡ä¿¡æ¯: {url}")
        
        try:
            # é¦–å…ˆå°è¯•å¸¸è§„HTTPè¯·æ±‚
            response = self.session.get(url, timeout=30, allow_redirects=True)
            
            # å¦‚æœé‡åˆ°403é”™è¯¯ï¼Œä½¿ç”¨åçˆ¬è™«å¤„ç†ç­–ç•¥
            if response.status_code == 403:
                logger.warning(f"ğŸš« æ£€æµ‹åˆ°403å“åº”ï¼Œå¯ç”¨åçˆ¬è™«å¤„ç†ç­–ç•¥")
                return self._handle_403_response(url)
            
            elif response.status_code != 200:
                logger.warning(f"âš ï¸ HTTPçŠ¶æ€ç å¼‚å¸¸: {response.status_code}")
                # å¯¹äºå…¶ä»–é”™è¯¯çŠ¶æ€ç ï¼Œä¹Ÿå°è¯•åçˆ¬è™«å¤„ç†ç­–ç•¥
                if self._is_anti_crawler_site(url):
                    return self._handle_403_response(url)
                    
                return {
                    'error': f'HTTPé”™è¯¯: {response.status_code}',
                    'status_code': response.status_code
                }
            
            # æ­£å¸¸å¤„ç†HTTP 200å“åº”
            return self._process_successful_response(response, url)
            
        except requests.exceptions.Timeout:
            logger.error(f"â° è¯·æ±‚è¶…æ—¶: {url}")
            # è¶…æ—¶ä¹Ÿå¯èƒ½æ˜¯åçˆ¬è™«æœºåˆ¶ï¼Œå°è¯•æµè§ˆå™¨æ¨¡å¼
            if self._is_anti_crawler_site(url):
                return self._handle_403_response(url)
            return {'error': 'è¯·æ±‚è¶…æ—¶'}
            
        except requests.exceptions.ConnectionError as e:
            logger.error(f"ğŸŒ è¿æ¥é”™è¯¯: {e}")
            if self._is_anti_crawler_site(url):
                return self._handle_403_response(url)
            return {'error': f'è¿æ¥é”™è¯¯: {str(e)}'}
            
        except Exception as e:
            logger.error(f"âŒ æå–å¼‚å¸¸: {e}")
            return {'error': f'æå–å¼‚å¸¸: {str(e)}'}

    def _process_successful_response(self, response: requests.Response, url: str) -> Dict:
        """å¤„ç†æˆåŠŸçš„HTTPå“åº”"""
        html_content = response.text
        final_url = response.url
        
        logger.info(f"âœ… æˆåŠŸè·å–é¡µé¢å†…å®¹ (é•¿åº¦: {len(html_content)})")
        
        # è§£æHTMLå¹¶æå–ä¿¡æ¯
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ä»åŸŸåé…ç½®ä¸­è·å–æå–ç­–ç•¥
        domain_info = self._identify_domain(final_url)
        
        # åŸºç¡€å…ƒæ•°æ®æå–
        metadata = self._extract_comprehensive_metadata(html_content, final_url)
        
        # URLç‰¹å®šçš„å¢å¼ºå¤„ç†
        metadata = self._enhance_url_specific_metadata(metadata, final_url)
        
        # é¢„å°æœ¬ç‰¹æœ‰å­—æ®µå¢å¼º
        if metadata.get('itemType') == 'preprint':
            metadata = self._enhance_preprint_metadata(metadata)
        
        # PDFé™„ä»¶æ£€æµ‹
        page_data = {'content': html_content, 'final_url': final_url}
        attachments = self._detect_pdf_attachments(page_data, domain_info)
        
        # é€‰æ‹©ä¸»è¦PDF
        primary_pdf = self._select_primary_pdf(attachments)
        if primary_pdf:
            metadata['pdf_url'] = primary_pdf['url']
            metadata['pdf_source'] = primary_pdf['type']
        
        # ç«™ç‚¹ç‰¹å®šçš„åå¤„ç†
        metadata = self._post_process_by_site(metadata, domain_info)
        
        # éªŒè¯å¿…è¦å­—æ®µ
        if not metadata.get('title'):
            metadata['title'] = f"Paper from {self._extract_domain(final_url)}"
        
        logger.info(f"ğŸ“ æå–å®Œæˆ - æ ‡é¢˜: {metadata.get('title', 'N/A')[:50]}...")
        return metadata
    
    def _fetch_with_redirect_tracking(self, url: str) -> Optional[Dict]:
        """
        è·å–é¡µé¢å†…å®¹å¹¶è·Ÿè¸ªé‡å®šå‘é“¾
        ç±»ä¼¼äºZotero Connectorçš„é‡å®šå‘å¤„ç†
        """
        redirect_chain = []
        current_url = url
        
        try:
            for i in range(10):  # æœ€å¤šè·Ÿè¸ª10æ¬¡é‡å®šå‘
                logger.debug(f"ğŸ”— è¯·æ±‚: {current_url}")
                
                response = self.session.get(current_url, timeout=15, allow_redirects=False)
                
                redirect_info = {
                    'url': current_url,
                    'status': response.status_code,
                    'headers': dict(response.headers)
                }
                redirect_chain.append(redirect_info)
                
                if response.status_code in (301, 302, 303, 307, 308):
                    # å¤„ç†é‡å®šå‘
                    location = response.headers.get('Location')
                    if not location:
                        break
                    
                    # å¤„ç†ç›¸å¯¹URL
                    if not location.startswith('http'):
                        location = urljoin(current_url, location)
                    
                    logger.info(f"â†³ é‡å®šå‘: {current_url} â†’ {location}")
                    current_url = location
                    time.sleep(0.3)  # é¿å…è¿‡å¿«è¯·æ±‚
                    
                elif response.status_code == 200:
                    # æˆåŠŸè·å–å†…å®¹
                    return {
                        'content': response.text,
                        'final_url': current_url,
                        'redirect_chain': redirect_chain,
                        'final_headers': dict(response.headers)
                    }
                else:
                    # å…¶ä»–é”™è¯¯
                    logger.warning(f"âš ï¸ HTTPé”™è¯¯: {response.status_code}")
                    break
            
            logger.error(f"âŒ é‡å®šå‘æ¬¡æ•°è¿‡å¤šæˆ–å…¶ä»–é”™è¯¯")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def _identify_domain(self, url: str) -> Dict:
        """è¯†åˆ«åŸŸåç±»å‹"""
        url_lower = url.lower()
        
        for domain_pattern, config in self.DOMAIN_CONFIGS.items():
            if domain_pattern in url_lower:
                return {
                    'type': config['type'],
                    'source': config['source'],
                    'priority': config['priority'],
                    'patterns': config['pdf_patterns']
                }
        
        # é»˜è®¤é…ç½®
        return {
            'type': 'webpage',
            'source': 'Generic',
            'priority': 9,
            'patterns': []
        }
    
    def _extract_comprehensive_metadata(self, html_content: str, url: str) -> Dict:
        """æå–å…¨é¢çš„å…ƒæ•°æ®"""
        metadata = {'url': url}
        
        # Citationæ ‡ç­¾ - æœ€é«˜ä¼˜å…ˆçº§
        citation_fields = {
            'citation_title': 'title',
            'citation_author': 'authors',
            'citation_journal_title': 'publicationTitle',
            'citation_conference_title': 'publicationTitle',
            'citation_publisher': 'publisher',
            'citation_publication_date': 'date',
            'citation_online_date': 'date',
            'citation_doi': 'DOI',
            'citation_pmid': 'pmid',
            'citation_pmcid': 'pmcid',
            'citation_pdf_url': 'pdf_url',
            'citation_abstract': 'abstract',
            'citation_keywords': 'tags'
        }
        
        # æ”¶é›†æ‰€æœ‰ä½œè€…
        all_authors = []
        
        for citation_name, field_name in citation_fields.items():
            pattern = rf'<meta[^>]*name="{citation_name}"[^>]*content="([^"]+)"'
            if citation_name == 'citation_author':
                # æ”¶é›†æ‰€æœ‰ä½œè€…
                authors = re.findall(pattern, html_content, re.IGNORECASE)
                all_authors.extend(authors)
            else:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match and field_name not in metadata:
                    metadata[field_name] = match.group(1)
        
        # è®¾ç½®ä½œè€…ä¿¡æ¯
        if all_authors:
            metadata['authors'] = all_authors
        
        # Dublin Coreæ ‡ç­¾ - ä¸­ç­‰ä¼˜å…ˆçº§
        dc_fields = {
            'DC.title': 'title',
            'DC.creator': 'authors', 
            'DC.date': 'date',
            'DC.publisher': 'publisher',
            'DC.description': 'abstract'
        }
        
        for dc_name, field_name in dc_fields.items():
            if field_name not in metadata:
                pattern = rf'<meta[^>]*name="{dc_name}"[^>]*content="([^"]+)"'
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    metadata[field_name] = match.group(1)
        
        # Open Graphæ ‡ç­¾ - ä½ä¼˜å…ˆçº§
        og_fields = {
            'og:title': 'title',
            'og:description': 'abstract',
            'og:url': 'canonical_url'
        }
        
        for og_name, field_name in og_fields.items():
            if field_name not in metadata:
                pattern = rf'<meta[^>]*property="{og_name}"[^>]*content="([^"]+)"'
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    metadata[field_name] = match.group(1)
        
        # HTMLæ ‡ç­¾å›é€€ - æœ€ä½ä¼˜å…ˆçº§
        if 'title' not in metadata:
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
            if title_match:
                metadata['title'] = title_match.group(1).strip()
        
        # JSON-LDç»“æ„åŒ–æ•°æ®
        json_ld_data = self._extract_json_ld(html_content)
        for field_name, value in json_ld_data.items():
            if field_name not in metadata:
                metadata[field_name] = value
        
        return metadata
    
    def _extract_json_ld(self, html_content: str) -> Dict:
        """æå–JSON-LDç»“æ„åŒ–æ•°æ®"""
        metadata = {}
        
        try:
            json_ld_pattern = r'<script[^>]*type="application/ld\+json"[^>]*>(.*?)</script>'
            matches = re.findall(json_ld_pattern, html_content, re.DOTALL | re.IGNORECASE)
            
            for match in matches:
                try:
                    import json
                    data = json.loads(match.strip())
                    
                    # å¤„ç†å•ä¸ªå¯¹è±¡æˆ–æ•°ç»„
                    items = data if isinstance(data, list) else [data]
                    
                    for item in items:
                        if isinstance(item, dict):
                            # æå–ç›¸å…³å­—æ®µ
                            if '@type' in item:
                                if 'ScholarlyArticle' in str(item['@type']):
                                    if 'name' in item and 'title' not in metadata:
                                        metadata['title'] = item['name']
                                    if 'author' in item and 'authors' not in metadata:
                                        authors = item['author']
                                        if isinstance(authors, list):
                                            author_names = [a.get('name', str(a)) if isinstance(a, dict) else str(a) for a in authors]
                                            metadata['authors'] = author_names
                                        elif isinstance(authors, dict):
                                            metadata['authors'] = [authors.get('name', str(authors))]
                                    
                                    if 'datePublished' in item and 'date' not in metadata:
                                        metadata['date'] = item['datePublished']
                                    
                                    if 'description' in item and 'abstract' not in metadata:
                                        metadata['abstract'] = item['description']
                
                except (json.JSONDecodeError, KeyError, TypeError) as e:
                    logger.debug(f"JSON-LDè§£æå¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            logger.debug(f"JSON-LDæå–å¤±è´¥: {e}")
        
        return metadata
    
    def _detect_pdf_attachments(self, page_data: Dict, domain_info: Dict) -> List[Dict]:
        """
        åˆ†å±‚PDFæ£€æµ‹ - ç±»ä¼¼äºZoteroçš„é™„ä»¶æ£€æµ‹é€»è¾‘
        """
        attachments = []
        html_content = page_data['content']
        final_url = page_data['final_url']
        
        # 1. Citationæ ‡ç­¾PDF (æœ€é«˜ä¼˜å…ˆçº§)
        citation_pdf_patterns = [
            r'<meta[^>]*name="citation_pdf_url"[^>]*content="([^"]+)"',
            r'<meta[^>]*name="citation_fulltext_pdf_url"[^>]*content="([^"]+)"'
        ]
        
        for pattern in citation_pdf_patterns:
            match = re.search(pattern, html_content, re.IGNORECASE)
            if match:
                attachments.append({
                    'url': self._resolve_url(match.group(1), final_url),
                    'type': 'citation_pdf',
                    'source': 'citation_meta',
                    'priority': 1
                })
        
        # 2. ç½‘ç«™ç‰¹å®šæ¨¡å¼
        for pattern in domain_info.get('patterns', []):
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                url = self._resolve_url(match, final_url)
                if url not in [att['url'] for att in attachments]:  # é¿å…é‡å¤
                    attachments.append({
                        'url': url,
                        'type': 'site_specific', 
                        'source': domain_info['source'],
                        'priority': 2
                    })
        
        # 3. ç‰¹æ®Šç½‘ç«™å¤„ç†
        # OSFç‰¹æ®Šå¤„ç†
        if 'osf.io/preprints' in final_url:
            osf_pdf = self._extract_osf_pdf(final_url)
            if osf_pdf:
                attachments.append({
                    'url': osf_pdf,
                    'type': 'osf_download',
                    'source': 'osf',
                    'priority': 1
                })
        
        # ChemRxivç‰¹æ®Šå¤„ç†
        elif 'chemrxiv.org' in final_url:
            chemrxiv_pdf = self._extract_chemrxiv_pdf(final_url)
            if chemrxiv_pdf:
                attachments.append({
                    'url': chemrxiv_pdf,
                    'type': 'chemrxiv_api',
                    'source': 'chemrxiv',
                    'priority': 1
                })
        
        # bioRxiv/medRxivç¦»çº¿æ„é€  (å½“æ— æ³•è®¿é—®é¡µé¢æ—¶)
        elif any(domain in final_url.lower() for domain in ['biorxiv.org', 'medrxiv.org']):
            offline_pdf = self._extract_biorxiv_medrxiv_pdf(final_url)
            if offline_pdf:
                attachments.append({
                    'url': offline_pdf,
                    'type': 'offline_construction',
                    'source': domain_info['source'],
                    'priority': 3  # è¾ƒä½ä¼˜å…ˆçº§ï¼Œå› ä¸ºæ˜¯ç¦»çº¿æ„é€ 
                })
        
        # 4. é€šç”¨PDFé“¾æ¥æœç´¢ (æœ€ä½ä¼˜å…ˆçº§)
        generic_patterns = [
            r'href="([^"]*\.pdf[^"]*)"[^>]*>(?:[^<]*(?:PDF|Download|Full Text|ä¸‹è½½)[^<]*)</a>',
            r'href="([^"]*download[^"]*)"[^>]*>(?:[^<]*(?:PDF|pdf)[^<]*)</a>',
            r'href="([^"]*fulltext[^"]*\.pdf[^"]*)"',
            r'href="([^"]*manuscript[^"]*\.pdf[^"]*)"',
            r'href="([^"]*\.pdf[^"]*)"'
        ]
        
        for pattern in generic_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                url = self._resolve_url(match, final_url)
                # é¿å…é‡å¤å’Œæ˜æ˜¾çš„éä¸»æ–‡æ¡£PDF
                if (url not in [att['url'] for att in attachments] and 
                    not any(exclude in url.lower() for exclude in ['supplement', 'supporting', 'appendix'])):
                    attachments.append({
                        'url': url,
                        'type': 'generic_pdf',
                        'source': 'html_parsing',
                        'priority': 9
                    })
        
        # å»é‡å’Œæ¸…ç†
        unique_attachments = []
        seen_urls = set()
        
        for att in attachments:
            if att['url'] not in seen_urls and att['url'].startswith('http'):
                seen_urls.add(att['url'])
                unique_attachments.append(att)
        
        logger.info(f"ğŸ” æ‰¾åˆ° {len(unique_attachments)} ä¸ªPDFå€™é€‰")
        return unique_attachments
    
    def _extract_osf_pdf(self, url: str) -> Optional[str]:
        """æå–OSFçš„PDFä¸‹è½½é“¾æ¥"""
        # OSF URLæ ¼å¼: https://osf.io/preprints/psyarxiv/abc12/
        match = re.search(r'osf\.io/preprints/[^/]+/([a-z0-9]+)', url)
        if match:
            preprint_id = match.group(1)
            return f"https://osf.io/{preprint_id}/download"
        return None
    
    def _extract_chemrxiv_pdf(self, url: str) -> Optional[str]:
        """æå–ChemRxivçš„PDFä¸‹è½½é“¾æ¥"""
        # ChemRxiv URLæ ¼å¼: https://chemrxiv.org/engage/chemrxiv/article-details/abc123
        match = re.search(r'article-details/([a-f0-9]{24,})', url)
        if match:
            article_id = match.group(1)
            return f"https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/{article_id}/original/manuscript.pdf"
        return None
    
    def _extract_biorxiv_medrxiv_pdf(self, url: str) -> Optional[str]:
        """æå–bioRxiv/medRxivçš„ç¦»çº¿PDFé“¾æ¥"""
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šæå–å®Œæ•´æ–‡æ¡£IDï¼ˆåŒ…å«ç‰ˆæœ¬å·v1/v2ç­‰ï¼‰
        # ä¾‹å¦‚: /content/10.1101/2025.09.22.25336422v1 â†’ 2025.09.22.25336422v1
        doc_id_match = re.search(r'/content/(?:10\.1101/)?([0-9]{4}\.[0-9]{2}\.[0-9]{2}\.[0-9]+v?\d*)', url)
        if doc_id_match:
            full_doc_id = doc_id_match.group(1)
            if 'biorxiv.org' in url.lower():
                return f"https://www.biorxiv.org/content/10.1101/{full_doc_id}.full.pdf"
            elif 'medrxiv.org' in url.lower():
                return f"https://www.medrxiv.org/content/10.1101/{full_doc_id}.full.pdf"
        return None
    
    def _select_primary_pdf(self, attachments: List[Dict]) -> Optional[Dict]:
        """
        é€‰æ‹©ä¸»è¦PDF - ç±»ä¼¼äºZoteroçš„ä¸»è¦é™„ä»¶é€»è¾‘
        """
        if not attachments:
            return None
        
        # æŒ‰ä¼˜å…ˆçº§å’Œç±»å‹æ’åº
        def attachment_score(att):
            score = 0
            
            # åŸºç¡€ä¼˜å…ˆçº§ (è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜)
            score += (10 - att['priority']) * 100
            
            # ç±»å‹å¥–åŠ±
            type_scores = {
                'citation_pdf': 50,
                'osf_download': 45,  
                'chemrxiv_api': 45,
                'site_specific': 35,
                'offline_construction': 25,
                'generic_pdf': 10
            }
            score += type_scores.get(att['type'], 0)
            
            # URLè´¨é‡å¥–åŠ±
            url_lower = att['url'].lower()
            if 'full.pdf' in url_lower:
                score += 25
            if 'manuscript' in url_lower:
                score += 20
            if 'download' in url_lower:
                score += 15
            if 'main' in url_lower:
                score += 10
            
            # URLè´¨é‡æƒ©ç½š
            if 'supplement' in url_lower:
                score -= 30
            if 'supporting' in url_lower:
                score -= 25
            if 'appendix' in url_lower:
                score -= 20
            if 'si.' in url_lower:  # Supporting Information
                score -= 15
            
            return score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        best_attachment = max(attachments, key=attachment_score)
        logger.info(f"ğŸ¯ é€‰æ‹©ä¸»PDF: {best_attachment['url'][:60]}... (ç±»å‹: {best_attachment['type']})")
        
        return best_attachment
    
    def _post_process_by_site(self, metadata: Dict, domain_info: Dict) -> Dict:
        """é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„åå¤„ç†"""
        source = domain_info['source']
        
        # arXivç‰¹æ®Šå¤„ç†
        if source == 'arXiv':
            metadata = self._enhance_arxiv_metadata(metadata)
        
        # é¢„å°æœ¬é€šç”¨å¤„ç†
        if metadata.get('itemType') == 'preprint':
            metadata = self._enhance_preprint_metadata(metadata)
        
        return metadata
    
    def _enhance_arxiv_metadata(self, metadata: Dict) -> Dict:
        """å¢å¼ºarXivå…ƒæ•°æ®"""
        url = metadata.get('url', '')
        
        # æå–arXiv ID
        arxiv_match = re.search(r'arxiv\.org/(?:abs/|pdf/)?([0-9]{4}\.[0-9]{4,5})', url)
        if arxiv_match:
            arxiv_id = arxiv_match.group(1)
            metadata['archiveID'] = arxiv_id
            metadata['repository'] = 'arXiv'
            metadata['libraryCatalog'] = 'arXiv.org'
            
            # æ„é€ è§„èŒƒURLå’ŒPDF URL
            metadata['url'] = f"https://arxiv.org/abs/{arxiv_id}"
            if not metadata.get('pdf_url'):
                metadata['pdf_url'] = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        
        return metadata
    
    def _enhance_preprint_metadata(self, metadata: Dict) -> Dict:
        """å¢å¼ºé¢„å°æœ¬å…ƒæ•°æ®"""
        # è®¾ç½®è®¿é—®æ—¥æœŸ
        if not metadata.get('accessDate'):
            import datetime
            metadata['accessDate'] = datetime.date.today().isoformat()
        
        # é¢„å°æœ¬ç‰¹æœ‰çš„é¢å¤–å­—æ®µ
        if not metadata.get('extra'):
            extra_parts = []
            if metadata.get('repository'):
                extra_parts.append(f"type: article")
            if extra_parts:
                metadata['extra'] = '\n'.join(extra_parts)
        
        return metadata
    
    def _resolve_url(self, url: str, base_url: str) -> str:
        """è§£æç›¸å¯¹URLä¸ºç»å¯¹URL"""
        if url.startswith('http'):
            return url
        return urljoin(base_url, url)

    def _is_anti_crawler_site(self, url: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºåçˆ¬è™«ç½‘ç«™"""
        return any(domain in url.lower() for domain in self.anti_crawler_sites)

    def _handle_403_response(self, url: str) -> Optional[Dict]:
        """å¤„ç†403å“åº”çš„é€šç”¨ç­–ç•¥"""
        logger.warning(f"ğŸ›¡ï¸ æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶: {url}")
        
        # å¯¹äºå·²çŸ¥çš„åçˆ¬è™«ç½‘ç«™ï¼Œå¼ºåˆ¶ä½¿ç”¨æµè§ˆå™¨æ¨¡å¼
        if self._is_anti_crawler_site(url):
            logger.info(f"ğŸŒ åˆ‡æ¢åˆ°æµè§ˆå™¨æ¨¡å¼å¤„ç†åçˆ¬è™«ç½‘ç«™")
            
            # åŠ¨æ€å¯¼å…¥æµè§ˆå™¨æå–å™¨ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
            try:
                from .browser_extractor import BrowserExtractor
                browser_extractor = BrowserExtractor()
                result = browser_extractor.extract_paper_info(url)
                if result and not result.get('error'):
                    logger.info(f"âœ… æµè§ˆå™¨æ¨¡å¼æˆåŠŸç»•è¿‡åçˆ¬è™«é™åˆ¶")
                    return result
                else:
                    logger.warning(f"âš ï¸ æµè§ˆå™¨æ¨¡å¼ä¹Ÿæ— æ³•å¤„ç†: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as e:
                logger.error(f"âŒ æµè§ˆå™¨æ¨¡å¼å¼‚å¸¸: {e}")
        
        # å°è¯•ç¦»çº¿PDFæ„é€ ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
        fallback_pdf = self._construct_offline_pdf_url(url)
        if fallback_pdf:
            return {
                'title': f'Paper from {self._extract_domain(url)}',
                'pdf_url': fallback_pdf,
                'url': url,
                'itemType': 'preprint',
                'source': self._extract_domain(url),
                'note': 'PDFé“¾æ¥é€šè¿‡ç¦»çº¿æ„é€ è·å¾—ï¼Œå¯èƒ½éœ€è¦éªŒè¯æœ‰æ•ˆæ€§',
                'fallback_mode': True
            }
        
        return {
            'error': f'æ— æ³•è®¿é—®é¡µé¢(403)ï¼Œä¸”æ— æœ‰æ•ˆçš„å›é€€ç­–ç•¥: {url}',
            'status_code': 403
        }

    def _construct_offline_pdf_url(self, url: str) -> Optional[str]:
        """ä¸ºåçˆ¬è™«ç½‘ç«™æ„é€ ç¦»çº¿PDF URL"""
        
        if 'biorxiv.org' in url.lower() or 'medrxiv.org' in url.lower():
            # bioRxiv/medRxiv DOIæå–å’ŒPDFæ„é€ 
            doi_match = re.search(r'10\.1101/([0-9]{4}\.[0-9]{2}\.[0-9]{2}\.[0-9]+)', url)
            if doi_match:
                doi_id = doi_match.group(1)
                
                if 'biorxiv.org' in url.lower():
                    return f"https://www.biorxiv.org/content/10.1101/{doi_id}.full.pdf"
                else:
                    return f"https://www.medrxiv.org/content/10.1101/{doi_id}.full.pdf"
        
        elif 'chemrxiv.org' in url.lower():
            # ChemRxivæ–‡ç« IDæå–
            article_match = re.search(r'article-details/([a-f0-9]{24,})', url)
            if article_match:
                article_id = article_match.group(1)
                return f"https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/{article_id}/original/manuscript.pdf"
        
        elif 'osf.io/preprints' in url.lower():
            # OSF preprints (PsyArXiv, SocArXivç­‰)
            preprint_match = re.search(r'osf\.io/preprints/[^/]+/([a-z0-9]+)', url)
            if preprint_match:
                preprint_id = preprint_match.group(1)
                return f"https://osf.io/{preprint_id}/download"
        
        return None

    def _extract_domain(self, url: str) -> str:
        """æå–URLçš„åŸŸå"""
        from urllib.parse import urlparse
        return urlparse(url).netloc
