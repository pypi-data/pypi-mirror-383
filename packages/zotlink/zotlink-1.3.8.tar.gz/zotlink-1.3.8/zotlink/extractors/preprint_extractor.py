#!/usr/bin/env python3
"""
ğŸ”— PreprintæœåŠ¡å™¨æå–å™¨
å¤„ç† medRxiv å’Œ chemRxiv ç­‰é¢„å°æœ¬ç½‘ç«™çš„è®ºæ–‡å…ƒæ•°æ®æå–
"""

import re
import requests
import logging
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
from .base_extractor import BaseExtractor

logger = logging.getLogger(__name__)

class PreprintExtractor(BaseExtractor):
    """medRxivå’ŒchemRxivé¢„å°æœ¬æå–å™¨"""
    
    def __init__(self, session: requests.Session = None):
        """åˆå§‹åŒ–é¢„å°æœ¬æå–å™¨"""
        super().__init__(session)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # æ”¯æŒçš„åŸŸåé…ç½®
        self.domain_config = {
            'medrxiv.org': {
                'name': 'medRxiv',
                'type': 'preprint',
                'base_url': 'https://www.medrxiv.org',
                'pdf_pattern': r'10\.1101/(\d{4}\.\d{2}\.\d{2}\.\d+)',
                'pdf_template': 'https://www.medrxiv.org/content/10.1101/{doi}.full.pdf'
            },
            'chemrxiv.org': {
                'name': 'ChemRxiv', 
                'type': 'preprint',
                'base_url': 'https://chemrxiv.org',
                'pdf_pattern': r'/([a-f0-9-]+)/',
                'pdf_template': 'https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/{article_id}/original/manuscript.pdf'
            }
        }
    
    def can_handle(self, url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¤„ç†æ­¤URL"""
        return any(domain in url.lower() for domain in self.domain_config.keys())
    
    def requires_authentication(self) -> bool:
        """é¢„å°æœ¬ç½‘ç«™é€šå¸¸ä¸éœ€è¦è®¤è¯"""
        return False
    
    def get_database_name(self) -> str:
        """è·å–æ•°æ®åº“åç§°"""
        return "Preprints"
    
    def extract_metadata(self, url: str) -> Dict[str, Any]:
        """ä»é¢„å°æœ¬URLæå–è®ºæ–‡å…ƒæ•°æ®"""
        try:
            # ç¡®å®šç½‘ç«™ç±»å‹
            site_config = None
            for domain, config in self.domain_config.items():
                if domain in url.lower():
                    site_config = config
                    break
            
            if not site_config:
                return {"error": "ä¸æ”¯æŒçš„é¢„å°æœ¬ç½‘ç«™"}
            
            logger.info(f"ğŸ§¬ æå–{site_config['name']}è®ºæ–‡å…ƒæ•°æ®: {url}")
            
            # è·å–é¡µé¢å†…å®¹
            response = self.session.get(url, timeout=15)
            if response.status_code != 200:
                return {'error': f'æ— æ³•è®¿é—®é¡µé¢: {response.status_code}', 'url': url}
            
            soup = BeautifulSoup(response.content, 'html.parser')
            metadata = {}
            
            # æå–æ ‡é¢˜
            title = self._extract_title(soup, site_config)
            if title:
                metadata['title'] = title
                logger.info(f"âœ… æå–æ ‡é¢˜: {title}")
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°æ ‡é¢˜")
            
            # æå–ä½œè€…
            authors = self._extract_authors(soup, site_config)
            if authors:
                metadata['creators'] = authors
                logger.info(f"âœ… æå–ä½œè€…: {len(authors)}ä½")
            
            # æå–æ‘˜è¦
            abstract = self._extract_abstract(soup, site_config)
            if abstract:
                metadata['abstractNote'] = abstract
                logger.info(f"âœ… æå–æ‘˜è¦: {len(abstract)}å­—ç¬¦")
            
            # æå–DOI
            doi = self._extract_doi(soup, url, site_config)
            if doi:
                metadata['DOI'] = doi
                logger.info(f"âœ… æå–DOI: {doi}")
            
            # æ„é€ PDF URL
            pdf_url = self._construct_pdf_url(url, doi, site_config)
            if pdf_url:
                metadata['pdf_url'] = pdf_url
                logger.info(f"âœ… æ„é€ PDFé“¾æ¥: {pdf_url}")
            
            # è®¾ç½®åŸºæœ¬å­—æ®µ
            metadata.update({
                'extractor': site_config['name'],
                'itemType': site_config['type'],
                'url': url,
                'libraryCatalog': site_config['name'],
                'repository': site_config['name']
            })
            
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ {site_config['name'] if site_config else 'é¢„å°æœ¬'}å…ƒæ•°æ®æå–å¤±è´¥: {e}")
            return {
                'error': f'é¢„å°æœ¬å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}',
                'url': url
            }
    
    def _extract_title(self, soup: BeautifulSoup, site_config: Dict) -> Optional[str]:
        """æå–æ ‡é¢˜"""
        title_selectors = [
            'meta[name="citation_title"]',
            'meta[name="dc.title"]',
            'meta[property="og:title"]',
            'h1.highwire-cite-title',
            'h1#page-title',
            'h1.article-title',
            '.article-title h1',
            'h1.entry-title',
            'h1'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    title = element.get('content', '').strip()
                else:
                    title = element.get_text().strip()
                
                # æ¸…ç†æ ‡é¢˜
                title = re.sub(r'\s+', ' ', title)
                title = re.sub(r'^\s*[-â€“]\s*', '', title)
                
                if title and len(title) > 10:
                    return title
        
        return None
    
    def _extract_authors(self, soup: BeautifulSoup, site_config: Dict) -> List[Dict]:
        """æå–ä½œè€…"""
        authors = []
        
        # å°è¯•ä»metaæ ‡ç­¾æå–
        meta_authors = soup.select('meta[name="citation_author"]')
        if meta_authors:
            for meta in meta_authors[:15]:  # é™åˆ¶ä½œè€…æ•°é‡
                author_name = meta.get('content', '').strip()
                if author_name:
                    authors.append(self._parse_author_name(author_name))
        
        # å¦‚æœmetaæ ‡ç­¾æ²¡æœ‰ï¼Œå°è¯•ä»é¡µé¢å†…å®¹æå–
        if not authors:
            author_selectors = [
                '.contrib-group .contrib',
                '.author-list .author',
                '.authors .author',
                '.highwire-cite-authors .contrib'
            ]
            
            for selector in author_selectors:
                author_elements = soup.select(selector)
                if author_elements:
                    for author_el in author_elements[:15]:
                        author_name = author_el.get_text().strip()
                        if author_name:
                            authors.append(self._parse_author_name(author_name))
                    break
        
        return authors
    
    def _parse_author_name(self, author_name: str) -> Dict:
        """è§£æä½œè€…å§“å"""
        # æ¸…ç†ä½œè€…å§“å
        author_name = re.sub(r'[,\d\*â€ â€¡Â§Â¶#].*$', '', author_name).strip()
        
        name_parts = author_name.split()
        if len(name_parts) >= 2:
            return {
                "creatorType": "author",
                "firstName": " ".join(name_parts[:-1]),
                "lastName": name_parts[-1]
            }
        else:
            return {
                "creatorType": "author",
                "firstName": "",
                "lastName": author_name
            }
    
    def _extract_abstract(self, soup: BeautifulSoup, site_config: Dict) -> Optional[str]:
        """æå–æ‘˜è¦"""
        abstract_selectors = [
            'meta[name="citation_abstract"]',
            'meta[name="dc.description"]',
            '.abstract p',
            '#abstract p',
            '.article-summary p',
            '.summary p'
        ]
        
        for selector in abstract_selectors:
            element = soup.select_one(selector)
            if element:
                if element.name == 'meta':
                    abstract = element.get('content', '').strip()
                else:
                    abstract = element.get_text().strip()
                
                if abstract and len(abstract) > 50:
                    return abstract
        
        return None
    
    def _extract_doi(self, soup: BeautifulSoup, url: str, site_config: Dict) -> Optional[str]:
        """æå–DOI"""
        # å°è¯•ä»metaæ ‡ç­¾æå–
        doi_meta = soup.select_one('meta[name="citation_doi"]')
        if doi_meta:
            doi = doi_meta.get('content', '').strip()
            if doi:
                return doi
        
        # ä»URLæå–DOI
        if 'medrxiv.org' in url.lower():
            doi_match = re.search(r'10\.1101/(\d{4}\.\d{2}\.\d{2}\.\d+)', url)
            if doi_match:
                return f"10.1101/{doi_match.group(1)}"
        
        return None
    
    def _construct_pdf_url(self, url: str, doi: Optional[str], site_config: Dict) -> Optional[str]:
        """æ„é€ PDF URL"""
        if 'medrxiv.org' in url.lower() or 'biorxiv.org' in url.lower():
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šä»URLæå–å®Œæ•´çš„æ–‡æ¡£IDï¼ˆåŒ…å«ç‰ˆæœ¬å·v1/v2ç­‰ï¼‰
            # æ­£ç¡®: https://www.medrxiv.org/content/10.1101/2025.09.22.25336422v1
            # æå–: 2025.09.22.25336422v1
            doc_id_match = re.search(r'/content/(?:10\.1101/)?([^/?]+)', url)
            if doc_id_match:
                full_doc_id = doc_id_match.group(1)
                # ç¡®ä¿åŒ…å«ç‰ˆæœ¬å·ï¼ˆå¦‚æœåŸURLæœ‰çš„è¯ï¼‰
                if 'medrxiv.org' in url.lower():
                    return f"https://www.medrxiv.org/content/10.1101/{full_doc_id}.full.pdf"
                else:
                    return f"https://www.biorxiv.org/content/10.1101/{full_doc_id}.full.pdf"
            # å›é€€ï¼šå¦‚æœURLæå–å¤±è´¥ï¼Œä½¿ç”¨DOIï¼ˆå¯èƒ½ç¼ºå°‘ç‰ˆæœ¬å·ï¼‰
            elif doi:
                doi_id = doi.replace('10.1101/', '')
                logger.warning(f"âš ï¸ ä»URLæå–å¤±è´¥ï¼Œä½¿ç”¨DOIæ„é€ PDFé“¾æ¥ï¼ˆå¯èƒ½ç¼ºå°‘ç‰ˆæœ¬å·ï¼‰")
                if 'medrxiv.org' in url.lower():
                    return f"https://www.medrxiv.org/content/10.1101/{doi_id}.full.pdf"
                else:
                    return f"https://www.biorxiv.org/content/10.1101/{doi_id}.full.pdf"
        
        elif 'chemrxiv.org' in url.lower():
            # ğŸ¯ ä¿®å¤ï¼šæ”¯æŒ24å­—ç¬¦å’Œ36å­—ç¬¦çš„Article ID
            # ä¾‹å¦‚ï¼š68d4f0953e708a7649229138 (24å­—ç¬¦) æˆ– UUIDæ ¼å¼ (36å­—ç¬¦)
            article_match = re.search(r'article-details/([a-f0-9-]{24,})', url)
            if article_match:
                article_id = article_match.group(1)
                logger.info(f"âœ… æå–ChemRxiv Article ID: {article_id}")
                return f"https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/{article_id}/original/manuscript.pdf"
            else:
                logger.warning(f"âš ï¸ æ— æ³•ä»URLæå–ChemRxiv Article ID: {url}")
        
        return None
    
    def test_access(self, test_url: str = None) -> bool:
        """æµ‹è¯•ç½‘ç«™è®¿é—®"""
        if not test_url:
            test_url = "https://www.medrxiv.org/"
        
        try:
            response = self.session.get(test_url, timeout=10)
            return response.status_code == 200
        except:
            return False
    
    def get_supported_item_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¡ç›®ç±»å‹"""
        return ['preprint']
