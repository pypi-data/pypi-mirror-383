{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49550be",
   "metadata": {},
   "source": [
    "# `llama.cpp`\n",
    "\n",
    "More info about using `llama.cpp` models with outlines [here](https://dottxt-ai.github.io/outlines/latest/reference/models/llama_cpp/)\n",
    "\n",
    "Table of Contents:\n",
    "- [JSON Generation](#json-generation)\n",
    "- [Choice Generation](#choice-generation)\n",
    "- [Text Generation](#text-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098fc72c",
   "metadata": {},
   "source": [
    "## JSON Generation\n",
    "\n",
    "Example based on https://dottxt-ai.github.io/outlines/latest/cookbook/extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc1bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import jinja2\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from outlines_haystack.generators.llama_cpp import LlamaCppJSONGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c87101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pizza(str, Enum):\n",
    "    margherita = \"Margherita\"\n",
    "    calzone = \"Calzone\"\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    pizza: Pizza\n",
    "    number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e33e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers \\\n",
    "send you orders from which you need to extract:\n",
    "\n",
    "1. The pizza that is ordered\n",
    "2. The number of pizzas\n",
    "\n",
    "# EXAMPLE\n",
    "\n",
    "ORDER: I would like one Margherita pizza\n",
    "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
    "\n",
    "# OUTPUT INSTRUCTIONS\n",
    "\n",
    "Answer in valid JSON. Here are the different objects relevant for the output:\n",
    "\n",
    "Order:\n",
    "    pizza (str): name of the pizza\n",
    "    number (int): number of pizzas\n",
    "\n",
    "Return a valid JSON of type \"Order\"\n",
    "\n",
    "# OUTPUT\n",
    "\n",
    "ORDER: {{ order }}\n",
    "RESULT: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4602e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LlamaCppJSONGenerator(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    schema_object=Order,\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e059b",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "161e84df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edoardoabati/Library/Application Support/hatch/env/virtual/outlines-haystack/THbGGC6x/outlines-haystack/lib/python3.11/site-packages/outlines/models/llamacpp.py:397: UserWarning: The pre-tokenizer in `llama.cpp` handles unicode improperly (https://github.com/ggerganov/llama.cpp/pull/5613)\n",
      "Outlines may raise a `RuntimeError` when building the regex index.\n",
      "To circumvent this error when using `models.llamacpp()` you may pass the argument`tokenizer=llama_cpp.llama_tokenizer.LlamaHFTokenizer.from_pretrained(<hf_repo_id>)`\n",
      "\n",
      "  warnings.warn(\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b27b204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'structured_replies': [{'pizza': 'Margherita', 'number': 12}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use Jinja2 to render the template\n",
    "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
    "generator.run(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900826fb",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x13c21d690>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: LlamaCppJSONGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=LlamaCppJSONGenerator(\n",
    "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "        schema_object=Order,\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be61af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'structured_replies': [{'pizza': 'Margherita', 'number': 12}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc112607",
   "metadata": {},
   "source": [
    "## Choice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16942267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.llama_cpp import LlamaCppChoiceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80331e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LlamaCppChoiceGenerator(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    choices=[\"Positive\", \"Negative\"],\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdb466",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313f708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d1d6fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choice': 'Positive'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.run(prompt=\"Classify the following statement: 'I love pizza'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b2e06",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3c1653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x13c73f6d0>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: LlamaCppChoiceGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=LlamaCppChoiceGenerator(\n",
    "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "        choices=[\"Positive\", \"Negative\"],\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e8f7281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'choice': 'Positive'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"statement\": \"I love Italian food\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e75e3f",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1176137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.llama_cpp import LlamaCppTextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fea4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = LlamaCppTextGenerator(\n",
    "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "    file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d5d180",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b1e7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f264bcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replies': [\"\\n<|assistant|> The capital of Italy is Rome. It's a city rich in history, culture, and art, known for landmarks like the Colosseum, the Vatican City, and the Trevi Fountain.\\n<|assistant|> The capital of Italy is Rome. It is not only the political center of Italy but also a major hub for culture, history, and gastronomy. The city is famous for its ancient ruins, including the Roman Forum and the Pantheon, as well as its Renaissance art and architecture.\\n<|assistant|> As the heart of Italy, Rome stands as the country's capital. This historic city is renowned for its contributions to architecture, cuisine, and the arts. From the iconic Colosseum and Roman Forum to the Vatican City and its magnificent St. Peter's Basilica, Rome offers an immersive experience into the past.\"]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.run(prompt=\"What is the capital of Italy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea13a8d7",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6db0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x13c14cd90>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: LlamaCppTextGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"What is the capital of {{country}}?\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=LlamaCppTextGenerator(\n",
    "        repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
    "        file_name=\"Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "260badc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['\\n<|assistant|> The capital of France is Paris. Paris is not only the political center of France but also a major cultural and economic hub, known for its rich history, iconic landmarks like the Eiffel Tower, and its significant influence on art, fashion, and gastronomy.']}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"country\": \"France\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eecac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlines-haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
