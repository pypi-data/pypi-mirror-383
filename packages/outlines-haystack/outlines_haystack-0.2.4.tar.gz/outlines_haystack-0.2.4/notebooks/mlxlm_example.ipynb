{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `mlx-lm`\n",
    "\n",
    "More info about using `mlx-lm` models with outlines [here](https://dottxt-ai.github.io/outlines/latest/reference/models/mlxlm/)\n",
    "\n",
    "Table of Contents:\n",
    "- [JSON Generation](#json-generation)\n",
    "- [Text Generation](#text-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation\n",
    "\n",
    "Example based on https://dottxt-ai.github.io/outlines/latest/cookbook/extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import jinja2\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from outlines_haystack.generators.mlxlm import MLXLMJSONGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pizza(str, Enum):\n",
    "    margherita = \"Margherita\"\n",
    "    calzone = \"Calzone\"\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    pizza: Pizza\n",
    "    number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers \\\n",
    "send you orders from which you need to extract:\n",
    "\n",
    "1. The pizza that is ordered\n",
    "2. The number of pizzas\n",
    "\n",
    "# EXAMPLE\n",
    "\n",
    "ORDER: I would like one Margherita pizza\n",
    "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
    "\n",
    "# OUTPUT INSTRUCTIONS\n",
    "\n",
    "Answer in valid JSON. Here are the different objects relevant for the output:\n",
    "\n",
    "Order:\n",
    "    pizza (str): name of the pizza\n",
    "    number (int): number of pizzas\n",
    "\n",
    "Return a valid JSON of type \"Order\"\n",
    "\n",
    "# OUTPUT\n",
    "\n",
    "ORDER: {{ order }}\n",
    "RESULT: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MLXLMJSONGenerator(\n",
    "    model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "    schema_object=Order,\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb6d9b889764773a80835a4410dd2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'structured_replies': [{'pizza': 'Margherita', 'number': 12}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use Jinja2 to render the template\n",
    "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
    "generator.run(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x38429b710>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: MLXLMJSONGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=MLXLMJSONGenerator(\n",
    "        model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "        schema_object=Order,\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0500bafa263d43f2b42f09301ea5e285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'structured_replies': [{'pizza': 'Margherita', 'number': 12}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.mlxlm import MLXLMChoiceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MLXLMChoiceGenerator(\n",
    "    model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "    choices=[\"Positive\", \"Negative\"],\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e40464e373a472291cb2e2271ef3784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choice': 'Positive'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.run(prompt=\"Classify the following statement: 'I love pizza'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x177679b90>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: MLXLMChoiceGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=MLXLMChoiceGenerator(\n",
    "        model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "        choices=[\"Positive\", \"Negative\"],\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547f176c4a2c4521994ef06209280404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'choice': 'Positive'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"statement\": \"I love Italian food\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.mlxlm import MLXLMTextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MLXLMTextGenerator(\n",
    "    model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0a8e5af7a74bcba763bb1e3ec13d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replies': [\"Rome\\nRome is the capital and largest city of Italy, with a population of over 2.8 million people. It is a global center for art, architecture, history, and culture, and is home to numerous iconic landmarks such as the Colosseum, the Pantheon, and the Vatican City, including the Sistine Chapel and St. Peter's Basilica.\\nThe city has a rich history, dating back to the 8th century BC, when it was founded by the ancient Romans. It has been ruled by various empires and civilizations, including the Roman Empire, the Holy Roman Empire, and the Kingdom of Italy.\\nRome is also a major tourist destination, attracting millions of visitors each year. Its unique blend of ancient and modern architecture, as well as its vibrant cultural scene, make it a fascinating city to visit and explore.\\nSome of the most famous attractions in Rome include:\\n* The Colosseum: a ancient amphitheater that hosted gladiator battles and other events\\n* The Pantheon: a ancient temple dedicated to all the gods of ancient Rome\\n* The Vatican City: a sovereign city-state within Rome, home to the Pope and numerous iconic landmarks such as St. Peter's Basilica and the Sistine Chapel\\n* The Roman Forum: a ancient public space that was once the heart of ancient Rome\\n* The Trevi Fountain: a beautiful baroque fountain that is said to grant wishes to those who toss a coin into its waters\\n* The Spanish Steps: a grand staircase that connects the Piazza di Spagna to the Trinita dei Monti church\\n* The Castel Sant'Angelo: a ancient fortress that offers stunning views of the city\\n\\nOverall, Rome is a city that has something for everyone, from history and culture to architecture and entertainment. Its unique blend of ancient and modern attractions makes it a fascinating destination for tourists and locals alike.\"]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.run(prompt=\"What is the capital of Italy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x37d7b1f90>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: MLXLMTextGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"What is the capital of {{country}}?\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=MLXLMTextGenerator(\n",
    "        model_name=\"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.3},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80bd1be39ba4289bca436bd57e522de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['Paris\\nThe capital of France is indeed Paris. This is a well-known fact that many people are familiar with. Paris is not only the capital but also the largest city in France, known for its iconic landmarks like the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum.\\n\\nHowever, I must correct a common misconception. The capital of France is not \"Paris\" (in quotes), but simply \"Paris\". The word \"Paris\" is a proper noun, referring to the city, and it\\'s not enclosed in quotes to indicate that it\\'s a common noun.\\n\\nSo, to summarize: the capital of France is indeed Paris, and it\\'s a well-known fact that many people are familiar with!']}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"country\": \"France\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlines-haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
