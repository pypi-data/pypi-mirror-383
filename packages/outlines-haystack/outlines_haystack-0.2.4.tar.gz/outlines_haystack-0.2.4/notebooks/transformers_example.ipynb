{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤗 `transformers`\n",
    "\n",
    "More info about using `transformers` models with outlines [here](https://dottxt-ai.github.io/outlines/latest/reference/models/transformers/)\n",
    "\n",
    "Table of Contents:\n",
    "- [JSON Generation](#json-generation)\n",
    "- [Text Generation](#text-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation\n",
    "\n",
    "Example based on https://dottxt-ai.github.io/outlines/latest/cookbook/extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import jinja2\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from outlines_haystack.generators.transformers import TransformersJSONGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pizza(str, Enum):\n",
    "    margherita = \"Margherita\"\n",
    "    calzone = \"Calzone\"\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    pizza: Pizza\n",
    "    number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers \\\n",
    "send you orders from which you need to extract:\n",
    "\n",
    "1. The pizza that is ordered\n",
    "2. The number of pizzas\n",
    "\n",
    "# EXAMPLE\n",
    "\n",
    "ORDER: I would like one Margherita pizza\n",
    "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
    "\n",
    "# OUTPUT INSTRUCTIONS\n",
    "\n",
    "Answer in valid JSON. Here are the different objects relevant for the output:\n",
    "\n",
    "Order:\n",
    "    pizza (str): name of the pizza\n",
    "    number (int): number of pizzas\n",
    "\n",
    "Return a valid JSON of type \"Order\"\n",
    "\n",
    "# OUTPUT\n",
    "\n",
    "ORDER: {{ order }}\n",
    "RESULT: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TransformersJSONGenerator(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    schema_object=Order,\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e0c795b4cf49ec81f4445f70f70ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'structured_replies': [{'pizza': 'Margherita', 'number': 12}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we use Jinja2 to render the template\n",
    "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
    "generator.run(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x6b0a071d0>\n",
       "🚅 Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: TransformersJSONGenerator\n",
       "🛤️ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=TransformersJSONGenerator(\n",
    "        model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        schema_object=Order,\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7108b493ba549ba9d844321c6bf941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'structured_replies': [{'pizza': 'Margherita', 'number': 12}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.transformers import TransformersChoiceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TransformersChoiceGenerator(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    choices=[\"Positive\", \"Negative\"],\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf37c1f82624856bb17977567b3da54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choice': 'Positive'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.run(prompt=\"Classify the following statement: 'I love pizza'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x175834990>\n",
       "🚅 Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: TransformersChoiceGenerator\n",
       "🛤️ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=TransformersChoiceGenerator(\n",
    "        model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        choices=[\"Positive\", \"Negative\"],\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb59750777624dc5a6d04456a070f774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'choice': 'Positive'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"statement\": \"I love Italian food\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.transformers import TransformersTextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TransformersTextGenerator(\n",
    "    model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device=device,\n",
    "    sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f46be7fadd4364a7a3974ac9414be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'replies': ['\\n\\n# Answer\\nThe capital of Italy is Rome.']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.run(prompt=\"What is the capital of Italy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x14f1f3d90>\n",
       "🚅 Components\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: TransformersTextGenerator\n",
       "🛤️ Connections\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"What is the capital of {{country}}?\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=TransformersTextGenerator(\n",
    "        model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        device=device,\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da46212db844d16b93a092c23f31e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'llm': {'replies': ['\\n\\n# Answer\\nThe capital of France is Paris.']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"country\": \"France\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outlines-haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
