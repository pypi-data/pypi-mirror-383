{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI, Azure OpenAI or compatible APIs\n",
    "\n",
    "More info about using OpenAI, Azure OpenAI models or compatible APIs with `outlines` [here](https://dottxt-ai.github.io/outlines/latest/reference/models/openai/)\n",
    "\n",
    "Table of Contents:\n",
    "- [JSON Generation](#json-generation)\n",
    "- [Text Generation](#text-generation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Generation\n",
    "\n",
    "Example based on https://dottxt-ai.github.io/outlines/latest/cookbook/extraction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import jinja2\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from outlines_haystack.generators.openai import OpenAIJSONGenerator\n",
    "\n",
    "# or for Azure\n",
    "# from outlines_haystack.generators.azure_openai import AzureOpenAIJSONtGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pizza(str, Enum):\n",
    "    margherita = \"Margherita\"\n",
    "    calzone = \"Calzone\"\n",
    "\n",
    "\n",
    "class Order(BaseModel):\n",
    "    pizza: Pizza\n",
    "    number: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are the owner of a pizza parlor. Customers \\\n",
    "send you orders from which you need to extract:\n",
    "\n",
    "1. The pizza that is ordered\n",
    "2. The number of pizzas\n",
    "\n",
    "# EXAMPLE\n",
    "\n",
    "ORDER: I would like one Margherita pizza\n",
    "RESULT: {\"pizza\": \"Margherita\", \"number\": 1}\n",
    "\n",
    "# OUTPUT INSTRUCTIONS\n",
    "\n",
    "Answer in valid JSON. Here are the different objects relevant for the output:\n",
    "\n",
    "Order:\n",
    "    pizza (str): name of the pizza\n",
    "    number (int): number of pizzas\n",
    "\n",
    "Return a valid JSON of type \"Order\"\n",
    "\n",
    "# OUTPUT\n",
    "\n",
    "ORDER: {{ order }}\n",
    "RESULT: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = OpenAIJSONGenerator(model_name=\"gpt-4o-mini\", schema_object=Order, generation_kwargs={\"temperature\": 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use Jinja2 to render the template\n",
    "prompt = jinja2.Template(prompt_template).render(order=\"Is it possible to have 12 margheritas?\")\n",
    "generator.run(prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=OpenAIJSONGenerator(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        schema_object=Order,\n",
    "        sampling_algorithm_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"order\": \"Is it possible to have 12 margheritas?\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.openai import OpenAIChoiceGenerator\n",
    "\n",
    "# or for Azure\n",
    "# from outlines_haystack.generators.azure_openai import AzureOpenAIChoiceGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = OpenAIChoiceGenerator(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    choices=[\"Positive\", \"Negative\"],\n",
    "    generation_kwargs={\"temperature\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.run(prompt=\"Classify the following statement: 'I love pizza'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Classify the following statement: '{{statement}}'\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=OpenAIChoiceGenerator(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        choices=[\"Positive\", \"Negative\"],\n",
    "        generation_kwargs={\"temperature\": 0.5},\n",
    "    ),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"statement\": \"I love Italian food\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from outlines_haystack.generators.openai import OpenAITextGenerator\n",
    "\n",
    "# or for Azure\n",
    "# from outlines_haystack.generators.azure_openai import AzureOpenAITextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = OpenAITextGenerator(model_name=\"gpt-4o-mini\", generation_kwargs={\"temperature\": 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standalone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.run(prompt=\"What is the capital of Italy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What is the capital of {{country}}?\"\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "pipeline.add_component(\n",
    "    instance=OpenAITextGenerator(model_name=\"gpt-4o-mini\", generation_kwargs={\"temperature\": 0.5}),\n",
    "    name=\"llm\",\n",
    ")\n",
    "pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run({\"prompt_builder\": {\"country\": \"France\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
