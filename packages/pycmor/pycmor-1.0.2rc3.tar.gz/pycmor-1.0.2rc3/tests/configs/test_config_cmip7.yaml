pycmor:
  version: "unreleased"
  use_xarray_backend: True
  warn_on_no_rule: False
  minimum_jobs: 8
  maximum_jobs: 10
general:
  name: "test"
  description: "This is a test configuration"
  maintainer: "pgierz"
  email: "pgierz@awi.de"
  cmor_version: "CMIP7"
  mip: "CMIP"
  frequency: "mon"
  CMIP_Tables_Dir: "./CMIP7_DReq_Software/scripts/variable_info/"
  CV_Dir: "./cmip6-cmor-tables/CMIP6_CVs"
pipelines:
  - name: "test_pipeline"
    uses: "pycmor.core.pipeline.TestingPipeline"
  - name: "my_pipeline"
    steps:
      - "pycmor.std_lib.generic.dummy_load_data"
      - "pycmor.std_lib.units.handle_unit_conversion"
      - "pycmor.std_lib.generic.dummy_save_data"
  - name: "sleeper_pipeline"
    steps:
      - "pycmor.std_lib.generic.dummy_sleep"
rules:
  - name: "tas_rule"
    pipelines: ["my_pipeline"]
    enabled: true
    description: "This is a test rule"
    cmor_variable: "tas"
    input_type: "xr.DataArray"
    input_source: "xr_tutorial"
    output_directory: .
    variant_label: r1i1p1f1
    experiment_id: piControl
    source_id: "AWI-CM-1-1-HR"
    model_component: ocean
    grid_label: gn
    inputs:
      - path: "./"
        pattern: "test_input"
      - path: "./some/other/path"
        pattern: "test_input2"
  - name: test_rule3
    enabled: false
    inputs:
      - path: "/a/b/c"
        pattern: ".*"
    cmor_variable: "so"
    output_directory: .
    variant_label: r1i1p1f1
    experiment_id: piControl
    source_id: "AWI-CM-1-1-HR"
    model_component: ocean
    grid_label: gn
  - name: test_rule4
    cmor_variable: "thetao"
    pipelines: ["sleeper_pipeline"]
    output_directory: .
    variant_label: r1i1p1f1
    experiment_id: piControl
    source_id: "AWI-CM-1-1-HR"
    model_component: ocean
    grid_label: gn
    inputs:
      - path: "/a/b/c"
        pattern: ".*"
distributed:
  worker:
    memory:
      target: 0.6 # Target 60% of worker memory usage
      spill: 0.7 # Spill to disk when 70% of memory is used
      pause: 0.8 # Pause workers if memory usage exceeds 80%
      terminate: 0.95 # Terminate workers at 95% memory usage
    resources:
      CPU: 4 # Assign 4 CPUs per worker
    death-timeout: 60 # Worker timeout if no heartbeat (seconds)
# SLURM-specific settings for launching workers
jobqueue:
  slurm:
    queue: compute # SLURM queue/partition to submit jobs
    project: ab0246 # SLURM project/account name
    cores: 4 # Number of cores per worker
    memory: 128GB # Memory per worker
    walltime: '00:30:00' # Maximum walltime per job
    # interface: ib0 # Network interface for communication
    job-extra: # Additional SLURM job options
      - '--exclusive' # Run on exclusive nodes
    # How to launch workers and scheduler
    worker-template:
      # Command to launch a Dask worker via SLURM
      command: |
        srun --ntasks=1 --cpus-per-task=4 --mem=128G dask-worker \
          --nthreads 4 --memory-limit 128GB --death-timeout 60
    # Command to launch the Dask scheduler
    scheduler-template:
      command: |
        srun --ntasks=1 --cpus-per-task=1 dask-scheduler
