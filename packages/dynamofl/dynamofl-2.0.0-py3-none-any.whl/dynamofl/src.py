"""
User facing methods on the dfl object of core sdk
"""
import logging
from typing import Any, Dict, List, Literal, Union

from .api.billing import BillingAPI
from .api.custom_rag_app import CustomRagAPI
from .compatibility_engine import CompatibilityEngine
from .datasets.dataset import Dataset
from .datasets.hf_dataset import HFDataset
from .entities.billing import BillingReport
from .entities.custom_rag_app import (
    AllCustomRagApplicationResponseEntity,
    AuthTypeEnum,
    CustomRagApplicationResponseEntity,
    CustomRagApplicationRoutesEntity,
    CustomRagApplicationRoutesResponseEntity,
    RouteTypeEnum,
)
from .entities.dataset import HFDatasetEntity
from .entities.model import RemoteModelEntity
from .entities.test import TestEntity
from .Helpers import Helpers, URLUtils
from .logging import set_logger
from .MessageHandler import _MessageHandler
from .models import remote_model
from .State import _State
from .tests.cpu_config import CPUSpecification
from .tests.gpu_config import GPUSpecification
from .tests.test import Test
from .vector_db import ChromaDB, CustomRagDB, LlamaIndexDB, LlamaIndexWithChromaDB, PostgresVectorDB

try:
    from typing import Optional
except ImportError:
    from typing_extensions import Optional

RETRY_AFTER = 5  # seconds


class DynamoFL:
    """Creates client instance that communicates with the API through REST and websockets.

    Args:
        token - Your auth token. Required.

        host - API server url. Defaults to DynamoFL prod API.

        metadata - Sets a default metadata object for attach_datasource calls; can be overriden.

        log_level - Set the log_level for the client.
            Accepts all of logging._Level. Defaults to logging.INFO.
    """

    def __init__(
        self,
        token: str,
        host: str = "https://api.dynamofl.com",
        metadata: object = None,
        log_level=logging.INFO,
        bi_directional_client=True,
    ):
        self._state = _State(token, host, metadata=metadata)
        if bi_directional_client:
            self._messagehandler = _MessageHandler(self._state)
            self._messagehandler.connect_to_ws()

        set_logger(log_level=log_level)
        self._compatibility_engine = CompatibilityEngine()
        self._compatibility_engine.validate_version_compatibility(self._state.get_version())

    def attach_datasource(self, key, train=None, test=None, name=None, metadata=None):
        return self._state.attach_datasource(
            key, train=train, test=test, name=name, metadata=metadata
        )

    def delete_datasource(self, key):
        return self._state.delete_datasource(key)

    def get_datasources(self):
        return self._state.get_datasources()

    def delete_project(self, key):
        return self._state.delete_project(key)

    def get_user(self):
        return self._state.get_user()

    def create_project(
        self,
        base_model_path,
        params,
        dynamic_trainer_key=None,
        dynamic_trainer_path=None,
    ):
        return self._state.create_project(
            base_model_path,
            params,
            dynamic_trainer_key=dynamic_trainer_key,
            dynamic_trainer_path=dynamic_trainer_path,
        )

    def get_project(self, project_key):
        return self._state.get_project(project_key)

    def get_projects(self):
        return self._state.get_projects()

    def is_datasource_labeled(self, project_key=None, datasource_key=None):
        """
        Accepts a valid datasource_key and project_key
        Returns True if the datasource is labeled for the project; False otherwise

        """
        return self._state.is_datasource_labeled(
            project_key=project_key, datasource_key=datasource_key
        )

    def upload_dynamic_trainer(self, dynamic_trainer_key, dynamic_trainer_path):
        return self._state.upload_dynamic_trainer(dynamic_trainer_key, dynamic_trainer_path)

    def download_dynamic_trainer(self, dynamic_trainer_key):
        return self._state.download_dynamic_trainer(dynamic_trainer_key)

    def delete_dynamic_trainer(self, dynamic_trainer_key):
        return self._state.delete_dynamic_trainer(dynamic_trainer_key)

    def get_dynamic_trainer_keys(self):
        return self._state.get_dynamic_trainer_keys()

    def create_performance_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        gpu: GPUSpecification,
        performance_metrics: List[str],
        input_column: str,
        topic_list: Optional[List[str]] = None,
        prompts_column: Optional[str] = None,
        reference_column: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Creates a performance test on a model with a dataset
        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            performance_metrics (List[str]): Performance evaluation metrics used.
                E.g rouge, bertscore
            input_column (str): Input column in the dataset to use for performance evaluation
            topic_list (Optional[List[str]]): List of topics to cluster the result
            prompts_column (Optional[str]): Column to specify the prompts for the input
            reference_column (Optional[str]): Column to specify the reference for the input
            grid (List[Dict[str, List[str |  float  |  int]]]): Grid of hyper parameters
        Returns:
            TestEntity: TestEntity object
        """
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                performance_metrics=performance_metrics,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                topic_list=topic_list,
                prompts_column_name=prompts_column,
                mia_input_text_column_name=input_column,
                mia_target_text_column_name=reference_column,
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="perf-test",
            compute=gpu,
        )

    def create_membership_inference_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        input_column: str,
        reference_column: Optional[str] = None,
        base_model: Optional[str] = None,
        pii_classes: Optional[List[str]] = None,
        regex_expressions: Optional[Dict[str, str]] = None,
    ) -> TestEntity:
        """Create a membership inference test on a model with a dataset

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            input_column (str): Input column in the dataset to use for performance evaluation
            reference_column (Optional[str]): Column to specify the reference for the input,
                defaults to input_column
            base_model (Optional[str]): Base model to use for the attack
            pii_classes (Optional[List[str]]): PII classes to attack. E.g PERSON
            regex_expressions (Optional[Dict[str, str]]): list of regex expressions to use
                for extraction

        Returns:
            TestEntity: TestEntity object
        """

        Helpers.validate_pii_inputs(regex_expressions)

        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                pii_classes=pii_classes,
                regex_expressions=regex_expressions,
            ),
            "model": Helpers.construct_dict_filtering_none_values(
                base_model=base_model,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                mia_input_text_column_name=input_column,
                mia_target_text_column_name=reference_column,
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=[{}],
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="membership_inference",
            compute=compute,
        )

    def create_pii_inference_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        pii_ref_column: str,
        pii_classes: Optional[List[str]] = None,
        num_targets: Optional[int] = None,
        candidate_size: Optional[int] = None,
        regex_expressions: Optional[Dict[str, str]] = None,
        prompts_column: Optional[str] = None,
        sample_and_shuffle: Optional[int] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a pii inference test on a model with a dataset
        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            pii_ref_column (str): Column in the dataset to sample prompts from
            pii_classes (Optional[List[str]]): PII classes to attack. E.g PERSON
            num_targets (int): Number of target sequence to sample to attack
            candidate_size (int): Number of PII candidates to sample randomly for the attack.
            regex_expressions (Optional[Dict[str, str]]): list of regex expressions to use
                for extraction
            prompts_column (Optional[str]): Column to specify the prompts for the input.
                Used for seq2seq models only.
            sample_and_shuffle (int): number of times to sample and shuffle candidates
            grid (List[Dict[str, List[str |  float  |  int]]]): Grid of hyper parameters
        Returns:
            TestEntity: TestEntity object
        """

        Helpers.validate_pii_inputs(regex_expressions)

        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                pii_classes=pii_classes,
                num_targets=num_targets,
                candidate_size=candidate_size,
                regex_expressions=regex_expressions,
                sample_and_shuffle=sample_and_shuffle,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                column_name=pii_ref_column, prompts_column_name=prompts_column
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="pii_inference",
            compute=compute,
        )

    def create_pii_reconstruction_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        pii_ref_column: str,
        pii_classes: Optional[List[str]] = None,
        num_targets: Optional[int] = None,
        candidate_size: Optional[int] = None,
        sampling_rate: Optional[float] = None,
        regex_expressions: Optional[Dict[str, str]] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a pii reconstruction test on a model with a dataset
        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            pii_ref_column (str): Column in the dataset to sample prompts from
            pii_classes (Optional[List[str]]): PII classes to attack. E.g PERSON
            num_targets (int): Number of target sequence to sample to attack
            candidate_size (int): Number of PII candidates to sample randomly for the attack.
                Ranks PII candidates based on highest likelihood and selects top
                candidate.
            sampling_rate (Optional[float]): The number of times we prompt the model during a test.
            regex_expressions (Optional[Dict[str, str]]): list of regex expressions to use
                for extraction
            grid (List[Dict[str, List[str |  float  |  int]]]): Grid of hyper parameters
        Returns:
            TestEntity: TestEntity object
        """

        Helpers.validate_pii_inputs(regex_expressions)

        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                pii_classes=pii_classes,
                num_targets=num_targets,
                candidate_size=candidate_size,
                sampling_rate=sampling_rate,
                regex_expressions=regex_expressions,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(column_name=pii_ref_column),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="pii_reconstruction",
            compute=compute,
        )

    def create_hallucination_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        hallucination_metrics: List[str],
        input_column: str,
        topic_list: Optional[List[str]] = None,
        prompts_column: Optional[str] = None,
        reference_column: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a hallucination test on a model with a dataset

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            hallucation_metrics (List[str]): Hallucation metrics used. E.g
                nli-consistency, unieval-factuality
            topic_list (Optional[List[str]]): List of topics to cluster the result
            input_column (str): Input column in the dataset to use for performance evaluation
            prompts_column (Optional[str]): Column to specify the prompts for the input
            reference_column (Optional[str]): Column to specify the reference for the input
            grid (List[Dict[str, List[str |  float  |  int]]]): Grid of hyper parameters

        Returns:
            TestEntity: TestEntity object
        """
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                hallucination_metrics=hallucination_metrics,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                topic_list=topic_list,
                prompts_column_name=prompts_column,
                mia_input_text_column_name=input_column,
                mia_target_text_column_name=reference_column,
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="hallucination-test",
            compute=compute,
        )

    def create_pii_extraction_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        pii_ref_column: str,
        pii_classes: Optional[List[str]] = None,
        extraction_prompt: Optional[str] = None,
        sampling_rate: Optional[float] = None,
        regex_expressions: Optional[Dict[str, str]] = None,
        prompts_column: Optional[str] = None,
        responses_column: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a pii extraction test on a model with a dataset

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            pii_classes (Optional[List[str]]): PII classes to attack. E.g PERSON
            extraction_prompt (Optional[str]): Prompt for PII extraction. Can be '' (empty string),
            or one of the pre-defined strategies: 'dfl_dynamic', 'dfl_ata'
            sampling_rate (Optional[float]): The number of times we prompt the model during a test.
            regex_expressions (Optional[Dict[str, str]]): list of regex expressions to use
                for extraction
            pii_ref_column (str): Column in the dataset to sample prompts from
            prompts_column (Optional[str]): containing the dataset prompts. Used for encoder-decoder models, and dfl_dynamic prompting strategy only.
            responses_column (Optional[str]): Column containing the dataset responses to prompts. Used for dfl_dynamic prompting strategy only.
            grid (List[Dict[str, List[str |  float  |  int]]]): Grid of hyper parameters

        Returns:
            TestEntity: TestEntity object
        """

        Helpers.validate_pii_inputs(regex_expressions)
        Helpers.validate_extraction_prompt(extraction_prompt)

        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                pii_classes=pii_classes,
                extraction_prompt=extraction_prompt,
                sampling_rate=sampling_rate,
                regex_expressions=regex_expressions,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                column_name=pii_ref_column,
                prompts_column_name=prompts_column,
                responses_column_name=responses_column,
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="pii_extraction",
            compute=compute,
        )

    def create_rag_hallucination_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        rag_hallucination_metrics: List[str],
        input_column: str,
        example_column: Optional[str] = None,
        question_type_column: Optional[str] = None,
        topic_list: Optional[List[str]] = None,
        prompts_column: Optional[str] = None,
        prompt_template: Optional[str] = None,
        vector_db: Optional[
            Union[ChromaDB, LlamaIndexDB, LlamaIndexWithChromaDB, PostgresVectorDB, CustomRagDB]
        ] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a rag hallucination test on a model with a dataset

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            rag_hallucation_metrics (List[str]): Rag hallucination metrics used. E.g
                nli-consistency, unieval-factuality
            topic_list (Optional[List[str]]): List of topics to cluster the result
            input_column (str): Input column in the dataset to use for rag hallucination evaluation
            example_column (Optional[str]): Example column used for few shot examples
            question_type_column (Optional[str]): Question type column used for question type view
            prompts_column (Optional[str]): Column to specify the prompts for the input
            prompt_template (Optional[str]): Prompt template to use for the attack
            vector_db (Optional[Union[ChromaDB, LlamaIndexDB, LlamaIndexWithChromaDB]]):
                Vector db to use for the attack

        Returns:
            TestEntity: TestEntity object
        """
        if vector_db:
            vdb = vector_db.__dict__
        else:
            vdb = None

        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                rag_hallucination_metrics=rag_hallucination_metrics,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                topic_list=topic_list,
                prompt_template=prompt_template,
                prompts_column_name=prompts_column,
                mia_input_text_column_name=input_column,
                example_text_column_name=example_column,
                question_type_column_name=question_type_column,
                vector_db=vdb,
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="rag-hallucination-test",
            compute=compute,
        )

    def create_sequence_extraction_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        memorization_granularity: str,
        sampling_rate: int,
        is_finetuned: bool,
        title: Optional[str] = None,
        title_column: Optional[str] = None,
        text_column: Optional[str] = None,
        source: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a sequence extraction test on a model with a dataset

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            dataset_id (str): Id of the dataset to be used
            gpu (GPUSpecification): GPU specification
            memorization_granularity (str): Granularity of memorization. E.g paragraph, sentence
            sampling_rate (int): The number of times we prompt the model during a test.
            is_finetuned (bool): Whether the model is finetuned or not; determines
                whether to generate the fine-tuned or the base model report
            title (Optional[str]): Title to use for the attack
            title_column (Optional[str]): Title column to use for the attack
            text_column (Optional[str]): Text column to use for the attack
            source (Optional[str]): Source of the dataset, e.g. NYT

        Returns:
            TestEntity: TestEntity object
        """
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                memorization_granularity=memorization_granularity,
                sampling_rate=sampling_rate,
                source=source,
                is_finetuned=is_finetuned,
            ),
            "dataset": Helpers.construct_dict_filtering_none_values(
                title=title,
                title_column=title_column,
                text_column=text_column,
            ),
        }

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="sequence_extraction",
            compute=compute,
        )

    def create_cybersecurity_compliance_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        sampling_rate: Optional[int] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
    ) -> TestEntity:
        """Create a Cybersec compliance Mitre test on a model

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            sampling_rate (int): The number of times we prompt the model during a test.
            gpu (GPUSpecification): GPU specification
        Returns:
            TestEntity: TestEntity object
        """
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(sampling_rate=sampling_rate),
        }
        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        return Test.create_test_with_grid(
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=None,
            test_type="mitre",
            compute=compute,
        )

    def create_static_jailbreak_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        dataset_id: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create a static jailbreak test on a model

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            gpu (GPUSpecification): GPU specification
            dataset_id (str): Id of the dataset to be used. If not provided,
                the test will default to the v0 dataset, which is a small
                dataset with 50 prompts for testing purposes:
                https://github.com/patrickrchao/JailbreakingLLMs/blob/main/data/harmful_behaviors_custom.csv
                If using a custom dataset, ensure that the dataset has the following columns:
                - "goal": the prompt
                - "category": the category of the prompt
                - "shortened_prompt": the goal column shortened to 1-2 words
                    (used for encoding attack and ascii art attack)
                - "gcg": the prompt that includes the gcg suffix
        Returns:
            TestEntity: TestEntity object
        """
        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        # sampling_rate for each attack to 10.
        # Defaults to False; only use fast mode for internal
        # testing purposes.
        fast_mode = kwargs.get("fast_mode", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(fast_mode=fast_mode),
        }

        return Test.create_test_with_grid(  # pylint: disable=unexpected-keyword-arg
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="static_jailbreak",
            compute=compute,
        )

    def create_bias_toxicity_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        compute: GPUSpecification | CPUSpecification,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create a bias/toxicity test on a model

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            gpu (GPUSpecification): GPU specification
        Returns:
            TestEntity: TestEntity object
        """
        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        # sampling_rate for each attack to 10.
        # Defaults to False; only use fast mode for internal
        # testing purposes.
        fast_mode = kwargs.get("fast_mode", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(fast_mode=fast_mode),
        }

        return Test.create_test_with_grid(  # pylint: disable=unexpected-keyword-arg
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=None,
            test_type="bias_toxicity",
            compute=compute,
        )

    def create_adaptive_jailbreak_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        dataset_id: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create adaptive jailbreak test. Runs Tree of Attacks (TAP) attack.

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            gpu (GPUSpecification): GPU specification
            dataset_id (str): Id of the dataset to be used. If not provided,
                the test will default to the v0 dataset, which is a small
                dataset with 50 prompts for testing purposes:
                https://github.com/patrickrchao/JailbreakingLLMs/blob/main/data/harmful_behaviors_custom.csv
                If using a custom dataset, ensure that the dataset has the following columns:
                - "goal": the prompt
        Returns:
            TestEntity: TestEntity object
        """

        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        #     sampling_rate for each attack to 5 and width / depth = 1.
        #     Defaults to False; only use fast mode for internal
        #     testing purposes.
        fast_mode = kwargs.get("fast_mode", False)
        # perturbation (bool): Whether to use perturbation mode for the attack.
        perturbation = kwargs.get("perturbation", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                fast_mode=fast_mode, perturbation=perturbation
            ),
        }

        return Test.create_test_with_grid(  # pylint: disable=unexpected-keyword-arg
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="adaptive_jailbreak",
            compute=compute,
        )

    def create_policy_jailbreak_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        dataset_id: Optional[str] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create a policy jailbreak test on a model

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            gpu (GPUSpecification): GPU specification
            dataset_id (str): Id of the dataset to be used. If not provided,
                the test will default to the v0 dataset, which is a small
                dataset with 50 prompts for testing purposes:
                https://github.com/patrickrchao/JailbreakingLLMs/blob/main/data/harmful_behaviors_custom.csv
                If using a custom dataset, ensure that the dataset has the following columns:
                - "goal": the prompt
                - "category": the category of the prompt
                - "shortened_prompt": the goal column shortened to 1-2 words
                    (used for encoding attack and ascii art attack)
                - "gcg": the prompt that includes the gcg suffix
        Returns:
            TestEntity: TestEntity object
        """
        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        # sampling_rate for each attack to 10.
        # Defaults to False; only use fast mode for internal
        # testing purposes.
        fast_mode = kwargs.get("fast_mode", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(fast_mode=fast_mode),
        }

        return Test.create_test_with_grid(  # pylint: disable=unexpected-keyword-arg
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="policy_jailbreak",
            compute=compute,
        )

    def create_prompt_extraction_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create adaptive jailbreak test. Runs Tree of Attacks (TAP) attack.

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            gpu (GPUSpecification): GPU specification
        Returns:
            TestEntity: TestEntity object
        """

        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        #     sampling_rate for each attack to 5 and width / depth = 1.
        #     Defaults to False; only use fast mode for internal
        #     testing purposes.
        fast_mode = kwargs.get("fast_mode", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(fast_mode=fast_mode),
        }

        return Test.create_test_with_grid(  # pylint: disable=unexpected-keyword-arg
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=None,
            test_type="prompt_extraction",
            compute=compute,
        )

    def create_multilingual_jailbreak_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        model_key: str,
        language: str,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create a static jailbreak test on a model

        Args:
            name (str): Name of the test
            model_key (str): Key of the model to be tested
            gpu (GPUSpecification): GPU specification
            dataset_id (str): Id of the dataset to be used. If not provided,
                the test will default to the v0 dataset, which is a small
                dataset with 50 prompts for testing purposes:
                https://github.com/patrickrchao/JailbreakingLLMs/blob/main/data/harmful_behaviors_custom.csv
                If using a custom dataset, ensure that the dataset has the following columns:
                - "goal": the prompt
                - "category": the category of the prompt
                - "shortened_prompt": the goal column shortened to 1-2 words
                    (used for encoding attack and ascii art attack)
                - "gcg": the prompt that includes the gcg suffix
        Returns:
            TestEntity: TestEntity object
        """
        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        # sampling_rate for each attack to 10.
        # Defaults to False; only use fast mode for internal
        # testing purposes.
        fast_mode = kwargs.get("fast_mode", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                fast_mode=fast_mode, language=language
            ),
        }

        return Test.create_test_with_grid(  # pylint: disable=unexpected-keyword-arg
            common_attack_config=common_attack_config,
            grid=grid,
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=None,
            test_type="multilingual_jailbreak",
            compute=compute,
        )

    def create_rag_test(
        self,
        name: str,
        model_key: str,
        dataset_id: str,
        compute: GPUSpecification | CPUSpecification,
        prompt_template: str,
        config: list,
        vector_db: Union[ChromaDB, LlamaIndexDB, LlamaIndexWithChromaDB],
        retrieve_top_k: int,
        rag_hallucination_metrics: list[str],
        api_key=None,
    ) -> TestEntity:
        """Create a RAG Hallucination test on a model with a dataset"""

        for c in config:
            for wrapper in ["attack", "dataset", "hyper_parameters"]:
                if wrapper not in c:
                    c[wrapper] = {}
            c["dataset"]["prompt_template"] = prompt_template
            c["attack"]["rag_hallucination_metrics"] = rag_hallucination_metrics
            c["hyper_parameters"]["retrieve_top_k"] = retrieve_top_k
            c["dataset"]["vector_db"] = vector_db.__dict__

        return Test.create_test(
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=dataset_id,
            test_type="rag-hallucination-test",
            compute=compute,
            config=config,
            api_key=api_key,
        )

    def create_system_policy_compliance_test(  # pylint: disable=dangerous-default-value
        self,
        name: str,
        applied_dynamoguard_policies: Optional[List[str]] = None,
        evaluated_dynamoguard_policies: Optional[List[str]] = None,
        dynamoguard_endpoint: Optional[str] = None,
        dynamoguard_api_key: Optional[str] = None,
        model_key: Optional[str] = None,
        compute: Optional[GPUSpecification | CPUSpecification] = None,
        enable_perturbations: bool = True,
        perturbation_methods: List[
            Literal[
                "rewording",
                "common_misspelling",
                "leet_letters",
                "random_upper",
            ]
        ] = [
            "rewording",
            "common_misspelling",
            "leet_letters",
            "random_upper",
        ],
        grid: List[Dict[str, List[Union[str, float, int]]]] = [{}],
        **kwargs
    ) -> TestEntity:
        """Create System Policy Compliance benchmark test. Evaluate compliance of AI system with
        applied and evaluated dynamoguard policies and associated benchmark
        datasets / policy descriptions.
        Args:
            name (str): Name of the test
            applied_dynamoguard_policies (List[str]): List of DynamoGuard policy IDs.
                These guardrail models will be applied and evaluated.
            evaluated_dynamoguard_policies (List[str]): List of DynamoGuard policy IDs.
                These guardrail models will only evaluated.
            dynamoguard_endpoint (str): Endpoint for the DynamoGuard policies. This
                should be the analyze endpoint and end with "v1/moderation/analyze/".
            dynamoguard_api_key (str): API key for the DynamoGuard policies.
            model_key (str): Key of the target model
            gpu (Optional[GPUSpecification]): GPU specification.
                Defaults to None.
            enable_perturbations (bool): Defaulted to True; perturbations
                will run by default.
            perturbation_methods (List[Literal[...]]): If enable_perturbations
                is True, these perturbation_methods will run. By default,
                the full set of perturbations is applied.

        Returns:
            TestEntity: TestEntity object
        """
        # fast_mode (bool): Whether to use fast mode for the attack. Reduces
        #     sampling_rate to 10. Defaults to False; only use fast
        #     mode for internal testing purposes.
        if model_key is None:
            raise ValueError("model_key is required.")
        if applied_dynamoguard_policies is None and evaluated_dynamoguard_policies is None:
            raise ValueError(
                "Either applied_dynamoguard_policies or evaluated_dynamoguard_policies "
                "must be provided for this test."
            )

        if compute is None:
            compute = CPUSpecification(cpu_count=1, memory_count=2)

        fast_mode = kwargs.get("fast_mode", False)
        common_attack_config = {
            "attack": Helpers.construct_dict_filtering_none_values(
                applied_dynamoguard_policies=applied_dynamoguard_policies,
                evaluated_dynamoguard_policies=evaluated_dynamoguard_policies,
                dynamoguard_endpoint=dynamoguard_endpoint,
                dynamoguard_api_key=dynamoguard_api_key,
                fast_mode=fast_mode,
                enable_perturbations=enable_perturbations,
                perturbation_methods=perturbation_methods,
            ),
        }

        return Test.create_test_with_grid(
            request=self._state.request,
            name=name,
            model_key=model_key,
            dataset_id=None,
            test_type="guardrail_benchmark",
            common_attack_config=common_attack_config,
            grid=grid,
            compute=compute,
        )

    def get_use_cases(self):
        self._state.get_use_cases()

    def get_test_info(self, test_id: str):
        return self._state.get_test_info(test_id)

    def get_test_report_url(self, test_id: str):
        return URLUtils.get_test_report_ui_url(self._state.host, test_id)

    def get_attack_info(self, attack_id: str):
        return self._state.get_attack_info(attack_id)

    def get_datasets(self):
        self._state.get_datasets()

    def create_centralized_project(
        self,
        name,
        datasource_key,
        rounds=None,
        use_case_key=None,
        use_case_path=None,
    ):
        self._state.create_centralized_project(
            name,
            datasource_key,
            rounds=rounds,
            use_case_key=use_case_key,
            use_case_path=use_case_path,
        )

    def create_azure_model(
        self,
        name: str,
        api_instance: str,
        api_key: str,
        model_endpoint: str,
        api_version: Optional[str] = None,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates Azure AI (along with Azure OpenAI) model

        Args:
            name (str): Name of the model to be created
            api_instance (str): Azure AI api instance
            api_key (str): Azure AI api key
            api_version (str): Azure AI api version
            model_endpoint (str): Azure AI model endpoint to use
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_azure_model(
            request=self._state.request,
            name=name,
            api_instance=api_instance,
            api_key=api_key,
            api_version=api_version,
            model_endpoint=model_endpoint,
            key=key,
        )

    def create_lambdalabs_model(
        self,
        name: str,
        api_instance: str,
        api_key: str,
        model_endpoint: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates LambdaLabs model

        Args:
            name (str): Name of the model to be created
            api_instance (str): LambdaLabs api instance
            api_key (str): LambdaLabs api key
            api_version (str): LambdaLabs api version
            model_endpoint (str): LambdaLabs model endpoint to use
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_lambdalabs_model(
            request=self._state.request,
            name=name,
            api_instance=api_instance,
            api_key=api_key,
            model_endpoint=model_endpoint,
            key=key,
        )

    def create_openai_model(
        self,
        name: str,
        api_instance: str,
        api_key: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates openai model

        Args:
            name (str): Name of the model to be created
            api_instance (str): OpenAI api instance
            api_key (str): OpenAI api key
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_openai_model(
            request=self._state.request,
            name=name,
            api_instance=api_instance,
            api_key=api_key,
            key=key,
        )

    def create_anthropic_model(
        self,
        name: str,
        api_instance: str,
        api_key: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates Anthropic model

        Args:
            name (str): Name of the model to be created
            api_instance (str): Anthropic api instance
            api_key (str): Anthropic api key
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_anthropic_model(
            request=self._state.request,
            name=name,
            api_instance=api_instance,
            api_key=api_key,
            key=key,
        )

    def create_mistral_model(
        self,
        name: str,
        api_instance: str,
        api_key: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates Mistral model

        Args:
            name (str): Name of the model to be created
            api_instance (str): Mistral api instance
            api_key (str): Mistral api key
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_mistral_model(
            request=self._state.request,
            name=name,
            api_instance=api_instance,
            api_key=api_key,
            key=key,
        )

    def create_togetherai_model(
        self,
        name: str,
        api_instance: str,
        api_key: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates TogetherAI model

        Args:
            name (str): Name of the model to be created
            api_instance (str): TogetherAI api instance
            api_key (str): TogetherAI api key
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_togetherai_model(
            request=self._state.request,
            name=name,
            api_instance=api_instance,
            api_key=api_key,
            key=key,
        )

    def create_databricks_model(
        self,
        name: str,
        api_key: str,
        model_endpoint: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates databricks model

        Args:
            name (str): Name of the model to be created
            api_key (str): Databricks api token
            model_endpoint (str): Databricks model endpoint to use
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_databricks_model(
            request=self._state.request,
            name=name,
            api_key=api_key,
            model_endpoint=model_endpoint,
            key=key,
        )

    def create_custom_model(
        self,
        name: str,
        remote_model_endpoint: str,
        remote_api_auth_config: dict,
        request_transformation_expression: Optional[str] = None,
        response_transformation_expression: Optional[str] = None,
        response_type: Optional[str] = "string",
        batch_size: Optional[int] = 1,
        multi_turn_support: Optional[bool] = True,
        enable_retry: Optional[bool] = False,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates custom model

        Args:
            name (str): Name of the model to be created
            remote_model_endpoint (str): Endpoint for the remote model
            remote_api_auth_config (dict): Authentication configuration for the remote API
            request_transformation_expression (Optional[str]): Expression for request transformation
            response_transformation_expression (Optional[str]): Expression for response transformation
            batch_size (int): Size of the batch for requests
            multi_turn_support (bool): Indicates if the interaction is single-turn
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_custom_model(
            request=self._state.request,
            name=name,
            remote_model_endpoint=remote_model_endpoint,
            remote_api_auth_config=remote_api_auth_config,
            request_transformation_expression=request_transformation_expression,
            response_transformation_expression=response_transformation_expression,
            response_type=response_type,
            batch_size=batch_size,
            multi_turn_support=multi_turn_support,
            enable_retry=enable_retry,
            key=key,
        )

    def create_guardrail_model(
        self,
        name: str,
        api_key: str,
        model_endpoint: str,
        policy_id: str,
        key: Optional[str] = None,
    ) -> RemoteModelEntity:
        """Creates custom model

        Args:
            name (str): Name of the model to be created
            api_key (str): api token
            model_endpoint (str): model endpoint to use
            policy_id (str): policy id to use with the model
            key (str): Unique key for the model

        Returns:
            RemoteModelEntity: RemoteModelEntity object
        """
        return remote_model.RemoteModel.create_guardrail_model(
            request=self._state.request,
            name=name,
            api_key=api_key,
            model_endpoint=model_endpoint,
            policy_id=policy_id,
            key=key,
        )

    def get_model(self, key: str) -> RemoteModelEntity:
        return self._state.get_model(key)

    def create_dataset(
        self,
        file_path,
        key: Optional[str] = None,
        name: Optional[str] = None,
        test_file_path: Optional[str] = None,
    ):
        return Dataset(
            request=self._state.request,
            name=name,
            key=key,
            file_path=file_path,
            test_file_path=test_file_path,
        )

    def create_hf_dataset(
        self,
        name: str,
        hf_id: str,
        hf_token: Optional[str] = None,
        key: Optional[str] = None,
    ) -> HFDatasetEntity:
        """_summary_

        Args:
            name (str): Name of the dataset
            hf_id (str): Dataset id from huggingface
            hf_token (str): Dataset token from huggingface. Please provide the token
                that has access to the dataset
            key (Optional[str]): Unique key for the dataset

        Returns:
            HFDatasetEntity: HFDatasetEntity object
        """
        return HFDataset.create_dataset(
            request=self._state.request, name=name, hf_id=hf_id, hf_token=hf_token, key=key
        )

    def generate_billing_report(
        self,
        from_date: str,
        to_date: str,
    ):
        """Generate billing report

        Args:
            from_date (str): Start date for the report [Inclusive]
                Format: YYYY-MM-DD
            to_date (str): End date for the report [Non-Exclusive]
                Format: YYYY-MM-DD
        """

        billing_api = BillingAPI(self._state.request)
        return billing_api.generate_report(
            params={
                "fromDate": from_date,
                "toDate": to_date,
            }
        )

    def get_billing_reports(
        self,
    ) -> List[BillingReport]:
        """Get Billing Reports"""

        billing_api = BillingAPI(self._state.request)
        return billing_api.get_billing_reports()

    def get_billing_report(self, report_id: int) -> BillingReport:
        """Get Billing Report

        Args:
            report_id (int): Id of the report

        Returns:
            BillingReport: BillingReport object
        """

        billing_api = BillingAPI(self._state.request)
        return billing_api.get_billing_report(report_id)

    def get_billing_report_url(self, report_id: int) -> str:
        """Get billing report url using which we can download the report

        Args:
            report_id (int): Id of the report

        Returns:
            str: Url to download the report
        """

        billing_api = BillingAPI(self._state.request)
        return billing_api.get_billing_report_download_url(report_id)

    def create_custom_rag_application(
        self,
        base_url: str,
        auth_type: AuthTypeEnum,
        auth_config: Optional[Dict[str, Any]] = None,
        custom_rag_application_routes: Optional[List[CustomRagApplicationRoutesEntity]] = None,
    ) -> CustomRagApplicationResponseEntity:
        """
        Creates and registers a new Custom RAG Application.

        This method allows the integration of a customized adapter for the vector database,
        facilitating advanced configurations and specialized functionalities within the RAG ecosystem.

        Args:
            base_url (str): The base URL for the RAG application.
            auth_type (AuthTypeEnum): The authentication type to be employed.
            auth_config (Optional[Dict[str, Any]]): A dictionary containing authentication configuration parameters.
            custom_rag_application_routes (Optional[List[CustomRagApplicationRoutesEntity]]): A list of route entities for the custom RAG application.

        Returns:
            CustomRagApplicationResponseEntity: An entity encompassing the details of the RAG application along with the custom_rag_application_id.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.create(
            base_url=base_url,
            auth_type=auth_type,
            auth_config=auth_config,
            custom_rag_application_routes=custom_rag_application_routes,
        )

    def update_custom_rag_application(
        self,
        custom_rag_application_id: int,
        base_url: str,
        auth_type: AuthTypeEnum,
        auth_config: Optional[Dict[str, Any]] = None,
        custom_rag_application_routes: Optional[List[CustomRagApplicationRoutesEntity]] = None,
    ) -> CustomRagApplicationResponseEntity:
        """
        Updates an existing Custom RAG Application identified by its ID.

        This method allows modifications to the application's base URL, authentication type,
        configuration, and its routes, facilitating dynamic updates in the system.

        Args:
            custom_rag_application_id (int): The unique identifier of the RAG application to update.
            base_url (str): The base URL for the RAG application.
            auth_type (AuthTypeEnum): The type of authentication to be applied.
            auth_config (Optional[Dict[str, Any]]): A dictionary containing authentication configuration parameters.
            custom_rag_application_routes (Optional[List[CustomRagApplicationRoutesEntity]]): A list of route entities for potential update within the application.

        Returns:
            CustomRagApplicationResponseEntity: An entity reflecting the updated state of the specified RAG application along with the custom_rag_application_id.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.update(
            custom_rag_application_id=custom_rag_application_id,
            base_url=base_url,
            auth_type=auth_type,
            auth_config=auth_config,
            custom_rag_application_routes=custom_rag_application_routes,
        )

    def get_all_custom_rag_applications(
        self, include_routes: bool = False
    ) -> AllCustomRagApplicationResponseEntity:
        """
        Retrieves all Custom RAG Applications.

        This method returns a list of all registered RAG applications,
        optionally including their route configurations.

        Args:
            include_routes (bool): Determines whether to include route details for each application.

        Returns:
            AllCustomRagApplicationResponseEntity: An entity containing a list of all RAG applications and their details.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.find_all(include_routes=include_routes)

    def get_custom_rag_application(
        self, custom_rag_application_id: int, include_routes: bool = False
    ) -> List[CustomRagApplicationResponseEntity]:
        """
        Retrieves a specific Custom RAG Application by its ID.

        This method allows for the fetching of details for a particular RAG application,
        with the option to include detailed route information.

        Args:
            custom_rag_application_id (int): The unique identifier of the RAG application to retrieve.
            include_routes (bool): Indicates whether route details should be included in the response.

        Returns:
            List[CustomRagApplicationResponseEntity]: A list containing the details of the requested RAG application.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.find(
            custom_rag_application_id=custom_rag_application_id, include_routes=include_routes
        )

    def delete_custom_rag_application(self, custom_rag_application_id: int) -> None:
        """
        Deletes a specific Custom RAG Application based on its ID.

        This method enables the removal of a RAG application,
        effectively eliminating its configurations and routes from the system.

        Args:
            custom_rag_application_id (int): The unique identifier of the RAG application to delete.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.delete(custom_rag_application_id=custom_rag_application_id)

    def create_custom_rag_application_route(
        self,
        custom_rag_application_id: int,
        route_type: RouteTypeEnum,
        route_path: str,
        request_transformation_expression: Optional[str] = None,
        response_transformation_expression: Optional[str] = None,
    ) -> List[CustomRagApplicationRoutesResponseEntity]:
        """
        Creates a new route for a specified Custom RAG Application.

        This method allows the definition of routes within a RAG application,
        enabling detailed control over request and response transformations.

        Args:
            custom_rag_application_id (int): The ID of the RAG application to which the route belongs.
            route_type (RouteTypeEnum): The type of route to create.
            route_path (str): The URL path defining the route.
            request_transformation_expression (Optional[str]): Expression to transform incoming requests.
            response_transformation_expression (Optional[str]): Expression to transform outgoing responses.

        Returns:
            List[CustomRagApplicationRoutesResponseEntity]: A list of entities representing the newly created routes.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.create_route(
            custom_rag_application_id=custom_rag_application_id,
            route_type=route_type,
            route_path=route_path,
            request_transformation_expression=request_transformation_expression,
            response_transformation_expression=response_transformation_expression,
        )

    def update_custom_rag_application_route(
        self,
        custom_rag_application_id: int,
        route_id: int,
        route_type: RouteTypeEnum,
        route_path: str,
        request_transformation_expression: Optional[str] = None,
        response_transformation_expression: Optional[str] = None,
    ) -> CustomRagApplicationRoutesResponseEntity:
        """
        Updates a route for a specified Custom RAG Application identified by the route ID.

        This method permits alterations to existing routes within a RAG application,
        allowing modifications to route types, paths, and transformation expressions.

        Args:
            custom_rag_application_id (int): The ID of the RAG application to which the route belongs.
            route_id (int): The unique identifier of the route to update.
            route_type (RouteTypeEnum): The type of route to update.
            route_path (str): The URL path defining the route.
            request_transformation_expression (Optional[str]): Expression to transform incoming requests.
            response_transformation_expression (Optional[str]): Expression to transform outgoing responses.

        Returns:
            CustomRagApplicationRoutesResponseEntity: An entity representing the updated route.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.update_route(
            custom_rag_application_id=custom_rag_application_id,
            route_id=route_id,
            route_type=route_type,
            route_path=route_path,
            request_transformation_expression=request_transformation_expression,
            response_transformation_expression=response_transformation_expression,
        )

    def delete_custom_rag_application_route(
        self, custom_rag_application_id: int, route_id: int
    ) -> None:
        """
        Deletes a specific route from a Custom RAG Application.

        This method enables the removal of a specified route,
        ensuring that it is eliminated from the associated RAG application.

        Args:
            custom_rag_application_id (int): The ID of the RAG application to which the route belongs.
            route_id (int): The unique identifier of the route to delete.
        """
        custom_rag_api = CustomRagAPI(self._state.request)
        return custom_rag_api.delete_route(
            custom_rag_application_id=custom_rag_application_id,
            route_id=route_id,
        )
