"""
Pipeline de production avec l'architecture refactoris√©e.
Traite tous les flux configur√©s avec des options flexibles.
"""

import dlt
import yaml
from pathlib import Path
from sources.sftp_enedis import flux_enedis

def run_production_pipeline(
    flux_selection=None,
    max_files=None,
    destination="duckdb",
    dataset_name="flux_enedis"
):
    """
    Lance le pipeline de production avec l'architecture modulaire refactoris√©e.
    
    Args:
        flux_selection: Liste des flux √† traiter (ex: ['R151', 'C15']) ou None pour tous
        max_files: Limitation du nombre de fichiers par resource (pour tests)
        destination: Destination DLT (duckdb, postgres, etc.)
        dataset_name: Nom du dataset de destination
    """
    
    print("="*80)
    print("üöÄ PIPELINE PRODUCTION - ARCHITECTURE REFACTORIS√âE")
    print("="*80)
    
    # Charger la configuration des flux
    config_path = Path("config/flux.yaml")
    if not config_path.exists():
        raise FileNotFoundError("Configuration flux.yaml non trouv√©e")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        all_flux_config = yaml.safe_load(f)
    
    # Filtrer les flux si sp√©cifi√©
    if flux_selection:
        flux_config = {k: v for k, v in all_flux_config.items() if k in flux_selection}
        print(f"üìã Flux s√©lectionn√©s: {list(flux_config.keys())}")
    else:
        flux_config = all_flux_config
        print(f"üìã Tous les flux configur√©s: {list(flux_config.keys())}")
    
    if not flux_config:
        print("‚ùå Aucun flux √† traiter!")
        return
    
    # Calculer le nombre total de tables
    total_tables = 0
    for flux_name, config in flux_config.items():
        if 'xml_configs' in config:
            total_tables += len(config['xml_configs'])
        if 'csv_configs' in config:
            total_tables += len(config['csv_configs'])
    
    print(f"üìä {len(flux_config)} flux ‚Üí {total_tables} tables cibles")
    if max_files:
        print(f"‚ö° Mode test: max {max_files} fichiers par resource")
    
    # Cr√©er le pipeline
    pipeline = dlt.pipeline(
        pipeline_name="flux_enedis_pipeline",
        destination=destination,
        dataset_name=dataset_name
    )
    
    print(f"üéØ Destination: {destination} ‚Üí dataset '{dataset_name}'")
    print()
    
    # Cr√©er la source avec l'architecture modulaire
    print("üèóÔ∏è CR√âATION SOURCE MODULAIRE...")
    source = flux_enedis(flux_config, max_files=max_files)
    print()
    
    # Ex√©cution du pipeline
    print("üöÄ EX√âCUTION PIPELINE...")
    print("-" * 50)
    
    try:
        # Pipeline complet: Extract + Normalize + Load
        load_info = pipeline.run(source)

        print()
        print("="*80)
        print("‚úÖ PIPELINE R√âUSSI!")
        print("="*80)

        # Extraire les statistiques depuis le trace
        trace = pipeline.last_trace
        total_rows = 0

        if trace and trace.last_normalize_info:
            table_metrics = trace.last_normalize_info.row_counts

            print("\nüìä STATISTIQUES DU CHARGEMENT:")
            for table_name, row_count in table_metrics.items():
                if not table_name.startswith("_dlt"):  # Ignorer les tables syst√®me
                    print(f"   - {table_name}: {row_count} nouveaux enregistrements")
                    total_rows += row_count

            if total_rows == 0:
                print("   ‚ö†Ô∏è Aucune nouvelle donn√©e √† traiter")
            else:
                print(f"   ‚úÖ Total: {total_rows} enregistrements ajout√©s")
        else:
            print("\nüìä Pas de m√©triques disponibles dans le trace")

        # Afficher les informations de timing
        print(f"\nüì¶ Load info: {load_info}")

        # R√©sum√© simple
        packages_count = len(load_info.load_packages) if hasattr(load_info, 'load_packages') else 1
        print(f"üìã {packages_count} package(s) trait√©(s) avec succ√®s")

        print()
        print("üéâ ARCHITECTURE REFACTORIS√âE OP√âRATIONNELLE!")
        
    except Exception as e:
        print(f"‚ùå Erreur pipeline: {e}")
        raise

def main():
    """Point d'entr√©e principal avec diff√©rents modes"""
    
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        
        if mode == "test":
            # Mode test rapide - seulement R151 avec 2 fichiers
            print("üß™ MODE TEST RAPIDE")
            run_production_pipeline(
                flux_selection=['R151', 'C15', 'F15', 'R64'],
                max_files=2,
                dataset_name="flux_enedis_test"
            )
            
        elif mode == "r151":
            # Mode R151 complet
            print("üìä MODE R151 COMPLET") 
            run_production_pipeline(
                flux_selection=['R151'],
                dataset_name="flux_enedis_r151"
            )
            
        elif mode == "all":
            # Mode production compl√®te
            print("üåü MODE PRODUCTION COMPL√àTE")
            run_production_pipeline(
                dataset_name="flux_enedis"
            )
            
        else:
            print(f"‚ùå Mode inconnu: {mode}")
            print("Usage: python pipeline_production.py [test|r151|all]")
            sys.exit(1)
    else:
        # Mode par d√©faut - test rapide  
        print("üß™ MODE PAR D√âFAUT: TEST RAPIDE")
        run_production_pipeline(
            flux_selection=['R151'],
            max_files=2,
            dataset_name="flux_enedis_default"
        )

if __name__ == "__main__":
    main()