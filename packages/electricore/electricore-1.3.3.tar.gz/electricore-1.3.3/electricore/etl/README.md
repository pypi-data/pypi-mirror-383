# ETL ElectriCore

Pipeline ETL pour les flux Ã©nergÃ©tiques Enedis avec architecture modulaire DLT.

## ğŸ—ï¸ Architecture RefactorisÃ©e

```
electricore/etl/
â”œâ”€â”€ transformers/            # Transformers DLT modulaires
â”‚   â”œâ”€â”€ crypto.py           # DÃ©chiffrement AES + transformer
â”‚   â”œâ”€â”€ archive.py          # Extraction ZIP + transformer  
â”‚   â””â”€â”€ parsers.py          # Parsing XML/CSV + transformers
â”‚
â”œâ”€â”€ sources/                 # Sources DLT (@dlt.source)
â”‚   â””â”€â”€ sftp_enedis.py      # Source SFTP multi-ressources refactorisÃ©e
â”‚
â”œâ”€â”€ config/                  # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ settings.py         # Chargement YAML et constantes
â”‚   â””â”€â”€ flux.yaml           # Configuration des flux (C15, F12, F15, R15, R151, R64)
â”‚
â”œâ”€â”€ pipeline_production.py   # Pipeline de production avec modes
â””â”€â”€ .dlt/                   # Configuration DLT
    â”œâ”€â”€ config.toml
    â””â”€â”€ secrets.toml
```

## ğŸš€ Utilisation

### Pipeline de Production

```bash
# Test rapide (2 fichiers) - quelques secondes
poetry run python pipeline_production.py test

# R151 complet - environ 6 secondes  
poetry run python pipeline_production.py r151

# Tous les flux - production complÃ¨te
poetry run python pipeline_production.py all

# Mode par dÃ©faut - test rapide
poetry run python pipeline_production.py
```

### DÃ©veloppement et Tests

```bash
# Test avec limitation personnalisÃ©e
from sources.sftp_enedis import sftp_flux_enedis_multi
source = sftp_flux_enedis_multi(flux_config, max_files=5)
```

### Commandes DLT Utiles

```bash
# VÃ©rifier l'Ã©tat du pipeline
poetry run dlt pipeline enedis_data info

# Reset complet si nÃ©cessaire  
poetry run dlt pipeline enedis_data drop --drop-all

# Logs dÃ©taillÃ©s
poetry run dlt pipeline enedis_data trace
```

## ğŸ“Š Flux SupportÃ©s

| Flux | Description | Tables gÃ©nÃ©rÃ©es |
|------|-------------|-----------------|
| **C15** | Changements contractuels | `flux_c15` |
| **F12** | Facturation distributeur | `flux_f12` |
| **F15** | Facturation dÃ©taillÃ©e | `flux_f15_detail` |
| **R15** | RelevÃ©s index | `flux_r15`, `flux_r15_acc` |
| **R151** | RelevÃ©s courbe de charge | `flux_r151` |
| **R64** | RelevÃ©s CSV (Polars) | `flux_r64` |

## ğŸ”§ Configuration

### Secrets DLT (`.dlt/secrets.toml`)
```toml
[sftp]
url = "sftp://user:pass@host/path/"
file_pattern = "**/*.zip"

[aes]
key = "hex_encoded_key"
iv = "hex_encoded_iv"
```

### Configuration des Flux (`config/flux.yaml`)
Chaque flux dÃ©finit :
- `zip_pattern` : Pattern des fichiers ZIP Ã  traiter
- `xml_configs` : Configurations XML avec row_level, data_fields, nested_fields
- `csv_configs` : Configurations CSV avec dÃ©limiteurs et clÃ©s primaires

## ğŸ¯ Avantages de l'Architecture Modulaire

âœ… **Transformers rÃ©utilisables** : crypto | archive | parsers  
âœ… **Tests ultra-rapides** : max_files pour Ã©viter 15min d'attente  
âœ… **Pipeline flexible** : modes test/production/personnalisÃ©  
âœ… **Consolidation DRY** : Suppression duplications lib/transformers  
âœ… **Performance optimisÃ©e** : R151 complet en 6.3 secondes  

## âš¡ Performance

- **Test rapide** : 2 fichiers en ~3 secondes
- **R151 complet** : 108k enregistrements en 6.3 secondes  
- **IncrÃ©mental DLT** : Ã‰vite le retraitement automatiquement

## âš ï¸ Attention : Ã‰tat IncrÃ©mental DLT et Nommage

### ProblÃ¨me cachÃ© de l'Ã©tat incrÃ©mental

DLT maintient un Ã©tat incrÃ©mental basÃ© sur la **combinaison unique** de :
- Nom du pipeline (`pipeline_name`)
- Nom de la source (`@dlt.source(name="...")`)
- Nom du dataset (`dataset_name`)

**âš ï¸ IMPORTANT** : Si vous changez un de ces noms sans nettoyer l'Ã©tat, DLT peut :
- Utiliser l'ancien Ã©tat incrÃ©mental avec le nouveau nom â†’ donnÃ©es manquantes
- CrÃ©er un nouvel Ã©tat â†’ rechargement complet non voulu
- MÃ©langer les Ã©tats â†’ comportements imprÃ©visibles

### OÃ¹ est stockÃ© l'Ã©tat ?

1. **Local** : `~/.dlt/pipelines/{pipeline_name}/state.json`
2. **DuckDB** : `{dataset_name}._dlt_pipeline_state`

### Comment Ã©viter les problÃ¨mes ?

Lors d'un changement de nom :

```bash
# 1. Nettoyer l'Ã©tat local
rm -rf ~/.dlt/pipelines/ancien_nom_pipeline/

# 2. Nettoyer dans DuckDB (optionnel si nouveau dataset)
poetry run python -c "
import duckdb
conn = duckdb.connect('flux_enedis.duckdb')
conn.execute('DROP SCHEMA IF EXISTS ancien_dataset CASCADE')
"

# 3. Relancer avec les nouveaux noms
poetry run python pipeline_production.py all
```

### Exemple de migration propre

Si vous renommez `enedis_production` â†’ `flux_enedis` :

```python
# AVANT
pipeline = dlt.pipeline(
    pipeline_name="flux_enedis",  # âš ï¸ IncohÃ©rent !
    dataset_name="enedis_production"
)
source = sftp_flux_enedis_multi(...)  # Ancien nom

# APRÃˆS (cohÃ©rent)
pipeline = dlt.pipeline(
    pipeline_name="flux_enedis_pipeline",  # Nouveau nom
    dataset_name="flux_enedis"  # Nouveau schema
)
source = flux_enedis(...)  # Source renommÃ©e aussi !
```

**RÃ¨gle d'or** : En cas de doute sur l'Ã©tat incrÃ©mental, utilisez `--replace` ou supprimez `~/.dlt/pipelines/`.

## ğŸ”„ Architecture Modulaire

```python
# Pipeline avec chaÃ®nage de transformers
encrypted_files | decrypt_transformer | unzip_transformer | parse_transformer
```

Chaque transformer est :
- **IsolÃ©** : Testable indÃ©pendamment
- **RÃ©utilisable** : PartagÃ© entre flux
- **Composable** : ChaÃ®nable avec l'opÃ©rateur |

## ğŸ”® Extensions Futures

- `sources/api_enedis.py` : API REST Enedis
- `sources/sftp_axpo.py` : SFTP Axpo
- `transformers/validators.py` : Validation Pandera des donnÃ©es

## ğŸ”Œ IntÃ©grations

### Connecteurs disponibles

Les connecteurs Odoo ont Ã©tÃ© dÃ©placÃ©s vers les modules Core pour une meilleure sÃ©paration des responsabilitÃ©s :

- **Lecture** : `electricore.core.loaders` (OdooReader, OdooQuery)
- **Ã‰criture** : `electricore.core.writers` (OdooWriter)

Voir [docs/odoo-query-builder.md](../../docs/odoo-query-builder.md) pour plus de dÃ©tails.