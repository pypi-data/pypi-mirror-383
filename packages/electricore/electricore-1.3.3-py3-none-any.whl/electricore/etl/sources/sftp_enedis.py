"""
Source DLT pour les flux Enedis via SFTP avec architecture modulaire.
Utilise le cha√Ænage de transformers DLT pour une architecture propre.
"""

import dlt
import re
from typing import Iterator
from dlt.sources.filesystem import filesystem

def mask_password_in_url(url: str) -> str:
    """
    Masque le mot de passe dans une URL SFTP pour les logs.

    Args:
        url: URL SFTP avec mot de passe

    Returns:
        URL avec mot de passe masqu√©

    Example:
        >>> mask_password_in_url("sftp://user:pass@host:22/path")
        "sftp://user:****@host:22/path"
    """
    # Pattern pour capturer: protocol://user:password@host
    pattern = r'(sftp://[^:]+:)[^@]+(@.+)'
    replacement = r'\1****\2'
    return re.sub(pattern, replacement, url)


# Imports des transformers modulaires
from electricore.etl.transformers.crypto import create_decrypt_transformer
from electricore.etl.transformers.archive import create_unzip_transformer
from electricore.etl.transformers.parsers import (
    create_xml_parser_transformer,
    create_csv_parser_transformer,
    create_json_parser_transformer,
    create_json_r64_transformer
)


def create_sftp_resource(flux_type: str, table_name: str, file_pattern: str, sftp_url: str, max_files: int = None):
    """
    Cr√©e une resource SFTP r√©utilisable avec limitation optionnelle.

    Args:
        flux_type: Type de flux (R15, C15, etc.)
        table_name: Nom de la table cible (pour un √©tat incr√©mental unique)
        file_pattern: Pattern pour les fichiers (ZIP ou JSON directs)
        sftp_url: URL du serveur SFTP
        max_files: Nombre max de fichiers √† traiter
    """
    @dlt.resource(
        name=f"sftp_files_{table_name}",  # Nom unique par table pour √©tat incr√©mental ind√©pendant
        write_disposition="append"
    )
    def sftp_files_resource():
        print(f"üîç SFTP {flux_type}: {mask_password_in_url(sftp_url)} avec pattern {file_pattern}")

        files = filesystem(
            bucket_url=sftp_url,
            file_glob=file_pattern
        ).with_name(f"filesystem_{table_name}")  # Nom unique pour √©tat incr√©mental ind√©pendant

        # Appliquer l'incr√©mental sur la date de modification
        files.apply_hints(
            incremental=dlt.sources.incremental("modification_date")
        )

        # Limiter le nombre de fichiers si sp√©cifi√©
        file_count = 0
        for file_item in files:
            if max_files and file_count >= max_files:
                print(f"üîÑ Limitation atteinte: {max_files} fichiers trait√©s")
                break
            file_count += 1
            yield file_item

    return sftp_files_resource


@dlt.source(name="flux_enedis")
def flux_enedis(flux_config: dict, max_files: int = None):
    """
    Source DLT refactor√©e avec architecture modulaire pour tous les flux Enedis.
    
    Architecture unifi√©e :
    - XML: SFTP ‚Üí Decrypt ‚Üí Unzip ‚Üí XML Parse ‚Üí Table
    - CSV: SFTP ‚Üí Decrypt ‚Üí Unzip ‚Üí CSV Parse ‚Üí Table
    
    Args:
        flux_config: Configuration des flux depuis config/settings.py
    """
    # Configuration SFTP depuis secrets
    sftp_config = dlt.secrets['sftp']
    sftp_url = sftp_config['url']
    
    print("=" * 80)
    print("üöÄ ARCHITECTURE REFACTOR√âE - TOUS LES FLUX")
    print("=" * 80)
    print(f"üåê SFTP: {mask_password_in_url(sftp_url)}")
    
    # Cr√©er les transformers communs une seule fois (optimisation)
    decrypt_transformer = create_decrypt_transformer()
    
    # Traiter chaque type de flux
    for flux_type, flux_config_data in flux_config.items():
        file_pattern = flux_config_data['file_pattern']

        print(f"\nüèóÔ∏è  FLUX {flux_type}")
        print(f"   üìÅ Pattern: {file_pattern}")

        # === FLUX XML ===
        if 'xml_configs' in flux_config_data:
            xml_configs = flux_config_data['xml_configs']
            print(f"   üìÑ {len(xml_configs)} config(s) XML")

            for xml_config in xml_configs:
                table_name = xml_config['name']
                file_regex = xml_config.get('file_regex', '*.xml')

                print(f"   üîß Pipeline XML: {table_name}")

                # 1. Resource SFTP
                sftp_resource = create_sftp_resource(flux_type, table_name, file_pattern, sftp_url, max_files)
                
                # 2. Transformer unzip configur√© pour ce flux
                unzip_transformer = create_unzip_transformer('.xml', file_regex)
                
                # 3. Transformer XML parser configur√©
                xml_parser = create_xml_parser_transformer(
                    row_level=xml_config['row_level'],
                    metadata_fields=xml_config.get('metadata_fields', {}),
                    data_fields=xml_config.get('data_fields', {}),
                    nested_fields=xml_config.get('nested_fields', []),
                    flux_type=flux_type
                )
                
                # 4. üéØ CHA√éNAGE MODULAIRE
                xml_pipeline = (
                    sftp_resource |
                    decrypt_transformer |
                    unzip_transformer |
                    xml_parser
                ).with_name(table_name)
                
                # 5. Configuration DLT
                xml_pipeline.apply_hints(write_disposition="append")
                
                print(f"   ‚úÖ {table_name}: SFTP | decrypt | unzip | parse")
                yield xml_pipeline
        
        # === FLUX CSV ===
        if 'csv_configs' in flux_config_data:
            csv_configs = flux_config_data['csv_configs']
            print(f"   üìä {len(csv_configs)} config(s) CSV")
            
            for csv_config in csv_configs:
                table_name = csv_config['name']
                file_regex = csv_config.get('file_regex', '*.csv')
                delimiter = csv_config.get('delimiter', ',')
                encoding = csv_config.get('encoding', 'utf-8')
                primary_key = csv_config.get('primary_key', [])
                
                print(f"   üîß Pipeline CSV: {table_name}")
                
                # 1. Resource SFTP
                sftp_resource = create_sftp_resource(flux_type, table_name, file_pattern, sftp_url, max_files)
                
                # 2. Transformer unzip configur√©
                unzip_transformer = create_unzip_transformer('.csv', file_regex)
                
                # 3. Transformer CSV parser configur√©
                column_mapping = csv_config.get('column_mapping', {})
                
                csv_parser = create_csv_parser_transformer(
                    delimiter=delimiter,
                    encoding=encoding,
                    flux_type=flux_type,
                    column_mapping=column_mapping
                )
                
                # 4. üéØ CHA√éNAGE MODULAIRE
                csv_pipeline = (
                    sftp_resource |
                    decrypt_transformer |
                    unzip_transformer |
                    csv_parser
                ).with_name(table_name)
                
                # 5. Configuration DLT avec d√©duplication si cl√© primaire
                if primary_key:
                    csv_pipeline.apply_hints(
                        primary_key=primary_key,
                        write_disposition="merge"
                    )
                else:
                    csv_pipeline.apply_hints(write_disposition="append")
                
                print(f"   ‚úÖ {table_name}: SFTP | decrypt | unzip | parse")
                if primary_key:
                    print(f"   üîë Cl√© primaire: {primary_key}")
                
                yield csv_pipeline

        # === FLUX JSON ===
        if 'json_configs' in flux_config_data:
            json_configs = flux_config_data['json_configs']
            print(f"   üîß {len(json_configs)} config(s) JSON")

            for json_config in json_configs:
                table_name = json_config['name']
                file_regex = json_config.get('file_regex', '*.json')
                transformer_type = json_config.get('transformer_type', 'standard')
                primary_key = json_config.get('primary_key', [])

                print(f"   üîß Pipeline JSON: {table_name} (type: {transformer_type})")

                # 1. Resource SFTP
                sftp_resource = create_sftp_resource(flux_type, table_name, file_pattern, sftp_url, max_files)

                # 2. Transformer unzip configur√©
                unzip_transformer = create_unzip_transformer('.json', file_regex)

                # 3. Choisir le transformer JSON selon le type
                if transformer_type == 'r64_timeseries':
                    # Transformer R64 sp√©cialis√© pour timeseries en format WIDE
                    json_parser = create_json_r64_transformer(flux_type=flux_type)
                    print(f"   üìä Utilisation transformer R64 sp√©cialis√© (format WIDE)")
                else:
                    # Transformer JSON g√©n√©rique
                    record_path = json_config['record_path']
                    json_parser = create_json_parser_transformer(
                        record_path=record_path,
                        metadata_fields=json_config.get('metadata_fields', {}),
                        data_fields=json_config.get('data_fields', {}),
                        nested_fields=json_config.get('nested_fields', []),
                        flux_type=flux_type
                    )
                    print(f"   üìÑ Utilisation transformer JSON standard")

                # 4. üéØ CHA√éNAGE COMPLET JSON
                json_pipeline = (
                    sftp_resource |
                    decrypt_transformer |
                    unzip_transformer |
                    json_parser
                ).with_name(table_name)

                # 5. Configuration DLT avec d√©duplication si cl√© primaire
                if primary_key:
                    json_pipeline.apply_hints(
                        primary_key=primary_key,
                        write_disposition="merge"
                    )
                else:
                    json_pipeline.apply_hints(write_disposition="append")

                print(f"   ‚úÖ {table_name}: SFTP | decrypt | unzip | parse")
                if primary_key:
                    print(f"   üîë Cl√© primaire: {primary_key}")

                yield json_pipeline

    print("\n" + "=" * 80)
    print("‚úÖ ARCHITECTURE REFACTOR√âE COMPL√àTE")
    print("   üîó Cha√Ænage unifi√© pour tous les flux")
    print("   üß™ Chaque transformer testable isol√©ment")
    print("   üîÑ Transformers r√©utilis√©s entre flux")
    print("   ‚ö° Optimis√©: cl√©s AES charg√©es une seule fois")
    print("=" * 80)


