"""
Transformers DLT pour le parsing XML et CSV des flux Enedis.
Inclut les fonctions pures de parsing et les transformers DLT.
"""

import dlt
import io
import re
import json
import fnmatch
from typing import Iterator, Dict, Any, Optional, List
import polars as pl
from lxml import etree


# =============================================================================
# FONCTIONS PURES DE PARSING XML
# =============================================================================

def match_xml_pattern(xml_name: str, pattern: str | None) -> bool:
    """
    VÃ©rifie si un nom de fichier XML correspond au pattern (wildcard ou regex).
    
    Args:
        xml_name: Nom du fichier XML
        pattern: Pattern wildcard (*,?) ou regex, ou None
    
    Returns:
        bool: True si le fichier match (ou si pas de pattern)
    """
    if pattern is None:
        return True  # Pas de pattern = accepte tout
    
    try:
        # Si le pattern contient des wildcards (* ou ?), utiliser fnmatch
        if '*' in pattern or '?' in pattern:
            return fnmatch.fnmatch(xml_name, pattern)
        else:
            # Sinon, traiter comme regex
            return bool(re.search(pattern, xml_name))
    except re.error:
        # En cas d'erreur regex, essayer comme wildcard en fallback
        try:
            return fnmatch.fnmatch(xml_name, pattern)
        except Exception:
            print(f"âš ï¸ Pattern invalide '{pattern}' pour {xml_name}")
            return False


def xml_to_dict_from_bytes(
    xml_bytes: bytes,
    row_level: str,
    metadata_fields: dict = None,
    data_fields: dict = None,
    nested_fields: list = None
) -> Iterator[dict]:
    """
    Version lxml de xml_to_dict qui parse directement des bytes - SANS Ã©criture disque.
    
    Args:
        xml_bytes: Contenu XML en bytes
        row_level: XPath pour les lignes
        metadata_fields: Champs de mÃ©tadonnÃ©es
        data_fields: Champs de donnÃ©es
        nested_fields: Champs imbriquÃ©s
    
    Yields:
        dict: Enregistrements extraits
    """
    # Initialiser les paramÃ¨tres par dÃ©faut
    metadata_fields = metadata_fields or {}
    data_fields = data_fields or {}
    nested_fields = nested_fields or []
    
    # Parser directement depuis bytes avec lxml - trÃ¨s efficace !
    root = etree.fromstring(xml_bytes)

    # Extraire les mÃ©tadonnÃ©es une seule fois avec XPath
    meta: dict[str, str] = {}
    for field_name, field_xpath in metadata_fields.items():
        elements = root.xpath(field_xpath)
        if elements and hasattr(elements[0], 'text') and elements[0].text:
            meta[field_name] = elements[0].text

    # Parcourir chaque ligne avec XPath (plus puissant qu'ElementTree)
    for row in root.xpath(row_level):
        # Extraire les champs de donnÃ©es principaux avec XPath relatif
        row_data: dict[str, Any] = {}
        
        for field_name, field_xpath in data_fields.items():
            elements = row.xpath(field_xpath)
            if elements and hasattr(elements[0], 'text') and elements[0].text:
                row_data[field_name] = elements[0].text
        
        # Extraire les champs imbriquÃ©s avec conditions (logique identique Ã  xml_to_dict)
        for nested in nested_fields:
            prefix = nested.get('prefix', '')
            child_path = nested['child_path']
            id_field = nested['id_field'] 
            value_field = nested['value_field']
            conditions = nested.get('conditions', [])
            additional_fields = nested.get('additional_fields', {})

            # Parcourir les Ã©lÃ©ments enfants avec XPath
            for nr in row.xpath(child_path):
                # VÃ©rifier toutes les conditions
                all_conditions_met = True
                
                for cond in conditions:
                    cond_xpath = cond['xpath']
                    cond_value = cond['value']
                    cond_elements = nr.xpath(cond_xpath)
                    
                    if not cond_elements or not hasattr(cond_elements[0], 'text') or cond_elements[0].text != cond_value:
                        all_conditions_met = False
                        break
                
                # Si toutes les conditions sont remplies
                if all_conditions_met:
                    key_elements = nr.xpath(id_field)
                    value_elements = nr.xpath(value_field)
                    
                    if (key_elements and value_elements and
                        hasattr(key_elements[0], 'text') and hasattr(value_elements[0], 'text') and
                        key_elements[0].text and value_elements[0].text):

                        # Ajouter la valeur principale avec prÃ©fixe et convention de nommage
                        # Format: {prefix}index_{cadran}_kwh (ex: "avant_index_hp_kwh", "index_base_kwh")
                        cadran = key_elements[0].text.lower()  # HP â†’ hp, BASE â†’ base
                        field_key = f"{prefix}index_{cadran}_kwh"
                        row_data[field_key] = value_elements[0].text
                        
                        # Ajouter les champs additionnels
                        for add_field_name, add_field_xpath in additional_fields.items():
                            add_field_key = f"{prefix}{add_field_name}"
                            
                            # Ã‰viter d'Ã©craser si dÃ©jÃ  prÃ©sent
                            if add_field_key not in row_data:
                                add_elements = nr.xpath(add_field_xpath)
                                if (add_elements and hasattr(add_elements[0], 'text') and 
                                    add_elements[0].text):
                                    row_data[add_field_key] = add_elements[0].text
        
        # Fusionner mÃ©tadonnÃ©es et donnÃ©es de ligne
        final_record = {**row_data, **meta}
        
        yield final_record


# =============================================================================
# TRANSFORMERS DLT
# =============================================================================


def _xml_parser_transformer_base(
    extracted_file: dict,
    row_level: str,
    metadata_fields: Dict[str, str],
    data_fields: Dict[str, str],
    nested_fields: List[Dict[str, Any]],
    flux_type: str
) -> Iterator[dict]:
    """
    Fonction de base pour parser les fichiers XML extraits.
    
    Args:
        extracted_file: Fichier extrait du transformer archive
        row_level: Niveau XPath pour les enregistrements
        metadata_fields: Champs de mÃ©tadonnÃ©es XML
        data_fields: Champs de donnÃ©es XML
        nested_fields: Configuration des champs imbriquÃ©s
        flux_type: Type de flux pour traÃ§abilitÃ©
    
    Yields:
        dict: Enregistrements parsÃ©s avec mÃ©tadonnÃ©es de traÃ§abilitÃ©
    """
    
    zip_name = extracted_file['source_zip']
    zip_modified = extracted_file['modification_date']
    xml_name = extracted_file['extracted_file_name']
    xml_content = extracted_file['extracted_content']
    
    try:
        # print(f"ðŸ” Parsing XML: {xml_name}")
        
        # Parser le XML avec la configuration
        records_count = 0
        for record in xml_to_dict_from_bytes(
            xml_content,
            row_level=row_level,
            metadata_fields=metadata_fields,
            data_fields=data_fields,
            nested_fields=nested_fields
        ):
            # Enrichir avec mÃ©tadonnÃ©es de traÃ§abilitÃ©
            enriched_record = record.copy()
            enriched_record.update({
                '_source_zip': zip_name,
                '_flux_type': flux_type,
                '_xml_name': xml_name,
                'modification_date': zip_modified
            })
            
            yield enriched_record
            records_count += 1
        
        # print(f"âœ… ParsÃ©: {records_count} enregistrements depuis {xml_name}")
        
    except Exception as e:
        print(f"âŒ Erreur parsing XML {xml_name}: {e}")
        return


def _csv_parser_transformer_base(
    extracted_file: dict,
    delimiter: str,
    encoding: str,
    flux_type: str,
    column_mapping: Dict[str, str]
) -> Iterator[dict]:
    """
    Fonction de base pour parser les fichiers CSV avec Polars.
    
    Args:
        extracted_file: Fichier extrait du transformer archive
        delimiter: DÃ©limiteur CSV
        encoding: Encodage du fichier
        flux_type: Type de flux pour traÃ§abilitÃ©
        column_mapping: Mapping des colonnes (franÃ§ais â†’ snake_case)
    
    Yields:
        dict: Lignes CSV parsÃ©es avec mÃ©tadonnÃ©es
    """
    
    zip_name = extracted_file['source_zip']
    zip_modified = extracted_file['modification_date']
    csv_name = extracted_file['extracted_file_name']
    csv_content = extracted_file['extracted_content']
    
    try:
        print(f"ðŸ“Š Parsing CSV: {csv_name}")
        
        # DÃ©coder le contenu CSV
        csv_text = csv_content.decode(encoding)
        
        # Parser avec Polars (plus performant)
        df = pl.read_csv(
            io.StringIO(csv_text),
            separator=delimiter,
            encoding=encoding,
            null_values=['null', '', 'NULL', 'None'],
            ignore_errors=True,
            infer_schema_length=10000
        )
        
        # Appliquer le mapping des colonnes si fourni
        if column_mapping:
            df = df.rename(column_mapping)
        
        # Ajouter mÃ©tadonnÃ©es de traÃ§abilitÃ©
        df_with_meta = df.with_columns([
            pl.lit(zip_modified).alias('modification_date'),
            pl.lit(zip_name).alias('_source_zip'),
            pl.lit(flux_type).alias('_flux_type'),
            pl.lit(csv_name).alias('_csv_name')
        ])
        
        # Yield chaque ligne comme dictionnaire
        records_count = 0
        for row_dict in df_with_meta.to_dicts():
            yield row_dict
            records_count += 1
        
        print(f"âœ… ParsÃ©: {records_count} lignes depuis {csv_name}")
        
    except Exception as e:
        print(f"âŒ Erreur parsing CSV {csv_name}: {e}")
        return


def create_xml_parser_transformer(
    row_level: str,
    metadata_fields: Dict[str, str] = None,
    data_fields: Dict[str, str] = None,
    nested_fields: List[Dict[str, Any]] = None,
    flux_type: str = "unknown"
):
    """
    Factory pour crÃ©er un transformer de parsing XML configurÃ©.
    
    Args:
        row_level: Niveau XPath pour les enregistrements
        metadata_fields: Champs de mÃ©tadonnÃ©es XML
        data_fields: Champs de donnÃ©es XML
        nested_fields: Configuration des champs imbriquÃ©s
        flux_type: Type de flux
    
    Returns:
        Transformer DLT configurÃ©
    """
    @dlt.transformer
    def configured_xml_parser(extracted_file: dict) -> Iterator[dict]:
        return _xml_parser_transformer_base(
            extracted_file, row_level, metadata_fields or {}, 
            data_fields or {}, nested_fields or [], flux_type
        )
    
    return configured_xml_parser


def create_csv_parser_transformer(
    delimiter: str = ',',
    encoding: str = 'utf-8',
    flux_type: str = "unknown",
    column_mapping: Dict[str, str] = None
):
    """
    Factory pour crÃ©er un transformer de parsing CSV configurÃ©.
    
    Args:
        delimiter: DÃ©limiteur CSV
        encoding: Encodage du fichier
        flux_type: Type de flux
        column_mapping: Mapping des colonnes
    
    Returns:
        Transformer DLT configurÃ©
    """
    @dlt.transformer
    def configured_csv_parser(extracted_file: dict) -> Iterator[dict]:
        return _csv_parser_transformer_base(
            extracted_file, delimiter, encoding, flux_type, column_mapping or {}
        )
    
    return configured_csv_parser

# =============================================================================
# TRANSFORMER JSON DLT
# =============================================================================

def _json_parser_transformer_base(
    extracted_file: dict,
    record_path: str,
    metadata_fields: Dict[str, str],
    data_fields: Dict[str, str],
    nested_fields: List[Dict[str, Any]],
    flux_type: str
) -> Iterator[dict]:
    """
    Fonction de base pour parser les fichiers JSON.
    ParallÃ¨le Ã  _xml_parser_transformer_base.

    Args:
        extracted_file: Fichier extrait du transformer archive
        record_path: Chemin vers les enregistrements
        metadata_fields: Champs de mÃ©tadonnÃ©es
        data_fields: Champs de donnÃ©es principaux
        nested_fields: Configuration des champs imbriquÃ©s
        flux_type: Type de flux pour traÃ§abilitÃ©

    Yields:
        dict: Enregistrements parsÃ©s avec mÃ©tadonnÃ©es de traÃ§abilitÃ©
    """
    # GÃ©rer les deux cas : JSON extrait de ZIP ou JSON direct du SFTP
    if 'source_zip' in extracted_file:
        # Cas 1: JSON extrait d'un ZIP
        zip_name = extracted_file['source_zip']
        zip_modified = extracted_file['modification_date']
        json_name = extracted_file['extracted_file_name']
        json_content = extracted_file['extracted_content']
    else:
        # Cas 2: JSON dÃ©cryptÃ© du SFTP (R64)
        zip_name = extracted_file['file_name']  # Nom du fichier JSON
        zip_modified = extracted_file['modification_date']
        json_name = extracted_file['file_name']
        json_content = extracted_file['decrypted_content']

    try:
        print(f"ðŸ“„ Parsing JSON: {json_name}")

        records_count = 0

        # Parser le JSON et extraire les enregistrements
        for record in json_to_dict_from_bytes(
            json_content, record_path, metadata_fields, data_fields, nested_fields
        ):
            # Enrichir avec mÃ©tadonnÃ©es de traÃ§abilitÃ© DLT
            enriched_record = {
                **record,
                'modification_date': zip_modified,
                '_source_zip': zip_name,
                '_flux_type': flux_type,
                '_json_name': json_name
            }

            yield enriched_record
            records_count += 1

        print(f"âœ… ParsÃ©: {records_count} enregistrements depuis {json_name}")

    except Exception as e:
        print(f"âŒ Erreur parsing JSON {json_name}: {e}")
        return


def create_json_parser_transformer(
    record_path: str,
    metadata_fields: Dict[str, str] = None,
    data_fields: Dict[str, str] = None,
    nested_fields: List[Dict[str, Any]] = None,
    flux_type: str = "unknown"
):
    """
    Factory pour crÃ©er un transformer de parsing JSON configurÃ©.
    ParallÃ¨le Ã  create_xml_parser_transformer.

    Args:
        record_path: Chemin vers les enregistrements dans le JSON
        metadata_fields: Champs de mÃ©tadonnÃ©es JSON
        data_fields: Champs de donnÃ©es JSON
        nested_fields: Configuration des champs imbriquÃ©s
        flux_type: Type de flux

    Returns:
        Transformer DLT configurÃ©
    """
    @dlt.transformer
    def configured_json_parser(extracted_file: dict) -> Iterator[dict]:
        return _json_parser_transformer_base(
            extracted_file, record_path, metadata_fields or {},
            data_fields or {}, nested_fields or [], flux_type
        )

    return configured_json_parser


# =============================================================================
# TRANSFORMER JSON R64 SPÃ‰CIALISÃ‰ - FORMAT WIDE
# =============================================================================

def extract_header_metadata(data: dict) -> dict:
    """
    Extrait les mÃ©tadonnÃ©es du header JSON R64.

    Args:
        data: DonnÃ©es JSON R64 complÃ¨tes

    Returns:
        dict: MÃ©tadonnÃ©es du header
    """
    header = data.get('header', {})
    return {
        'id_demande': header.get('idDemande'),
        'si_demandeur': header.get('siDemandeur'),
        'code_flux': header.get('codeFlux'),
        'format': header.get('format')
    }


def is_valid_calendrier(calendrier: dict) -> bool:
    """
    VÃ©rifie si le calendrier est un calendrier distributeur valide.

    Args:
        calendrier: DonnÃ©es du calendrier

    Returns:
        bool: True si calendrier distributeur valide
    """
    id_calendrier = calendrier.get('idCalendrier')
    libelle_calendrier = calendrier.get('libelleCalendrier', '').lower()

    # Filtrer par ID calendrier ou par libellÃ© distributeur
    valid_ids = {'DI000001', 'DI000002', 'DI000003'}
    return (id_calendrier in valid_ids or 'distributeur' in libelle_calendrier)


def is_valid_data_point(point: dict) -> bool:
    """
    VÃ©rifie si un point de donnÃ©es est valide (iv == 0).

    Args:
        point: Point de donnÃ©es avec 'd', 'v', 'iv'

    Returns:
        bool: True si point valide
    """
    return (
        point.get('d') and
        point.get('v') is not None and
        point.get('iv') == 0
    )


def should_process_grandeur(grandeur: dict) -> bool:
    """
    VÃ©rifie si une grandeur doit Ãªtre traitÃ©e (CONS + EA).

    Args:
        grandeur: DonnÃ©es de grandeur

    Returns:
        bool: True si grandeur Ã  traiter
    """
    return (
        grandeur.get('grandeurMetier') == 'CONS' and
        grandeur.get('grandeurPhysique') == 'EA'
    )


def build_base_record(mesure: dict, contexte: dict, grandeur: dict, header_meta: dict) -> dict:
    """
    Construit l'enregistrement de base avec toutes les mÃ©tadonnÃ©es.

    Args:
        mesure: DonnÃ©es de mesure (PDL)
        contexte: DonnÃ©es de contexte
        grandeur: DonnÃ©es de grandeur
        header_meta: MÃ©tadonnÃ©es du header

    Returns:
        dict: Enregistrement de base avec mÃ©tadonnÃ©es
    """
    periode = mesure.get('periode', {})

    return {
        # MÃ©tadonnÃ©es de base
        'pdl': mesure.get('idPrm'),

        # MÃ©tadonnÃ©es contexte
        'etape_metier': contexte.get('etapeMetier'),
        'contexte_releve': contexte.get('contexteReleve'),
        'type_releve': contexte.get('typeReleve'),

        # MÃ©tadonnÃ©es grandeur
        'grandeur_physique': grandeur.get('grandeurPhysique'),
        'grandeur_metier': grandeur.get('grandeurMetier'),
        'unite': grandeur.get('unite'),

        # MÃ©tadonnÃ©es header
        **header_meta
    }


def collect_timeseries_data(mesure: dict, base_record: dict) -> dict:
    """
    Collecte toutes les donnÃ©es de timeseries d'une mesure en format wide.

    Args:
        mesure: DonnÃ©es de mesure complÃ¨te
        base_record: Enregistrement de base avec mÃ©tadonnÃ©es

    Returns:
        dict: DonnÃ©es par date avec colonnes de cadrans
    """
    values_by_date = {}

    for contexte in mesure.get('contexte', []):
        for grandeur in contexte.get('grandeur', []):
            if not should_process_grandeur(grandeur):
                continue

            for calendrier in grandeur.get('calendrier', []):
                if not is_valid_calendrier(calendrier):
                    continue

                # Traiter les classes temporelles
                for classe in calendrier.get('classeTemporelle', []):
                    id_classe = classe.get('idClasseTemporelle')
                    if not id_classe:
                        continue

                    # Convention de nommage: index_{cadran}_kwh
                    cadran = id_classe.lower()
                    col_name = f"index_{cadran}_kwh"

                    # Traiter chaque point de donnÃ©es
                    for point in classe.get('valeur', []):
                        if not is_valid_data_point(point):
                            continue

                        date_str = point.get('d')
                        valeur = point.get('v')

                        # Initialiser l'enregistrement pour cette date
                        if date_str not in values_by_date:
                            values_by_date[date_str] = {
                                **base_record,
                                'date_releve': date_str
                            }

                        # Ajouter la valeur pour cette classe
                        values_by_date[date_str][col_name] = valeur

    return values_by_date


def process_single_mesure(mesure: dict, header_meta: dict) -> Iterator[dict]:
    """
    Traite une mesure unique et gÃ©nÃ¨re les enregistrements wide.

    Args:
        mesure: DonnÃ©es d'une mesure (PDL)
        header_meta: MÃ©tadonnÃ©es du header

    Yields:
        dict: Enregistrements wide par date
    """
    pdl = mesure.get('idPrm')
    print(f"ðŸ“Š Traitement PDL {pdl} - {len(mesure.get('contexte', []))} contexte(s)")

    # Pour trouver le premier contexte/grandeur valide pour les mÃ©tadonnÃ©es
    base_record = None

    for contexte in mesure.get('contexte', []):
        for grandeur in contexte.get('grandeur', []):
            if should_process_grandeur(grandeur):
                base_record = build_base_record(mesure, contexte, grandeur, header_meta)
                break
        if base_record:
            break

    if not base_record:
        print(f"âš ï¸ Aucune grandeur CONS/EA trouvÃ©e pour PDL {pdl}")
        return

    # Collecter toutes les donnÃ©es timeseries
    values_by_date = collect_timeseries_data(mesure, base_record)

    # GÃ©nÃ©rer les enregistrements
    for date_str, row_data in values_by_date.items():
        yield row_data


def r64_timeseries_to_wide_format(
    json_bytes: bytes,
    flux_type: str
) -> Iterator[Dict[str, Any]]:
    """
    Transforme les timeseries R64 en format WIDE pour cohÃ©rence avec R15/R151.

    Chaque enregistrement de sortie = 1 PDL + 1 date + tous les cadrans.

    Args:
        json_bytes: Contenu JSON R64 en bytes
        flux_type: Type de flux pour traÃ§abilitÃ©

    Yields:
        dict: Enregistrements en format wide par date
    """
    try:
        # Parser JSON R64
        data = json.loads(json_bytes.decode('utf-8'))

        # Extraire mÃ©tadonnÃ©es du header
        header_metadata = extract_header_metadata(data)

        # Traiter chaque mesure (PDL)
        total_records = 0
        for mesure in data.get('mesures', []):
            for record in process_single_mesure(mesure, header_metadata):
                yield record
                total_records += 1

        print(f"âœ… R64 transformÃ© en format WIDE: {total_records} enregistrements")

    except json.JSONDecodeError as e:
        print(f"âŒ Erreur parsing JSON R64: {e}")
        return
    except Exception as e:
        print(f"âŒ Erreur transformation R64: {e}")
        return


def _json_r64_transformer_base(
    extracted_file: dict,
    flux_type: str
) -> Iterator[dict]:
    """
    Transformer de base pour R64 avec gestion des timeseries en format WIDE.

    Args:
        extracted_file: Fichier extrait du transformer archive
        flux_type: Type de flux pour traÃ§abilitÃ©

    Yields:
        dict: Enregistrements R64 en format wide avec mÃ©tadonnÃ©es DLT
    """
    # GÃ©rer les deux cas : JSON extrait de ZIP ou JSON direct du SFTP
    if 'source_zip' in extracted_file:
        # Cas 1: JSON extrait d'un ZIP
        zip_name = extracted_file['source_zip']
        zip_modified = extracted_file['modification_date']
        json_name = extracted_file['extracted_file_name']
        json_content = extracted_file['extracted_content']
    else:
        # Cas 2: JSON dÃ©cryptÃ© du SFTP
        zip_name = extracted_file['file_name']
        zip_modified = extracted_file['modification_date']
        json_name = extracted_file['file_name']
        json_content = extracted_file['decrypted_content']

    try:
        print(f"ðŸ“„ Parsing JSON R64: {json_name}")

        records_count = 0

        # Transformer les timeseries R64 en format WIDE
        for record in r64_timeseries_to_wide_format(json_content, flux_type):
            # Enrichir avec mÃ©tadonnÃ©es de traÃ§abilitÃ© DLT
            enriched_record = {
                **record,
                'modification_date': zip_modified,
                '_source_zip': zip_name,
                '_flux_type': flux_type,
                '_json_name': json_name
            }

            yield enriched_record
            records_count += 1

        print(f"âœ… R64 parsÃ©: {records_count} enregistrements WIDE depuis {json_name}")

    except Exception as e:
        print(f"âŒ Erreur parsing JSON R64 {json_name}: {e}")
        return


def create_json_r64_transformer(flux_type: str = "R64"):
    """
    Factory pour crÃ©er un transformer R64 spÃ©cialisÃ© avec format WIDE.

    Args:
        flux_type: Type de flux (R64)

    Returns:
        Transformer DLT configurÃ© pour R64
    """
    @dlt.transformer
    def configured_r64_parser(extracted_file: dict) -> Iterator[dict]:
        return _json_r64_transformer_base(extracted_file, flux_type)

    return configured_r64_parser


