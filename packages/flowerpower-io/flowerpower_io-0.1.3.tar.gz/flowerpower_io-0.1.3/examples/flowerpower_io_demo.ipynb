{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlowerPower IO Library Demo\n",
    "\n",
    "This notebook demonstrates the key features of the FlowerPower IO library, including:\n",
    "\n",
    "1. **Reading CSV files** using `CSVFileReader`\n",
    "2. **Converting data** to different formats (Pandas DataFrame, Polars DataFrame, PyArrow Table)\n",
    "3. **Writing to Parquet** using `ParquetFileWriter`\n",
    "4. **Reading from SQLite database** using `SQLiteReader`\n",
    "5. **Writing to SQLite database** using `SQLiteWriter`\n",
    "\n",
    "Let's start by importing the necessary classes and creating some sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import FlowerPower IO classes\n",
    "from flowerpower_io.loader.csv import CSVFileReader\n",
    "from flowerpower_io.saver.parquet import ParquetFileWriter\n",
    "from flowerpower_io.loader.sqlite import SQLiteReader\n",
    "from flowerpower_io.saver.sqlite import SQLiteWriter\n",
    "\n",
    "print(\"Successfully imported all required classes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Sample Data\n",
    "\n",
    "First, let's create a sample CSV file that we'll use throughout this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sample_data = {\n",
    "    'id': range(1, 101),\n",
    "    'name': [f'Person_{i}' for i in range(1, 101)],\n",
    "    'age': [20 + (i % 50) for i in range(1, 101)],\n",
    "    'city': ['New York', 'London', 'Tokyo', 'Paris', 'Berlin'] * 20,\n",
    "    'salary': [50000 + (i * 1000) for i in range(1, 101)]\n",
    "}\n",
    "\n",
    "# Create a temporary directory for our demo files\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "csv_path = os.path.join(temp_dir, 'sample_data.csv')\n",
    "parquet_path = os.path.join(temp_dir, 'sample_data.parquet')\n",
    "db_path = os.path.join(temp_dir, 'sample_data.db')\n",
    "\n",
    "# Create CSV file using pandas\n",
    "df_pandas = pd.DataFrame(sample_data)\n",
    "df_pandas.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Created sample CSV file at: {csv_path}\")\n",
    "print(f\"Sample data shape: {df_pandas.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading CSV Files with CSVFileReader\n",
    "\n",
    "Now let's demonstrate how to read the CSV file using `CSVFileReader` and convert it to different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CSVFileReader\n",
    "csv_reader = CSVFileReader(path=csv_path)\n",
    "\n",
    "print(\"CSVFileReader initialized successfully!\")\n",
    "print(f\"File path: {csv_reader.path}\")\n",
    "print(f\"Format: {csv_reader.format}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting to Different Data Formats\n",
    "\n",
    "The `CSVFileReader` can convert data to multiple formats including Pandas DataFrame, Polars DataFrame, and PyArrow Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "print(\"=== Converting to Pandas DataFrame ===\")\n",
    "df_pandas_converted = csv_reader.to_pandas()\n",
    "print(f\"Pandas DataFrame shape: {df_pandas_converted.shape}\")\n",
    "print(f\"Data types:\\n{df_pandas_converted.dtypes}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_pandas_converted.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Polars DataFrame\n",
    "print(\"\\n=== Converting to Polars DataFrame ===\")\n",
    "df_polars = csv_reader.to_polars()\n",
    "print(f\"Polars DataFrame shape: {df_polars.shape}\")\n",
    "print(f\"Schema: {df_polars.schema}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df_polars.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyArrow Table\n",
    "print(\"\\n=== Converting to PyArrow Table ===\")\n",
    "arrow_table = csv_reader.to_pyarrow_table()\n",
    "print(f\"PyArrow Table shape: {arrow_table.shape}\")\n",
    "print(f\"Schema: {arrow_table.schema}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(arrow_table.slice(0, 3).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Writing to Parquet with ParquetFileWriter\n",
    "\n",
    "Now let's demonstrate how to write data to a Parquet file using `ParquetFileWriter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ParquetFileWriter\n",
    "parquet_writer = ParquetFileWriter(path=parquet_path)\n",
    "\n",
    "print(\"ParquetFileWriter initialized successfully!\")\n",
    "print(f\"Output path: {parquet_writer.path}\")\n",
    "print(f\"Format: {parquet_writer.format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to Parquet file using Pandas DataFrame\n",
    "print(\"=== Writing Pandas DataFrame to Parquet ===\")\n",
    "metadata = parquet_writer.write(df_pandas_converted)\n",
    "print(f\"Write operation completed!\")\n",
    "print(f\"Metadata: {metadata}\")\n",
    "\n",
    "# Check if file was created\n",
    "print(f\"\\nParquet file exists: {os.path.exists(parquet_path)}\")\n",
    "print(f\"File size: {os.path.getsize(parquet_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also try writing with Polars DataFrame\n",
    "print(\"\\n=== Writing Polars DataFrame to Parquet ===\")\n",
    "parquet_path_polars = os.path.join(temp_dir, 'sample_data_polars.parquet')\n",
    "parquet_writer_polars = ParquetFileWriter(path=parquet_path_polars)\n",
    "\n",
    "metadata_polars = parquet_writer_polars.write(df_polars)\n",
    "print(f\"Write operation completed!\")\n",
    "print(f\"Metadata: {metadata_polars}\")\n",
    "print(f\"File size: {os.path.getsize(parquet_path_polars)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reading from SQLite Database with SQLiteReader\n",
    "\n",
    "Now let's demonstrate how to write data to a SQLite database and then read it back using `SQLiteWriter` and `SQLiteReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's write data to SQLite database using SQLiteWriter\n",
    "print(\"=== Writing to SQLite Database ===\")\n",
    "sqlite_writer = SQLiteWriter(\n",
    "    table_name=\"employees\",\n",
    "    path=db_path\n",
    ")\n",
    "\n",
    "print(\"SQLiteWriter initialized successfully!\")\n",
    "print(f\"Database path: {sqlite_writer.path}\")\n",
    "print(f\"Table name: {sqlite_writer.table_name}\")\n",
    "print(f\"Type: {sqlite_writer.type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to SQLite\n",
    "write_metadata = sqlite_writer.write(df_pandas_converted)\n",
    "print(f\"Data written to SQLite successfully!\")\n",
    "print(f\"Write metadata: {write_metadata}\")\n",
    "\n",
    "# Verify database file was created\n",
    "print(f\"\\nDatabase file exists: {os.path.exists(db_path)}\")\n",
    "print(f\"Database file size: {os.path.getsize(db_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's read the data back using SQLiteReader\n",
    "print(\"\\n=== Reading from SQLite Database ===\")\n",
    "sqlite_reader = SQLiteReader(\n",
    "    table_name=\"employees\",\n",
    "    path=db_path\n",
    ")\n",
    "\n",
    "print(\"SQLiteReader initialized successfully!\")\n",
    "print(f\"Database path: {sqlite_reader.path}\")\n",
    "print(f\"Table name: {sqlite_reader.table_name}\")\n",
    "print(f\"Type: {sqlite_reader.type_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data as Pandas DataFrame\n",
    "print(\"=== Reading as Pandas DataFrame ===\")\n",
    "df_from_sqlite_pandas = sqlite_reader.to_pandas()\n",
    "print(f\"Data shape: {df_from_sqlite_pandas.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_from_sqlite_pandas.head())\n",
    "\n",
    "# Verify data integrity\n",
    "print(f\"\\nData integrity check - Original vs SQLite:\")\n",
    "print(f\"Original shape: {df_pandas_converted.shape}\")\n",
    "print(f\"SQLite shape: {df_from_sqlite_pandas.shape}\")\n",
    "print(f\"Data matches: {df_pandas_converted.equals(df_from_sqlite_pandas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data as Polars DataFrame\n",
    "print(\"\\n=== Reading as Polars DataFrame ===\")\n",
    "df_from_sqlite_polars = sqlite_reader.to_polars()\n",
    "print(f\"Data shape: {df_from_sqlite_polars.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_from_sqlite_polars.head())\n",
    "\n",
    "# Read data as PyArrow Table\n",
    "print(\"\\n=== Reading as PyArrow Table ===\")\n",
    "arrow_from_sqlite = sqlite_reader.to_pyarrow_table()\n",
    "print(f\"Data shape: {arrow_from_sqlite.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(arrow_from_sqlite.slice(0, 5).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Querying with SQLiteReader\n",
    "\n",
    "Let's demonstrate how to use custom SQL queries with the SQLiteReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for employees older than 50\n",
    "print(\"=== Custom SQL Query: Employees older than 50 ===\")\n",
    "query = \"SELECT * FROM employees WHERE age > 50\"\n",
    "df_older_employees = sqlite_reader.to_pandas(query=query)\n",
    "print(f\"Number of employees older than 50: {len(df_older_employees)}\")\n",
    "print(\"\\nEmployees older than 50:\")\n",
    "print(df_older_employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for average salary by city\n",
    "print(\"\\n=== Custom SQL Query: Average salary by city ===\")\n",
    "query = \"SELECT city, AVG(salary) as avg_salary, COUNT(*) as count FROM employees GROUP BY city ORDER BY avg_salary DESC\"\n",
    "df_salary_by_city = sqlite_reader.to_pandas(query=query)\n",
    "print(\"Average salary by city:\")\n",
    "print(df_salary_by_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Metadata and Performance Information\n",
    "\n",
    "Let's explore the metadata functionality and get some performance insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata from CSV reader\n",
    "print(\"=== CSV Reader Metadata ===\")\n",
    "df_pandas_with_metadata, csv_metadata = csv_reader.to_pandas(metadata=True)\n",
    "print(f\"CSV Metadata: {csv_metadata}\")\n",
    "\n",
    "# Get metadata from SQLite reader\n",
    "print(\"\\n=== SQLite Reader Metadata ===\")\n",
    "df_sqlite_with_metadata, sqlite_metadata = sqlite_reader.to_pandas(metadata=True)\n",
    "print(f\"SQLite Metadata: {sqlite_metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes\n",
    "print(\"\\n=== File Size Comparison ===\")\n",
    "csv_size = os.path.getsize(csv_path)\n",
    "parquet_size = os.path.getsize(parquet_path)\n",
    "db_size = os.path.getsize(db_path)\n",
    "\n",
    "print(f\"CSV file size: {csv_size:,} bytes\")\n",
    "print(f\"Parquet file size: {parquet_size:,} bytes\")\n",
    "print(f\"SQLite database size: {db_size:,} bytes\")\n",
    "print(f\"\\nCompression ratios:\")\n",
    "print(f\"Parquet vs CSV: {csv_size/parquet_size:.2f}x smaller\")\n",
    "print(f\"SQLite vs CSV: {csv_size/db_size:.2f}x smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Let's clean up the temporary files we created during this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "print(\"=== Cleaning up temporary files ===\")\n",
    "files_to_remove = [csv_path, parquet_path, parquet_path_polars, db_path]\n",
    "\n",
    "for file_path in files_to_remove:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Removed: {file_path}\")\n",
    "\n",
    "# Remove temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"\\nRemoved temporary directory: {temp_dir}\")\n",
    "\n",
    "print(\"\\nCleanup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the key features of the FlowerPower IO library:\n",
    "\n",
    "1. **CSV Reading**: Used `CSVFileReader` to read CSV files and convert them to multiple formats\n",
    "2. **Data Conversion**: Showed how to convert between Pandas, Polars, and PyArrow formats\n",
    "3. **Parquet Writing**: Used `ParquetFileWriter` to save data in the efficient Parquet format\n",
    "4. **Database Operations**: Demonstrated both reading from and writing to SQLite databases using `SQLiteReader` and `SQLiteWriter`\n",
    "5. **Advanced Querying**: Showed how to use custom SQL queries for data filtering and aggregation\n",
    "6. **Metadata**: Explored metadata functionality to get insights about the data\n",
    "\n",
    "The FlowerPower IO library provides a unified interface for various data operations, making it easy to work with different file formats and database systems while maintaining excellent performance and flexibility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}