# For optionally automatically creating an LLM provider
#   - Provider values include openai.sync, litellm.ollama, watsonx, and others
#   - (see toolkit-core/llm/README for more details)
LLM_PROVIDER=openai.sync
# Name or ID of model to use, note that this format is provider-specific
MODEL_NAME=

# For Ollama
# OLLAMA_API_URL= # optional, default is the ollama default localhost:11434
# OLLAMA_API_KEY= # optional

# For WatsonX
WATSONX_APIKEY=
WATSONX_PROJECT_ID=
WX_API_KEY=
WX_PROJECT_ID=
WX_URL="https://us-south.ml.cloud.ibm.com"

# For OpenAI
OPENAI_API_KEY=

# For Azure OpenAI
AZURE_OPENAI_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=

# Used by Code Generation to run generated code in sandbox container
#. - (see post-tool-reflection/code_generation/README for more details)
DOCKER_HOST=
