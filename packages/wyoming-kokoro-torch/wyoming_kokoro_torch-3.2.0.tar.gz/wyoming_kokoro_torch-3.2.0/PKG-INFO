Metadata-Version: 2.4
Name: wyoming-kokoro-torch
Version: 3.2.0
Summary: Wyoming Server for Kokoro Torch
Author-email: Laurent Debacker <debackerl@gmail.com>
License: MIT
Project-URL: Homepage, http://github.com/debackerl/wyoming-kokoro-torch
Keywords: rhasspy,wyoming,kokoro,tts
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Topic :: Text Processing :: Linguistic
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE.md
Requires-Dist: wyoming<1.8,>=1.7.2
Requires-Dist: kokoro==0.9.4
Requires-Dist: sentence-stream==1.2.0
Provides-Extra: dev
Requires-Dist: black==24.8.0; extra == "dev"
Requires-Dist: flake8==7.1.1; extra == "dev"
Requires-Dist: mypy==1.14.0; extra == "dev"
Requires-Dist: pylint==3.2.7; extra == "dev"
Requires-Dist: pytest==8.3.4; extra == "dev"
Requires-Dist: pytest-asyncio==1.1.0; extra == "dev"
Requires-Dist: build==1.2.2; extra == "dev"
Requires-Dist: scipy<2,>=1.10; extra == "dev"
Requires-Dist: numpy<2,>=1.20; extra == "dev"
Requires-Dist: python-speech-features==0.6; extra == "dev"
Requires-Dist: twine==6.1.0; extra == "dev"
Dynamic: license-file

# Wyoming Kokoro Torch

[Wyoming protocol](https://github.com/rhasspy/wyoming) server for the original [Kokoro](https://github.com/hexgrad/kokoro/) Torch TTS implementation.

Contrary to other Wyoming implementation, [wyoming-kokoro](https://github.com/nordwestt/kokoro-wyoming/), this is one uses Torch instead of ONNX.
As of the time of writing, our implementation also supports `streaming` mode, while the ONNX one doesn't. Streaming is important for LLM-based assistant,
so that it can start speaking before the LLM is finished generating.

## Local Install

Clone the repository and set up Python virtual environment:

``` sh
git clone https://github.com/debackerl/wyoming-kokoro-torch.cpp.git
cd wyoming-kokoro-torch
script/setup
```

Download the base model:

```sh
mkdir /data
wget -O /data/kokoro-v1_0.pth https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/kokoro-v1_0.pth
wget -O /data/config.json https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/config.json
```

Run a server anyone can connect to:

```sh
script/run --voice af_heart --streaming --uri 'tcp://0.0.0.0:10300' --data-dir /data --download-dir /data
```

See [available voices](https://huggingface.co/hexgrad/Kokoro-82M/tree/main/voices).

## Remarks

If you run this in a VM, you may see `Could not initialize NNPACK! Reason: Unsupported hardware.` in the logs. This seems to happen on heterogeneous CPU architectures,
like my AMD Ryzen AI 7 HX 370. Solutions seems to be either running bare-metal or pass the L3 cache information to the VM and tweak CPU affinity.
