"""
Smart Token-Aware Context Management Demo

Demonstrates intelligent token counting, budget optimization, and cost-aware
context selection for LLM applications.

Features demonstrated:
- Accurate token counting for multiple LLM models
- Dynamic context selection within token budgets
- Cost optimization strategies
- Relevance-based context ranking
- Token usage analytics and forecasting
- Model-specific optimization profiles

Usage:
    python examples/token_optimization_demo.py
"""

import json
import random
import time
from typing import List, Dict

try:
    import sys
    import os

    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    from context_store import (
        ContextReferenceStore,
        CacheEvictionPolicy,
        create_token_manager,
        OptimizationStrategy,
    )

    CONTEXT_STORE_AVAILABLE = True
except ImportError as e:
    print(f"Context Reference Store not available: {e}")
    print("Run from the library root directory or install with: pip install -e .")
    CONTEXT_STORE_AVAILABLE = False


def generate_diverse_content() -> List[Dict[str, str]]:
    """Generate diverse content types for token optimization testing."""

    content_samples = [
        {
            "type": "system_prompt",
            "content": "You are an AI assistant specialized in data analysis and business intelligence. Provide detailed, accurate responses based on the given context.",
        },
        {
            "type": "short_instruction",
            "content": "Analyze the quarterly sales data and identify key trends.",
        },
        {
            "type": "medium_context",
            "content": """
            Company Overview: TechCorp Inc.
            
            TechCorp is a leading software development company specializing in artificial intelligence
            and machine learning solutions. Founded in 2015, the company has grown from a startup
            to a mid-size enterprise with over 500 employees across three offices in San Francisco,
            New York, and Austin.
            
            Our primary products include:
            - AI-powered data analytics platform
            - Machine learning model deployment tools
            - Custom AI consulting services
            - Cloud-based inference APIs
            
            Recent achievements include securing Series C funding of $50M and partnerships with
            major cloud providers. The company is projected to achieve $100M ARR by 2025.
            """,
        },
        {
            "type": "large_data_context",
            "content": json.dumps(
                {
                    "quarterly_sales": {
                        "Q1_2024": [
                            {
                                "month": "January",
                                "revenue": 2500000,
                                "customers": 150,
                                "deals": [
                                    {
                                        "company": "Enterprise Corp",
                                        "value": 500000,
                                        "product": "AI Platform",
                                    },
                                    {
                                        "company": "DataTech LLC",
                                        "value": 300000,
                                        "product": "ML Tools",
                                    },
                                    {
                                        "company": "Analytics Inc",
                                        "value": 150000,
                                        "product": "Consulting",
                                    },
                                ],
                            },
                            {
                                "month": "February",
                                "revenue": 2800000,
                                "customers": 165,
                                "deals": [
                                    {
                                        "company": "Global Systems",
                                        "value": 750000,
                                        "product": "AI Platform",
                                    },
                                    {
                                        "company": "Smart Solutions",
                                        "value": 400000,
                                        "product": "ML Tools",
                                    },
                                    {
                                        "company": "Data Insights",
                                        "value": 200000,
                                        "product": "APIs",
                                    },
                                ],
                            },
                            {
                                "month": "March",
                                "revenue": 3200000,
                                "customers": 180,
                                "deals": [
                                    {
                                        "company": "Innovation Labs",
                                        "value": 900000,
                                        "product": "AI Platform",
                                    },
                                    {
                                        "company": "Tech Dynamics",
                                        "value": 550000,
                                        "product": "ML Tools",
                                    },
                                    {
                                        "company": "Future Analytics",
                                        "value": 350000,
                                        "product": "Consulting",
                                    },
                                ],
                            },
                        ],
                        "Q2_2024": [
                            {
                                "month": "April",
                                "revenue": 3500000,
                                "customers": 195,
                                "deals": [
                                    {
                                        "company": "Mega Corp",
                                        "value": 1200000,
                                        "product": "AI Platform",
                                    },
                                    {
                                        "company": "Advanced Tech",
                                        "value": 600000,
                                        "product": "ML Tools",
                                    },
                                    {
                                        "company": "Data Pioneers",
                                        "value": 400000,
                                        "product": "APIs",
                                    },
                                ],
                            },
                            {
                                "month": "May",
                                "revenue": 3800000,
                                "customers": 210,
                                "deals": [
                                    {
                                        "company": "Enterprise Plus",
                                        "value": 1100000,
                                        "product": "AI Platform",
                                    },
                                    {
                                        "company": "ML Innovations",
                                        "value": 700000,
                                        "product": "ML Tools",
                                    },
                                    {
                                        "company": "Analytics Pro",
                                        "value": 450000,
                                        "product": "Consulting",
                                    },
                                ],
                            },
                            {
                                "month": "June",
                                "revenue": 4200000,
                                "customers": 225,
                                "deals": [
                                    {
                                        "company": "Global Enterprises",
                                        "value": 1500000,
                                        "product": "AI Platform",
                                    },
                                    {
                                        "company": "Smart Tech Corp",
                                        "value": 800000,
                                        "product": "ML Tools",
                                    },
                                    {
                                        "company": "Data Experts",
                                        "value": 500000,
                                        "product": "APIs",
                                    },
                                ],
                            },
                        ],
                    },
                    "market_analysis": {
                        "industry_trends": [
                            "Increased demand for AI automation solutions",
                            "Growing adoption of cloud-based ML platforms",
                            "Rising focus on explainable AI and model governance",
                            "Expansion into edge computing and IoT applications",
                        ],
                        "competitive_landscape": {
                            "main_competitors": [
                                "AI Solutions Inc",
                                "ML Platform Corp",
                                "Data Analytics Pro",
                            ],
                            "market_share": {
                                "TechCorp": "15%",
                                "AI Solutions Inc": "22%",
                                "ML Platform Corp": "18%",
                            },
                            "differentiation": "Focus on ease-of-use and rapid deployment capabilities",
                        },
                    },
                },
                indent=2,
            ),
        },
        {
            "type": "code_documentation",
            "content": """
            # Advanced Analytics Engine Documentation
            
            ## Overview
            The Advanced Analytics Engine provides real-time data processing and machine learning
            capabilities for enterprise applications. This documentation covers the core API
            endpoints, usage patterns, and integration examples.
            
            ## Core Classes
            
            ### DataProcessor
            ```python
            class DataProcessor:
                def __init__(self, config: Dict[str, Any]):
                    '''Initialize processor with configuration'''
                    self.config = config
                    self.models = {}
                    
                def process_batch(self, data: List[Dict]) -> List[Dict]:
                    '''Process a batch of data records'''
                    results = []
                    for record in data:
                        processed = self.apply_transformations(record)
                        predictions = self.generate_predictions(processed)
                        results.append({
                            'input': record,
                            'processed': processed,
                            'predictions': predictions,
                            'confidence': self.calculate_confidence(predictions)
                        })
                    return results
                    
                def apply_transformations(self, record: Dict) -> Dict:
                    '''Apply data transformations'''
                    # Normalization
                    for key, value in record.items():
                        if isinstance(value, (int, float)):
                            record[key] = self.normalize_value(value, key)
                    
                    # Feature engineering
                    record['derived_features'] = self.extract_features(record)
                    
                    return record
            ```
            
            ### ModelManager
            ```python
            class ModelManager:
                def __init__(self):
                    self.models = {}
                    self.performance_metrics = {}
                    
                def load_model(self, model_id: str, model_path: str):
                    '''Load a trained model from disk'''
                    import joblib
                    self.models[model_id] = joblib.load(model_path)
                    
                def predict(self, model_id: str, features: Dict) -> Dict:
                    '''Generate predictions using specified model'''
                    if model_id not in self.models:
                        raise ValueError(f"Model {model_id} not found")
                    
                    model = self.models[model_id]
                    prediction = model.predict([list(features.values())])[0]
                    confidence = model.predict_proba([list(features.values())])[0].max()
                    
                    return {
                        'prediction': prediction,
                        'confidence': confidence,
                        'model_id': model_id,
                        'timestamp': time.time()
                    }
            ```
            
            ## API Endpoints
            
            ### POST /api/v1/analyze
            Process data and generate insights
            
            **Request Body:**
            ```json
            {
                "data": [
                    {"field1": "value1", "field2": 123},
                    {"field1": "value2", "field2": 456}
                ],
                "model_config": {
                    "type": "classification",
                    "version": "1.2.0"
                }
            }
            ```
            
            **Response:**
            ```json
            {
                "results": [
                    {
                        "input": {"field1": "value1", "field2": 123},
                        "prediction": "category_a",
                        "confidence": 0.95,
                        "features": {...}
                    }
                ],
                "metadata": {
                    "processing_time_ms": 250,
                    "model_version": "1.2.0",
                    "records_processed": 2
                }
            }
            ```
            
            ## Usage Examples
            
            ### Basic Data Processing
            ```python
            from analytics_engine import DataProcessor, ModelManager
            
            # Initialize components
            processor = DataProcessor({
                'normalization': True,
                'feature_extraction': True
            })
            
            model_manager = ModelManager()
            model_manager.load_model('classifier_v1', '/path/to/model.pkl')
            
            # Process data
            raw_data = [
                {'revenue': 50000, 'customers': 100, 'region': 'north'},
                {'revenue': 75000, 'customers': 150, 'region': 'south'}
            ]
            
            processed_data = processor.process_batch(raw_data)
            
            # Generate predictions
            for record in processed_data:
                prediction = model_manager.predict('classifier_v1', record['processed'])
                print(f"Prediction: {prediction['prediction']}, Confidence: {prediction['confidence']}")
            ```
            """
            * 2,  # Double the content to make it larger
        },
        {
            "type": "user_query",
            "content": "I need a comprehensive analysis of our Q2 2024 sales performance compared to Q1, including insights on customer acquisition trends, revenue growth patterns, and recommendations for Q3 strategy optimization.",
        },
        {
            "type": "detailed_requirements",
            "content": """
            Project Requirements: AI-Powered Sales Analytics Dashboard
            
            Stakeholder: Sales Operations Team
            Project Duration: 8 weeks
            Budget: $150,000
            
            Functional Requirements:
            
            1. Real-time Data Integration
               - Connect to Salesforce, HubSpot, and internal CRM systems
               - Support for REST APIs and database connections
               - Data synchronization every 15 minutes
               - Handle up to 10,000 sales records per hour
            
            2. Interactive Dashboards
               - Executive summary dashboard with key metrics
               - Detailed performance analytics by region, product, sales rep
               - Trend analysis with historical comparisons
               - Customizable date ranges and filters
               - Export capabilities (PDF, Excel, PowerPoint)
            
            3. Predictive Analytics
               - Sales forecasting using machine learning models
               - Lead scoring and conversion probability
               - Customer lifetime value predictions
               - Churn risk analysis and early warning alerts
               - Revenue forecasting with confidence intervals
            
            4. Automated Reporting
               - Weekly sales performance summaries
               - Monthly executive reports
               - Quarterly business reviews with insights
               - Alert notifications for significant changes
               - Scheduled email distribution to stakeholders
            
            5. Advanced Analytics Features
               - Cohort analysis for customer segments
               - Sales funnel optimization recommendations
               - Competitive analysis integration
               - Market trend correlation analysis
               - ROI calculations for marketing campaigns
            
            Technical Requirements:
            
            1. Architecture
               - Cloud-native deployment on AWS/Azure
               - Microservices architecture for scalability
               - Real-time data processing with Apache Kafka
               - Data lake storage for historical analysis
               - API-first design for future integrations
            
            2. Performance
               - Dashboard load times under 3 seconds
               - Support for concurrent users (up to 100)
               - 99.9% uptime SLA requirement
               - Sub-second query response times
               - Horizontal scaling capabilities
            
            3. Security
               - Role-based access control (RBAC)
               - Single sign-on (SSO) integration
               - Data encryption at rest and in transit
               - Audit logging for compliance
               - GDPR and SOX compliance features
            
            4. Data Management
               - Automated data quality checks
               - Data lineage tracking
               - Backup and disaster recovery
               - Data retention policies
               - Master data management integration
            
            Success Criteria:
            - 30% reduction in manual reporting time
            - 15% improvement in sales forecast accuracy
            - 95% user adoption rate within 3 months
            - ROI positive within 12 months
            - Integration with existing tools without disruption
            """,
        },
    ]

    return content_samples


def demonstrate_token_optimization():
    """Demonstrate token-aware context optimization capabilities."""

    if not CONTEXT_STORE_AVAILABLE:
        print("Cannot run demonstration - Context Reference Store not available")
        return

    print("SMART TOKEN-AWARE CONTEXT MANAGEMENT DEMONSTRATION")
    print("=" * 65)
    print()
    print("This demo shows how to optimize context selection based on:")
    print("- Token budgets for different LLM models")
    print("- Cost optimization strategies")
    print("- Relevance-based ranking and selection")
    print("- Real-time token counting and analytics")
    print()

    # Create context store with compression for efficiency
    print("Initializing Context Store with Token Management...")
    context_store = ContextReferenceStore(
        cache_size=100,
        eviction_policy=CacheEvictionPolicy.LRU,
        enable_compression=True,
        enable_cache_warming=True,
    )

    # Generate and store diverse content
    print("Generating diverse content samples...")
    content_samples = generate_diverse_content()

    context_ids = []
    for sample in content_samples:
        context_id = context_store.store(
            sample["content"], metadata={"content_type": sample["type"], "demo": True}
        )
        context_ids.append(context_id)

    print(f"    Stored {len(content_samples)} diverse content samples")

    # Demonstrate token analytics
    print("\nAnalyzing token distribution across different LLM models...")

    models_to_test = ["gemini-1.5-pro", "gpt-4", "gpt-3.5-turbo", "claude-3-sonnet"]

    for model in models_to_test:
        print(f"\nToken Analysis for {model}:")

        try:
            analytics = context_store.get_token_analytics(model)

            if "error" in analytics:
                print(f"{analytics['message']}")
                continue

            print(f"Total contexts: {analytics['total_contexts']:,}")
            print(f"Total tokens: {analytics['total_tokens']:,}")
            print(f"Estimated cost: ${analytics['estimated_cost']:.4f}")
            print(
                f"Avg tokens per context: {analytics['average_tokens_per_context']:.0f}"
            )
            print(
                f"Model capacity utilization: {analytics['model_capacity_analysis']['utilization_if_all_used']:.1%}"
            )

            # Show distribution
            dist = analytics["token_distribution"]
            print(
                f"Distribution: {dist['small_contexts_under_1k']} small, "
                + f"{dist['medium_contexts_1k_10k']} medium, {dist['large_contexts_over_10k']} large"
            )

        except Exception as e:
            print(f"Error analyzing {model}: {e}")

    # Demonstrate optimization strategies
    print("\nTesting different optimization strategies...")

    strategies = [
        ("cost_first", "Minimize cost while meeting requirements"),
        ("quality_first", "Maximize context quality within budget"),
        ("balanced", "Balance cost and quality optimally"),
        ("speed_first", "Optimize for fastest processing"),
        ("comprehensive", "Include maximum relevant context"),
    ]

    # Test with different budgets and queries
    test_scenarios = [
        {
            "name": "Budget Analysis Query",
            "model": "gemini-1.5-pro",
            "target_tokens": 50000,
            "query": "sales performance analysis Q2 2024 revenue trends",
            "keywords": ["sales", "revenue", "Q2", "performance", "trends"],
        },
        {
            "name": "Cost-Conscious Query",
            "model": "gpt-3.5-turbo",
            "target_tokens": 8000,
            "query": "quarterly sales data analysis",
            "keywords": ["sales", "quarterly", "data"],
        },
        {
            "name": "Large Context Query",
            "model": "claude-3-sonnet",
            "target_tokens": 100000,
            "query": "comprehensive business intelligence dashboard requirements",
            "keywords": ["dashboard", "requirements", "analytics", "business"],
        },
    ]

    for scenario in test_scenarios:
        print(f"\nScenario: {scenario['name']}")
        print(f"Model: {scenario['model']}")
        print(f"Budget: {scenario['target_tokens']:,} tokens")
        print(f"Query: '{scenario['query']}'")

        for strategy_name, description in strategies:
            print(f"\nStrategy: {strategy_name.replace('_', ' ').title()}")
            print(f"{description}")

            try:
                result = context_store.create_token_aware_selection(
                    model_name=scenario["model"],
                    target_tokens=scenario["target_tokens"],
                    strategy=strategy_name,
                    query=scenario["query"],
                    keywords=scenario["keywords"],
                )

                if "error" in result:
                    print(f"          {result['message']}")
                    continue

                print(f"Selected contexts: {len(result['selected_contexts'])}")
                print(f"Total tokens: {result['total_tokens']:,}")
                print(f"Budget utilization: {result['budget_utilization']:.1%}")
                print(f"Estimated cost: ${result['estimated_cost']:.4f}")
                print(f"Efficiency score: {result['efficiency_score']:.2f}")
                print(f"Excluded contexts: {result['excluded_count']}")

                if result["recommendations"]:
                    print(f"Recommendations:")
                    for rec in result["recommendations"][:2]:
                        print(f" - {rec}")

            except Exception as e:
                print(f"          Error: {e}")

    # Demonstrate cost comparison across models
    print("\nCost Comparison Across Models...")

    comparison_query = (
        "Analyze quarterly sales performance and provide strategic recommendations"
    )
    target_tokens = 25000

    cost_comparison = []

    for model in models_to_test:
        try:
            result = context_store.create_token_aware_selection(
                model_name=model,
                target_tokens=target_tokens,
                strategy="balanced",
                query=comparison_query,
            )

            if "error" not in result:
                cost_comparison.append(
                    {
                        "model": model,
                        "cost": result["estimated_cost"],
                        "tokens": result["total_tokens"],
                        "contexts": len(result["selected_contexts"]),
                        "efficiency": result["efficiency_score"],
                    }
                )

        except Exception as e:
            print(f"    Error testing {model}: {e}")

    if cost_comparison:
        print(f"\n    Cost Comparison for {target_tokens:,} token budget:")

        # Sort by cost
        cost_comparison.sort(key=lambda x: x["cost"])

        for comp in cost_comparison:
            print(
                f"      {comp['model']:<20}: ${comp['cost']:.4f} "
                + f"({comp['tokens']:,} tokens, {comp['contexts']} contexts, "
                + f"efficiency: {comp['efficiency']:.2f})"
            )

        # Calculate savings
        if len(cost_comparison) > 1:
            cheapest = cost_comparison[0]
            most_expensive = cost_comparison[-1]
            savings = most_expensive["cost"] - cheapest["cost"]
            savings_percent = (savings / most_expensive["cost"]) * 100

            print(
                f"\n    Potential savings: ${savings:.4f} ({savings_percent:.1f}%) "
                + f"by choosing {cheapest['model']} over {most_expensive['model']}"
            )

    # Show optimization recommendations
    print("\nSmart Optimization Recommendations...")

    # Create token manager for detailed recommendations
    try:
        token_manager = create_token_manager("gemini-1.5-pro")

        # Simulate some usage for analytics
        contexts = [sample["content"] for sample in content_samples]
        budget = token_manager.create_budget(target_tokens=50000)

        result = token_manager.optimize_context_selection(
            contexts=contexts,
            budget=budget,
            strategy=OptimizationStrategy.BALANCED,
            query="comprehensive business analysis",
        )

        analytics = token_manager.get_usage_analytics()
        suggestions = token_manager.suggest_model_upgrade(analytics)

        print(f"    Usage Analytics:")
        print(f"      • Total optimizations: {analytics['total_optimizations']}")
        print(
            f"      • Average tokens per optimization: {analytics.get('avg_tokens_per_optimization', 0):.0f}"
        )
        print(
            f"      • Total estimated cost: ${analytics.get('total_estimated_cost', 0):.4f}"
        )
        print(
            f"      • Average budget utilization: {analytics['average_budget_utilization']:.1%}"
        )

        if suggestions["suggestions"]:
            print(f"\n    Model Optimization Suggestions:")
            for suggestion in suggestions["suggestions"]:
                print(f"      • {suggestion['recommendation']}")
                if "potential_savings_percent" in suggestion:
                    print(
                        f"        Potential savings: {suggestion['potential_savings_percent']:.1f}%"
                    )

    except Exception as e:
        print(f"    Could not generate detailed recommendations: {e}")

    print("\nPerformance Impact Summary...")

    # Calculate combined benefits
    print("    Combined Performance Benefits:")
    print("      - Base Context Reference Store: Dramatic serialization speedup")
    print("      - + Smart Compression: 10-50x additional storage reduction")
    print("      - + Token Optimization: Intelligent cost and quality balance")
    print("      - + Real-time Analytics: Continuous optimization insights")
    print()
    print("    Total Value Proposition:")
    print("      - Significant performance gains over traditional approaches")
    print("      - Intelligent LLM cost optimization")
    print("      - Zero quality degradation with ROUGE validation")
    print("      - Real-time monitoring and optimization recommendations")
    print("      - Framework-agnostic design for universal adoption")

    print("\nToken optimization demonstration completed successfully!")
    print(
        "Context Reference Store now includes intelligent token management functionality."
    )


def main():
    """Run the comprehensive token optimization demonstration."""
    demonstrate_token_optimization()


if __name__ == "__main__":
    main()
