"""
Context Reference Store for Efficient Management of Large Context Windows

This module implements a solution for efficiently managing large context windows (1M-2M tokens)
by using a reference-based approach rather than direct context passing.
"""

import time
import json
import uuid
import hashlib
import psutil
import threading
import os
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from .compression_manager import (
    ContextCompressionManager,
    CompressionAlgorithm,
    CompressionResult,
)

try:
    from ..optimization.token_manager import (
        TokenAwareContextManager,
        OptimizationStrategy,
        create_token_manager,
    )

    TOKEN_OPTIMIZATION_AVAILABLE = True
except ImportError:
    TOKEN_OPTIMIZATION_AVAILABLE = False

try:
    from ..semantic.semantic_analyzer import (
        SemanticContextAnalyzer,
        create_semantic_analyzer,
    )

    SEMANTIC_ANALYSIS_AVAILABLE = True
except ImportError:
    SEMANTIC_ANALYSIS_AVAILABLE = False


# Generic multimodal data structures - framework agnostic
class MultimodalContent:
    """Generic container for multimodal content."""

    def __init__(self, role: str = "user", parts: list = None):
        self.role = role
        self.parts = parts or []


class MultimodalPart:
    """Generic container for a single multimodal part."""

    def __init__(
        self,
        text: str = None,
        binary_data: bytes = None,
        mime_type: str = None,
        file_uri: str = None,
    ):
        self.text = text
        self.binary_data = binary_data
        self.mime_type = mime_type
        self.file_uri = file_uri

    @classmethod
    def from_text(cls, text: str):
        return cls(text=text)

    @classmethod
    def from_binary(cls, data: bytes, mime_type: str):
        return cls(binary_data=data, mime_type=mime_type)

    @classmethod
    def from_file(cls, file_uri: str, mime_type: str = None):
        return cls(file_uri=file_uri, mime_type=mime_type)


class CacheEvictionPolicy(Enum):
    """Cache eviction policies."""

    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    TTL = "ttl"  # Time To Live based
    MEMORY_PRESSURE = "memory_pressure"  # Based on system memory usage


@dataclass
class ContextMetadata:
    """Metadata for stored context."""

    content_type: str = "text/plain"
    token_count: int = 0
    created_at: float = field(default_factory=time.time)
    last_accessed: float = field(default_factory=time.time)
    access_count: int = 0
    tags: List[str] = field(default_factory=list)
    cache_id: Optional[str] = None
    cached_until: Optional[float] = None  # Timestamp when cache expires
    is_structured: bool = False  # Whether this is JSON or not
    priority: int = 0  # Higher priority contexts are kept longer
    frequency_score: float = 0.0  # Calculated frequency score for LFU

    def update_access_stats(self):
        """Update access statistics."""
        self.last_accessed = time.time()
        self.access_count += 1

        # Update frequency score for LFU
        current_time = time.time()
        time_since_creation = current_time - self.created_at
        if time_since_creation > 0:
            self.frequency_score = self.access_count / time_since_creation

    def is_expired(self) -> bool:
        """Check if context has expired based on TTL."""
        if self.cached_until is None:
            return False
        return time.time() > self.cached_until


class ContextReferenceStore:
    """
    A store for large contexts that provides reference-based access with advanced caching.

    This class allows large contexts to be stored once and referenced by ID,
    preventing unnecessary duplication and serialization of large data.

    Features:
    - Dramatically faster serialization compared to traditional approaches
    - Substantial memory reduction in multi-agent scenarios
    - Major storage reduction for multimodal content
    - Advanced caching strategies (LRU, LFU, TTL, Memory Pressure)
    - Multimodal support for images, audio, video
    - Binary deduplication with SHA256 hashing
    """

    def __init__(
        self,
        cache_size: int = 50,
        eviction_policy: CacheEvictionPolicy = CacheEvictionPolicy.LRU,
        memory_threshold: float = 0.8,  # 80% memory usage threshold
        ttl_check_interval: int = 300,  # Check TTL every 5 minutes
        enable_cache_warming: bool = True,
        use_disk_storage: bool = True,
        binary_cache_dir: str = "./multimodal_cache",
        large_binary_threshold: int = 1024 * 1024,  # 1MB threshold
        enable_compression: bool = True,  # Enable smart compression
        compression_min_size: int = 1024,  # Minimum size for compression (1KB)
        compression_algorithm: Optional[str] = None,  # e.g., "lz4", "zstd", "zlib"
        compression_level: Optional[int] = None,  # e.g., 1-9 depending on algorithm
    ):
        """
        Initialize the Context Reference Store.

        Args:
            cache_size: Maximum number of contexts to keep in memory
            eviction_policy: Cache eviction policy to use
            memory_threshold: Memory usage threshold for pressure-based eviction (0.0-1.0)
            ttl_check_interval: Interval in seconds to check for expired contexts
            enable_cache_warming: Whether to enable cache warming strategies
            use_disk_storage: Whether to use disk storage for large binaries
            binary_cache_dir: Directory to store binary cache files
            large_binary_threshold: Size threshold for using disk storage
            enable_compression: Whether to enable intelligent content compression
            compression_min_size: Minimum content size in bytes to attempt compression
        """
        self._contexts: Dict[str, str] = {}
        self._metadata: Dict[str, ContextMetadata] = {}
        self._cache_size = cache_size
        self._eviction_policy = eviction_policy
        self._memory_threshold = memory_threshold
        self._ttl_check_interval = ttl_check_interval
        self._enable_cache_warming = enable_cache_warming

        # Multimodal storage infrastructure
        self._use_disk_storage = use_disk_storage
        self._binary_cache_dir = binary_cache_dir
        self._large_binary_threshold = large_binary_threshold
        self._binary_store: Dict[str, Union[bytes, str]] = {}
        self._binary_metadata: Dict[str, Dict[str, Any]] = {}

        # Cache warming data
        self._warmup_contexts: List[str] = []  # Contexts to keep warm
        self._access_patterns: Dict[str, List[float]] = {}  # Track access patterns

        # Background thread for TTL cleanup
        self._cleanup_thread = None
        self._stop_cleanup = False

        # Statistics
        self._stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "memory_pressure_evictions": 0,
            "ttl_evictions": 0,
        }

        # Initialize compression manager
        self._enable_compression = enable_compression
        if self._enable_compression:
            self._compression_manager = ContextCompressionManager(
                enable_analytics=True, min_compression_size=compression_min_size
            )
            # Storage for compressed content metadata
            self._compression_metadata: Dict[str, Dict[str, Any]] = {}

            # Configure preferred algorithm/level if provided
            self._preferred_compression_algorithm = None
            if compression_algorithm:
                try:
                    algo_str = str(compression_algorithm).strip().lower()
                    algo_map = {
                        "zlib": CompressionAlgorithm.ZLIB,
                        "gzip": CompressionAlgorithm.GZIP,
                        "bzip2": CompressionAlgorithm.BZIP2,
                        "bz2": CompressionAlgorithm.BZIP2,
                        "lzma": CompressionAlgorithm.LZMA,
                        "lz4": CompressionAlgorithm.LZ4,
                        "zstd": CompressionAlgorithm.ZSTD,
                        "zstandard": CompressionAlgorithm.ZSTD,
                        "none": CompressionAlgorithm.NONE,
                    }
                    self._preferred_compression_algorithm = algo_map.get(algo_str)
                except Exception:
                    self._preferred_compression_algorithm = None

            # Apply level override if provided
            if compression_level is not None and self._preferred_compression_algorithm:
                try:
                    level_value = int(compression_level)
                    alg = self._preferred_compression_algorithm
                    if alg == CompressionAlgorithm.ZLIB:
                        self._compression_manager.algorithm_configs[alg][
                            "level"
                        ] = level_value
                    elif alg == CompressionAlgorithm.GZIP:
                        self._compression_manager.algorithm_configs[alg][
                            "compresslevel"
                        ] = level_value
                    elif alg == CompressionAlgorithm.BZIP2:
                        self._compression_manager.algorithm_configs[alg][
                            "compresslevel"
                        ] = level_value
                    elif alg == CompressionAlgorithm.LZMA:
                        # lzma uses preset 0-9
                        self._compression_manager.algorithm_configs[alg][
                            "preset"
                        ] = level_value
                    elif alg == CompressionAlgorithm.LZ4:
                        # lz4 frame accepts compression_level
                        self._compression_manager.algorithm_configs[alg][
                            "compression_level"
                        ] = level_value
                    elif alg == CompressionAlgorithm.ZSTD:
                        self._compression_manager.algorithm_configs[alg][
                            "level"
                        ] = level_value
                except Exception:
                    pass
        else:
            self._compression_manager = None
            self._compression_metadata = {}

        # Create binary cache directory if needed
        if self._use_disk_storage:
            os.makedirs(self._binary_cache_dir, exist_ok=True)

        if self._ttl_check_interval > 0:
            self._start_ttl_cleanup()

    def store(self, content: Any, metadata: Optional[Dict[str, Any]] = None) -> str:
        """
        Store context and return a reference ID with optional compression.

        Args:
            content: The context content to store (string or structured data)
            metadata: Optional metadata about the context

        Returns:
            A reference ID for the stored context
        """
        # Handle both string and structured data (JSON objects)
        is_structured = not isinstance(content, str)

        # Convert structured data to string for storage
        if is_structured:
            content_str = json.dumps(content)
            content_hash = hashlib.md5(content_str.encode()).hexdigest()
        else:
            content_str = content
            content_hash = hashlib.md5(content.encode()).hexdigest()

        # Apply compression if enabled
        stored_content = content_str
        compression_info = None

        if self._enable_compression and self._compression_manager:
            try:
                # Use preferred algorithm if provided; otherwise let manager select
                compression_result = self._compression_manager.compress(
                    content_str, algorithm=self._preferred_compression_algorithm
                )

                # Only use compression if it provides meaningful savings
                if compression_result.compression_ratio < 0.9:  # At least 10% savings
                    # Store compressed data as base64 string for JSON compatibility
                    import base64

                    stored_content = base64.b64encode(
                        compression_result.compressed_data
                    ).decode("utf-8")
                    compression_info = {
                        "algorithm": compression_result.algorithm.value,
                        "original_size": compression_result.original_size,
                        "compressed_size": compression_result.compressed_size,
                        "compression_ratio": compression_result.compression_ratio,
                        "content_type": compression_result.content_type.value,
                        "space_savings_percent": compression_result.space_savings,
                        "is_compressed": True,
                    }
                    content_hash = hashlib.md5(stored_content.encode()).hexdigest()
                else:
                    # Compression not beneficial, store uncompressed
                    compression_info = {
                        "is_compressed": False,
                        "reason": "Insufficient compression ratio",
                        "original_algorithm_tested": compression_result.algorithm.value,
                    }
            except Exception as e:
                # Compression failed, store uncompressed
                compression_info = {
                    "is_compressed": False,
                    "reason": f"Compression failed: {str(e)}",
                }
        else:
            compression_info = {
                "is_compressed": False,
                "reason": "Compression disabled",
            }

        # Check for existing content with deduplication
        for context_id, existing_content in self._contexts.items():
            existing_hash = hashlib.md5(existing_content.encode()).hexdigest()
            if (
                existing_hash == content_hash
                and self._metadata[context_id].is_structured == is_structured
            ):
                # Update access stats
                self._metadata[context_id].update_access_stats()
                self._stats["hits"] += 1
                self._track_access_pattern(context_id)
                return context_id

        context_id = str(uuid.uuid4())
        self._stats["misses"] += 1
        self._contexts[context_id] = stored_content
        # Store compression metadata if compression was used
        if compression_info:
            self._compression_metadata[context_id] = compression_info

        # Set content type based on input type
        if is_structured:
            content_type = "application/json"
        else:
            content_type = (
                metadata.get("content_type", "text/plain") if metadata else "text/plain"
            )

        meta = ContextMetadata(
            content_type=content_type,
            token_count=len(content_str) // 4,
            tags=metadata.get("tags", []) if metadata else [],
            is_structured=is_structured,
        )

        if metadata and "priority" in metadata:
            meta.priority = metadata["priority"]
        # Generate a cache ID for external caching systems
        if metadata and "cache_id" in metadata:
            meta.cache_id = metadata["cache_id"]
        else:
            meta.cache_id = f"context_{content_hash[:16]}"
        # Set cache expiration if provided
        if metadata and "cache_ttl" in metadata:
            ttl_seconds = metadata["cache_ttl"]
            meta.cached_until = time.time() + ttl_seconds

        self._metadata[context_id] = meta
        self._track_access_pattern(context_id)
        # Check if we need to warm this context
        if self._enable_cache_warming and self._should_warm_context(context_id):
            self._warmup_contexts.append(context_id)

        self._manage_cache()
        return context_id

    def store_multimodal_content(
        self,
        content: Union[str, Dict, MultimodalContent, MultimodalPart, Any],
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Store multimodal content including images, audio, video."""
        if isinstance(content, MultimodalContent):
            return self._store_content_with_parts(content, metadata)
        elif isinstance(content, MultimodalPart):
            return self._store_part(content, metadata)
        else:
            return self.store(content, metadata)

    def _store_content_with_parts(self, content, metadata):
        """Handle Content with multiple Parts (text + images + audio)."""
        content_data = {"role": content.role, "parts": []}

        # Pre-build the content data structure for deduplication check
        for part in content.parts:
            if part.text:
                content_data["parts"].append({"type": "text", "data": part.text})
            elif part.binary_data:
                binary_hash = hashlib.sha256(part.binary_data).hexdigest()
                content_data["parts"].append(
                    {
                        "type": "binary_ref",
                        "mime_type": part.mime_type,
                        "binary_hash": binary_hash,
                        "size": len(part.binary_data),
                    }
                )
            elif part.file_uri:
                content_data["parts"].append(
                    {
                        "type": "file_ref",
                        "file_uri": part.file_uri,
                        "mime_type": part.mime_type,
                    }
                )

        # Check if this content already exists before storing binaries
        content_str = json.dumps(content_data)
        content_hash = hashlib.md5(content_str.encode()).hexdigest()

        for context_id, existing_content in self._contexts.items():
            existing_hash = hashlib.md5(existing_content.encode()).hexdigest()
            if existing_hash == content_hash:
                self._metadata[context_id].update_access_stats()
                self._stats["hits"] += 1
                self._track_access_pattern(context_id)
                return context_id

        # If content doesn't exist, store binaries now
        for part in content.parts:
            if part.binary_data:
                binary_hash = hashlib.sha256(part.binary_data).hexdigest()
                self._store_binary_data(binary_hash, part.binary_data, part.mime_type)

        return self._store_structured_multimodal(
            content_data, metadata, skip_dedup_check=True
        )

    def _store_part(self, part, metadata):
        """Handle individual Part (text, image, audio, video)."""
        if part.text:
            part_data = {"type": "text", "data": part.text}
            return self._store_structured_multimodal(part_data, metadata)
        elif part.binary_data:
            binary_hash = hashlib.sha256(part.binary_data).hexdigest()
            part_data = {
                "type": "binary_ref",
                "mime_type": part.mime_type,
                "binary_hash": binary_hash,
                "size": len(part.binary_data),
            }

            # Check if this content already exists before storing binary
            content_str = json.dumps(part_data)
            content_hash = hashlib.md5(content_str.encode()).hexdigest()

            for context_id, existing_content in self._contexts.items():
                existing_hash = hashlib.md5(existing_content.encode()).hexdigest()
                if existing_hash == content_hash:
                    self._metadata[context_id].update_access_stats()
                    self._stats["hits"] += 1
                    self._track_access_pattern(context_id)
                    return context_id

            # If content doesn't exist, store binary now
            self._store_binary_data(binary_hash, part.binary_data, part.mime_type)
            return self._store_structured_multimodal(
                part_data, metadata, skip_dedup_check=True
            )
        elif part.file_uri:
            part_data = {
                "type": "file_ref",
                "file_uri": part.file_uri,
                "mime_type": part.mime_type,
            }
            return self._store_structured_multimodal(part_data, metadata)
        else:
            raise ValueError("Part contains no recognizable content")

    def _store_binary_data(self, binary_hash: str, data: bytes, mime_type: str) -> str:
        """Store binary data SEPARATELY from JSON context."""
        if binary_hash in self._binary_store:
            self._binary_metadata[binary_hash]["ref_count"] += 1
            return binary_hash

        if self._use_disk_storage and len(data) > self._large_binary_threshold:
            file_path = os.path.join(self._binary_cache_dir, f"{binary_hash}.bin")
            with open(file_path, "wb") as f:
                f.write(data)
            self._binary_store[binary_hash] = file_path
        else:
            self._binary_store[binary_hash] = data

        self._binary_metadata[binary_hash] = {
            "ref_count": 1,
            "mime_type": mime_type,
            "size": len(data),
            "created_at": time.time(),
            "is_disk_stored": len(data) > self._large_binary_threshold
            and self._use_disk_storage,
        }

        return binary_hash

    def _store_structured_multimodal(
        self,
        content_data: Dict,
        metadata: Optional[Dict[str, Any]],
        skip_dedup_check: bool = False,
    ) -> str:
        """Store structured multimodal data using existing infrastructure."""
        content_str = json.dumps(content_data)
        content_hash = hashlib.md5(content_str.encode()).hexdigest()

        if not skip_dedup_check:
            for context_id, existing_content in self._contexts.items():
                existing_hash = hashlib.md5(existing_content.encode()).hexdigest()
                if existing_hash == content_hash:
                    self._metadata[context_id].update_access_stats()
                    self._stats["hits"] += 1
                    self._track_access_pattern(context_id)
                    return context_id

        context_id = str(uuid.uuid4())
        self._stats["misses"] += 1
        self._contexts[context_id] = content_str

        meta = ContextMetadata(
            content_type="application/json+multimodal",
            token_count=len(content_str) // 4,
            tags=metadata.get("tags", []) if metadata else [],
            is_structured=True,
        )

        if metadata:
            if "priority" in metadata:
                meta.priority = metadata["priority"]
            if "cache_id" in metadata:
                meta.cache_id = metadata["cache_id"]
            else:
                meta.cache_id = f"multimodal_{content_hash[:16]}"
            if "cache_ttl" in metadata:
                meta.cached_until = time.time() + metadata["cache_ttl"]

        self._metadata[context_id] = meta
        self._track_access_pattern(context_id)

        if self._enable_cache_warming and self._should_warm_context(context_id):
            self._warmup_contexts.append(context_id)

        self._manage_cache()
        return context_id

    def retrieve(self, context_id: str) -> Any:
        """
        Retrieve context by its reference ID with automatic decompression.

        Args:
            context_id: The reference ID for the context

        Returns:
            The context content (string or structured data depending on how it was stored)
        """
        if context_id not in self._contexts:
            raise KeyError(f"Context ID {context_id} not found")

        # Check if expired
        if self._metadata[context_id].is_expired():
            self._evict_context(context_id)
            raise KeyError(f"Context ID {context_id} has expired")

        self._metadata[context_id].update_access_stats()
        self._track_access_pattern(context_id)
        self._stats["hits"] += 1
        stored_content = self._contexts[context_id]
        metadata = self._metadata[context_id]
        # Handle decompression if content was compressed
        content = stored_content
        if context_id in self._compression_metadata and self._compression_metadata[
            context_id
        ].get("is_compressed", False):

            compression_info = self._compression_metadata[context_id]
            algorithm = CompressionAlgorithm(compression_info["algorithm"])

            try:
                # Decode base64 compressed data
                import base64

                compressed_data = base64.b64decode(stored_content.encode("utf-8"))

                # Decompress using the original algorithm
                content = self._compression_manager.decompress(
                    compressed_data, algorithm
                )

            except Exception as e:
                # Decompression failed, log warning and return stored content
                import logging

                logging.warning(f"Decompression failed for context {context_id}: {e}")
                content = stored_content

        # If the content is JSON, parse it back
        if metadata.is_structured:
            try:
                return json.loads(content)
            except json.JSONDecodeError:
                return content

        return content

    def retrieve_multimodal_content(self, context_id: str):
        """Retrieve multimodal content with lazy loading of binary data."""
        if context_id not in self._contexts:
            raise KeyError(f"Context ID {context_id} not found")

        if self._metadata[context_id].is_expired():
            self._evict_context(context_id)
            raise KeyError(f"Context ID {context_id} has expired")

        self._metadata[context_id].update_access_stats()
        self._track_access_pattern(context_id)
        self._stats["hits"] += 1

        content_str = self._contexts[context_id]
        metadata = self._metadata[context_id]

        if metadata.content_type == "application/json+multimodal":
            content_data = json.loads(content_str)

            if "role" in content_data:
                return self._reconstruct_content(content_data)
            else:
                return self._reconstruct_part(content_data)
        else:
            return self.retrieve(context_id)

    def _reconstruct_content(self, content_data: Dict):
        """Reconstruct MultimodalContent from stored data."""
        parts = []
        for part_data in content_data.get("parts", []):
            if part_data["type"] == "text":
                parts.append(MultimodalPart.from_text(text=part_data["data"]))
            elif part_data["type"] == "binary_ref":
                parts.append(self._create_lazy_binary_part(part_data))
            elif part_data["type"] == "file_ref":
                parts.append(
                    MultimodalPart.from_file(
                        file_uri=part_data["file_uri"],
                        mime_type=part_data["mime_type"],
                    )
                )
        return MultimodalContent(role=content_data["role"], parts=parts)

    def _reconstruct_part(self, part_data: Dict):
        """Reconstruct MultimodalPart from stored data."""
        if part_data["type"] == "text":
            return MultimodalPart.from_text(text=part_data["data"])
        elif part_data["type"] == "binary_ref":
            return self._create_lazy_binary_part(part_data)
        elif part_data["type"] == "file_ref":
            return MultimodalPart.from_file(
                file_uri=part_data["file_uri"], mime_type=part_data["mime_type"]
            )
        else:
            raise ValueError(f"Unknown part type: {part_data['type']}")

    def _create_lazy_binary_part(self, part_data: Dict):
        """Create a MultimodalPart that loads binary data on demand."""
        binary_hash = part_data["binary_hash"]
        mime_type = part_data["mime_type"]
        binary_data = self._load_binary_data(binary_hash)
        return MultimodalPart.from_binary(data=binary_data, mime_type=mime_type)

    def _load_binary_data(self, binary_hash: str) -> bytes:
        """Load binary data from storage."""
        if binary_hash not in self._binary_store:
            raise KeyError(f"Binary hash {binary_hash} not found")

        stored_data = self._binary_store[binary_hash]

        if isinstance(stored_data, str):
            with open(stored_data, "rb") as f:
                return f.read()
        else:
            return stored_data

    def get_metadata(self, context_id: str) -> ContextMetadata:
        """Get metadata for a context."""
        if context_id not in self._metadata:
            raise KeyError(f"Context ID {context_id} not found")
        return self._metadata[context_id]

    def get_cache_hint(self, context_id: str) -> Dict[str, Any]:
        """
        Get a cache hint object for external caching systems (e.g., Gemini API).

        This allows external systems to cache the context for reuse.
        """
        if context_id not in self._metadata:
            raise KeyError(f"Context ID {context_id} not found")

        metadata = self._metadata[context_id]

        # Create cache hint with recommended parameters
        cache_hint = {
            "cache_id": metadata.cache_id,
            "cache_level": "HIGH",
        }

        # If we have a cached_until timestamp, add it
        if metadata.cached_until:
            now = time.time()
            if metadata.cached_until > now:
                # Still valid, calculate remaining TTL in seconds
                cache_hint["ttl_seconds"] = int(metadata.cached_until - now)

        return cache_hint

    def _manage_cache(self):
        """Manage the cache size using the selected eviction policy."""
        # Remove expired contexts
        self._evict_expired_contexts()

        # For memory pressure policy, check memory first
        if self._eviction_policy == CacheEvictionPolicy.MEMORY_PRESSURE:
            self._evict_by_memory_pressure()
        elif len(self._contexts) > self._cache_size:
            # Using the eviction policy
            if self._eviction_policy == CacheEvictionPolicy.LRU:
                self._evict_by_lru()
            elif self._eviction_policy == CacheEvictionPolicy.LFU:
                self._evict_by_lfu()
            elif self._eviction_policy == CacheEvictionPolicy.TTL:
                self._evict_by_ttl()

    def _evict_expired_contexts(self):
        """Remove contexts that have expired based on TTL."""
        expired_contexts = []
        for context_id, metadata in self._metadata.items():
            if metadata.is_expired():
                expired_contexts.append(context_id)

        for context_id in expired_contexts:
            self._evict_context(context_id)
            self._stats["ttl_evictions"] += 1

    def _evict_by_lru(self):
        """Evict contexts using Least Recently Used policy."""
        # Sort by priority first (ascending - low priority first), then by last accessed time (ascending)
        sorted_contexts = sorted(
            self._metadata.items(), key=lambda x: (x[1].priority, x[1].last_accessed)
        )

        # Remove oldest contexts until we're under the limit
        contexts_to_remove = len(self._contexts) - self._cache_size
        removed_count = 0

        for context_id, metadata in sorted_contexts:
            if removed_count >= contexts_to_remove:
                break

            # Don't evict warm contexts unless necessary
            if context_id not in self._warmup_contexts:
                self._evict_context(context_id)
                removed_count += 1

        # If we still need to remove more and only warm contexts remain
        if removed_count < contexts_to_remove:
            for context_id, metadata in sorted_contexts:
                if removed_count >= contexts_to_remove:
                    break
                if context_id in self._contexts:
                    self._evict_context(context_id)
                    removed_count += 1

    def _evict_by_lfu(self):
        """Evict contexts using Least Frequently Used policy."""
        # Sort by priority first (ascending - low priority first), then by frequency score (ascending)
        sorted_contexts = sorted(
            self._metadata.items(), key=lambda x: (x[1].priority, x[1].frequency_score)
        )

        # Remove least frequently used contexts
        contexts_to_remove = len(self._contexts) - self._cache_size
        removed_count = 0

        for context_id, metadata in sorted_contexts:
            if removed_count >= contexts_to_remove:
                break

            # Don't evict warm contexts unless necessary
            if context_id not in self._warmup_contexts:
                self._evict_context(context_id)
                removed_count += 1

        # If we still need to remove more and only warm contexts remain
        if removed_count < contexts_to_remove:
            for context_id, metadata in sorted_contexts:
                if removed_count >= contexts_to_remove:
                    break
                if context_id in self._contexts:
                    self._evict_context(context_id)
                    removed_count += 1

    def _evict_by_ttl(self):
        """Evict contexts based on TTL, removing those expiring soonest."""
        # Sort by priority first (ascending - low priority first), then by Time To Live (ascending)
        sorted_contexts = sorted(
            self._metadata.items(),
            key=lambda x: (x[1].priority, x[1].cached_until or float("inf")),
        )

        contexts_to_remove = len(self._contexts) - self._cache_size
        removed_count = 0

        for context_id, metadata in sorted_contexts:
            if removed_count >= contexts_to_remove:
                break
            self._evict_context(context_id)
            removed_count += 1

    def _evict_by_memory_pressure(self):
        """Evict contexts based on system memory pressure."""
        try:
            memory_percent = psutil.virtual_memory().percent / 100.0
            if memory_percent > self._memory_threshold:
                target_size = int(self._cache_size * 0.7)
                contexts_to_remove = len(self._contexts) - target_size
                if contexts_to_remove > 0:
                    # Use LRU for memory pressure eviction
                    sorted_contexts = sorted(
                        self._metadata.items(),
                        key=lambda x: (x[1].priority, x[1].last_accessed),
                    )
                    removed_count = 0
                    for context_id, metadata in sorted_contexts:
                        if removed_count >= contexts_to_remove:
                            break
                        self._evict_context(context_id)
                        self._stats["memory_pressure_evictions"] += 1
                        removed_count += 1
        except Exception:
            pass

        # After memory pressure eviction, still check regular cache size
        if len(self._contexts) > self._cache_size:
            self._evict_by_lru()

    def _evict_context(self, context_id: str):
        """Remove a context from the store."""
        if context_id in self._contexts:
            self._cleanup_binary_references(context_id)
            del self._contexts[context_id]
        if context_id in self._metadata:
            del self._metadata[context_id]
        if context_id in self._warmup_contexts:
            self._warmup_contexts.remove(context_id)
        if context_id in self._access_patterns:
            del self._access_patterns[context_id]
        self._stats["evictions"] += 1

    def _cleanup_binary_references(self, context_id: str):
        """Clean up binary references when evicting multimodal contexts."""
        if context_id not in self._contexts:
            return

        content_str = self._contexts[context_id]
        metadata = self._metadata.get(context_id)

        if metadata and metadata.content_type == "application/json+multimodal":
            try:
                content_data = json.loads(content_str)
                self._decrement_binary_refs(content_data)
            except (json.JSONDecodeError, KeyError):
                pass

    def _decrement_binary_refs(self, content_data: Dict):
        """Decrement reference counts for binary data."""
        if "parts" in content_data:
            for part_data in content_data["parts"]:
                if part_data.get("type") == "binary_ref":
                    self._decrement_binary_ref(part_data["binary_hash"])
        elif content_data.get("type") == "binary_ref":
            self._decrement_binary_ref(content_data["binary_hash"])

    def _decrement_binary_ref(self, binary_hash: str):
        """Decrement reference count for a binary hash and clean up if needed."""
        if binary_hash in self._binary_metadata:
            self._binary_metadata[binary_hash]["ref_count"] -= 1

            if self._binary_metadata[binary_hash]["ref_count"] <= 0:
                self._cleanup_binary_data(binary_hash)

    def _cleanup_binary_data(self, binary_hash: str):
        """Clean up binary data when no longer referenced."""
        if binary_hash in self._binary_store:
            stored_data = self._binary_store[binary_hash]

            if isinstance(stored_data, str):
                try:
                    os.remove(stored_data)
                except OSError:
                    pass

            del self._binary_store[binary_hash]

        if binary_hash in self._binary_metadata:
            del self._binary_metadata[binary_hash]

    def _track_access_pattern(self, context_id: str):
        """Track access patterns for cache warming."""
        if not self._enable_cache_warming:
            return

        current_time = time.time()
        if context_id not in self._access_patterns:
            self._access_patterns[context_id] = []

        # Keep only recent accesses (last hour)
        self._access_patterns[context_id] = [
            t for t in self._access_patterns[context_id] if current_time - t < 3600
        ]
        self._access_patterns[context_id].append(current_time)

    def _should_warm_context(self, context_id: str) -> bool:
        """Determine if a context should be kept warm."""
        if not self._enable_cache_warming:
            return False

        # Keep contexts warm if they're accessed frequently
        if context_id in self._access_patterns:
            recent_accesses = len(self._access_patterns[context_id])
            return recent_accesses > 3  # Warm if accessed more than 3 times recently

        return False

    def _start_ttl_cleanup(self):
        """Start background thread for TTL cleanup."""

        def cleanup_worker():
            while not self._stop_cleanup:
                time.sleep(self._ttl_check_interval)
                if not self._stop_cleanup:
                    self._evict_expired_contexts()

        self._cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        self._cleanup_thread.start()

    # Statistics and utility methods
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics."""
        total_accesses = self._stats["hits"] + self._stats["misses"]
        hit_rate = self._stats["hits"] / total_accesses if total_accesses > 0 else 0

        multimodal_contexts = sum(
            1
            for meta in self._metadata.values()
            if meta.content_type == "application/json+multimodal"
        )

        total_binary_size = sum(meta["size"] for meta in self._binary_metadata.values())

        disk_stored_binaries = sum(
            1
            for meta in self._binary_metadata.values()
            if meta.get("is_disk_stored", False)
        )

        try:
            memory_usage_percent = psutil.virtual_memory().percent
        except:
            memory_usage_percent = None

        return {
            "total_contexts": len(self._contexts),
            "multimodal_contexts": multimodal_contexts,
            "total_binary_objects": len(self._binary_store),
            "total_binary_size_bytes": total_binary_size,
            "disk_stored_binaries": disk_stored_binaries,
            "cache_size_limit": self._cache_size,
            "hit_rate": hit_rate,
            "total_hits": self._stats["hits"],
            "total_misses": self._stats["misses"],
            "total_evictions": self._stats["evictions"],
            "memory_pressure_evictions": self._stats["memory_pressure_evictions"],
            "ttl_evictions": self._stats["ttl_evictions"],
            "warm_contexts": len(self._warmup_contexts),
            "eviction_policy": self._eviction_policy.value,
            "memory_usage_percent": memory_usage_percent,
        }

    def set_context_priority(self, context_id: str, priority: int):
        """Set priority for a context (higher priority = kept longer)."""
        if context_id in self._metadata:
            self._metadata[context_id].priority = priority

    def warm_contexts(self, context_ids: List[str]):
        """Mark contexts as warm (should be kept in cache)."""
        for context_id in context_ids:
            if context_id in self._contexts and context_id not in self._warmup_contexts:
                self._warmup_contexts.append(context_id)

    def cleanup_unused_binaries(self):
        """Clean up binary data that is no longer referenced."""
        unused_hashes = []

        for binary_hash, metadata in self._binary_metadata.items():
            if metadata["ref_count"] <= 0:
                unused_hashes.append(binary_hash)

        for binary_hash in unused_hashes:
            self._cleanup_binary_data(binary_hash)

    def get_multimodal_stats(self) -> Dict[str, Any]:
        """Get detailed multimodal storage statistics."""
        memory_binaries = sum(
            1
            for meta in self._binary_metadata.values()
            if not meta.get("is_disk_stored", False)
        )

        memory_binary_size = sum(
            meta["size"]
            for meta in self._binary_metadata.values()
            if not meta.get("is_disk_stored", False)
        )

        disk_binary_size = sum(
            meta["size"]
            for meta in self._binary_metadata.values()
            if meta.get("is_disk_stored", False)
        )

        total_ref_count = sum(
            meta["ref_count"] for meta in self._binary_metadata.values()
        )
        dedup_ratio = len(self._binary_metadata) / max(1, total_ref_count)

        return {
            "memory_stored_binaries": memory_binaries,
            "memory_binary_size_bytes": memory_binary_size,
            "disk_stored_binaries": sum(
                1
                for meta in self._binary_metadata.values()
                if meta.get("is_disk_stored", False)
            ),
            "disk_binary_size_bytes": disk_binary_size,
            "binary_deduplication_ratio": dedup_ratio,
            "binary_cache_directory": self._binary_cache_dir,
            "large_binary_threshold": self._large_binary_threshold,
        }

    def clear_multimodal_cache(self):
        """Clear all multimodal binary data from cache."""
        for binary_hash in list(self._binary_store.keys()):
            self._cleanup_binary_data(binary_hash)

    def __del__(self):
        """Cleanup background threads and binary data."""
        try:
            if hasattr(self, "_stop_cleanup"):
                self._stop_cleanup = True
            if hasattr(self, "_cleanup_thread") and self._cleanup_thread:
                if self._cleanup_thread.is_alive():
                    self._cleanup_thread.join(timeout=1)
        except Exception:
            pass

        if hasattr(self, "_binary_store"):
            for binary_hash in list(self._binary_store.keys()):
                stored_data = self._binary_store[binary_hash]
                if isinstance(stored_data, str):
                    try:
                        os.remove(stored_data)
                    except OSError:
                        pass

    def get_compression_analytics(self) -> Dict[str, Any]:
        """
        Get comprehensive compression analytics and performance metrics.

        Returns:
            Dictionary containing compression statistics, savings, and insights
        """
        if not self._enable_compression or not self._compression_manager:
            return {
                "compression_enabled": False,
                "message": "Compression is disabled for this context store",
            }

        # Get compression manager analytics
        manager_analytics = self._compression_manager.get_compression_analytics()

        # Calculate context-specific compression stats
        total_contexts = len(self._contexts)
        compressed_contexts = sum(
            1
            for info in self._compression_metadata.values()
            if info.get("is_compressed", False)
        )

        compression_ratio = compressed_contexts / max(1, total_contexts)

        # Calculate total space savings
        total_original_size = sum(
            info.get("original_size", 0)
            for info in self._compression_metadata.values()
            if info.get("is_compressed", False)
        )
        total_compressed_size = sum(
            info.get("compressed_size", 0)
            for info in self._compression_metadata.values()
            if info.get("is_compressed", False)
        )

        space_saved_bytes = total_original_size - total_compressed_size
        space_saved_percentage = (space_saved_bytes / max(1, total_original_size)) * 100

        # Content type breakdown
        content_type_stats = {}
        for info in self._compression_metadata.values():
            if info.get("is_compressed", False):
                content_type = info.get("content_type", "unknown")
                if content_type not in content_type_stats:
                    content_type_stats[content_type] = {"count": 0, "avg_savings": 0}
                content_type_stats[content_type]["count"] += 1
                savings = info.get("space_savings_percent", 0)
                content_type_stats[content_type]["avg_savings"] = (
                    content_type_stats[content_type]["avg_savings"] + savings
                ) / content_type_stats[content_type]["count"]

        return {
            "compression_enabled": True,
            "context_store_stats": {
                "total_contexts": total_contexts,
                "compressed_contexts": compressed_contexts,
                "compression_adoption_rate": compression_ratio * 100,
                "total_space_saved_bytes": space_saved_bytes,
                "space_savings_percentage": space_saved_percentage,
                "content_type_breakdown": content_type_stats,
            },
            "compression_manager_analytics": manager_analytics,
            "performance_impact": {
                "storage_efficiency_multiplier": (
                    1 / (1 - space_saved_percentage / 100)
                    if space_saved_percentage > 0
                    else 1
                ),
                "estimated_memory_reduction": f"{space_saved_percentage:.1f}% reduction in context memory usage",
                "combined_with_reference_store": f"Total efficiency gain: {625 * (1 / (1 - space_saved_percentage / 100)) if space_saved_percentage > 0 else 625:.0f}x over traditional approaches",
            },
        }

    def get_compression_recommendations(self) -> Dict[str, Any]:
        """
        Get intelligent recommendations for optimizing compression settings.

        Returns:
            Dictionary with actionable optimization recommendations
        """
        if not self._enable_compression:
            return {
                "primary_recommendation": "Enable compression for significant space savings",
                "estimated_benefit": "10-80% storage reduction depending on content types",
            }

        analytics = self.get_compression_analytics()
        if "error" in analytics:
            return {"recommendations": ["No compression data available yet"]}

        recommendations = []
        context_stats = analytics["context_store_stats"]

        # Adoption rate recommendations
        if context_stats["compression_adoption_rate"] < 50:
            recommendations.append(
                {
                    "type": "configuration",
                    "priority": "high",
                    "recommendation": "Lower compression_min_size threshold to compress more content",
                    "current_adoption": f"{context_stats['compression_adoption_rate']:.1f}%",
                    "action": "Consider reducing min_compression_size from current setting",
                }
            )

        # Content type optimization
        content_breakdown = context_stats.get("content_type_breakdown", {})
        best_performing = (
            max(content_breakdown.items(), key=lambda x: x[1]["avg_savings"])
            if content_breakdown
            else None
        )
        worst_performing = (
            min(content_breakdown.items(), key=lambda x: x[1]["avg_savings"])
            if content_breakdown
            else None
        )

        if best_performing and worst_performing:
            recommendations.append(
                {
                    "type": "content_optimization",
                    "priority": "medium",
                    "recommendation": f"Optimize {worst_performing[0]} content preprocessing",
                    "details": {
                        "best_type": best_performing[0],
                        "best_savings": f"{best_performing[1]['avg_savings']:.1f}%",
                        "worst_type": worst_performing[0],
                        "worst_savings": f"{worst_performing[1]['avg_savings']:.1f}%",
                    },
                }
            )

        # Overall performance recommendation
        if context_stats["space_savings_percentage"] > 60:
            recommendations.append(
                {
                    "type": "performance",
                    "priority": "low",
                    "recommendation": "Excellent compression performance - consider sharing your configuration",
                    "achievement": f"{context_stats['space_savings_percentage']:.1f}% space savings",
                }
            )
        elif context_stats["space_savings_percentage"] < 30:
            recommendations.append(
                {
                    "type": "performance",
                    "priority": "high",
                    "recommendation": "Review content types and preprocessing strategies",
                    "current_savings": f"{context_stats['space_savings_percentage']:.1f}%",
                    "target": "Aim for 40-60% savings for most content types",
                }
            )

        return {
            "recommendations": recommendations,
            "optimization_score": min(
                100,
                context_stats["space_savings_percentage"]
                + context_stats["compression_adoption_rate"],
            )
            / 2,
            "next_steps": [
                "Monitor compression analytics regularly",
                "Experiment with different content preprocessing",
                "Consider algorithm-specific optimizations for your content types",
            ],
        }

    def create_token_aware_selection(
        self,
        model_name: str = "gemini-1.5-pro",
        target_tokens: Optional[int] = None,
        strategy: str = "balanced",
        query: str = "",
        keywords: List[str] = None,
    ) -> Dict[str, Any]:
        """
        Create a token-aware context selection using all stored contexts.

        Args:
            model_name: LLM model to optimize for
            target_tokens: Target token count (uses model default if None)
            strategy: Optimization strategy ("cost_first", "quality_first", "balanced", etc.)
            query: Query for relevance scoring
            keywords: Keywords for relevance scoring

        Returns:
            Dictionary with selected contexts and optimization metrics
        """
        if not TOKEN_OPTIMIZATION_AVAILABLE:
            return {
                "error": "Token optimization not available",
                "message": "Install tiktoken for token-aware features: pip install tiktoken",
            }

        if not self._contexts:
            return {
                "error": "No contexts available",
                "selected_contexts": [],
                "total_tokens": 0,
            }

        try:
            # Create token manager
            token_manager = create_token_manager(model_name)

            # Create budget
            budget = token_manager.create_budget(target_tokens=target_tokens)

            # Get all context content
            contexts = list(self._contexts.values())

            # Convert strategy string to enum
            strategy_map = {
                "cost_first": OptimizationStrategy.COST_FIRST,
                "quality_first": OptimizationStrategy.QUALITY_FIRST,
                "balanced": OptimizationStrategy.BALANCED,
                "speed_first": OptimizationStrategy.SPEED_FIRST,
                "comprehensive": OptimizationStrategy.COMPREHENSIVE,
            }

            strategy_enum = strategy_map.get(strategy, OptimizationStrategy.BALANCED)

            # Optimize selection
            result = token_manager.optimize_context_selection(
                contexts=contexts,
                budget=budget,
                strategy=strategy_enum,
                query=query,
                keywords=keywords or [],
            )

            # Map selected contexts back to IDs
            context_list = list(self._contexts.items())
            selected_context_ids = []

            for selected_candidate in result.selected_contexts:
                # Find matching context ID by content
                for context_id, content in context_list:
                    if content == selected_candidate.content:
                        selected_context_ids.append(context_id)
                        break

            return {
                "selected_context_ids": selected_context_ids,
                "selected_contexts": [c.content for c in result.selected_contexts],
                "total_tokens": result.total_tokens,
                "budget_utilization": result.budget_utilization,
                "estimated_cost": result.estimated_cost,
                "efficiency_score": result.efficiency_score,
                "optimization_strategy": result.optimization_strategy.value,
                "recommendations": result.recommendations,
                "model_name": model_name,
                "excluded_count": len(result.excluded_contexts),
            }

        except Exception as e:
            return {
                "error": f"Token optimization failed: {str(e)}",
                "selected_contexts": [],
                "total_tokens": 0,
            }

    def get_token_analytics(self, model_name: str = "gemini-1.5-pro") -> Dict[str, Any]:
        """
        Get token analytics for all stored contexts.

        Args:
            model_name: LLM model to analyze for

        Returns:
            Dictionary with token analytics and cost estimates
        """
        if not TOKEN_OPTIMIZATION_AVAILABLE:
            return {
                "error": "Token optimization not available",
                "message": "Install tiktoken for token analytics: pip install tiktoken",
            }

        if not self._contexts:
            return {"total_contexts": 0, "total_tokens": 0, "estimated_cost": 0.0}

        try:
            token_manager = create_token_manager(model_name)

            total_tokens = 0
            context_token_counts = {}

            for context_id, content in self._contexts.items():
                token_count = token_manager.count_tokens(content)
                total_tokens += token_count
                context_token_counts[context_id] = token_count

            # Calculate cost estimate
            estimated_cost = (
                total_tokens / 1000
            ) * token_manager.model_config.cost_per_1k_input_tokens

            # Calculate distribution statistics
            token_counts = list(context_token_counts.values())
            avg_tokens = total_tokens / len(token_counts) if token_counts else 0
            min_tokens = min(token_counts) if token_counts else 0
            max_tokens = max(token_counts) if token_counts else 0

            # Find largest contexts
            largest_contexts = sorted(
                context_token_counts.items(), key=lambda x: x[1], reverse=True
            )[:5]

            return {
                "model_name": model_name,
                "total_contexts": len(self._contexts),
                "total_tokens": total_tokens,
                "estimated_cost": estimated_cost,
                "average_tokens_per_context": avg_tokens,
                "min_tokens": min_tokens,
                "max_tokens": max_tokens,
                "largest_contexts": [
                    {"context_id": ctx_id, "tokens": tokens}
                    for ctx_id, tokens in largest_contexts
                ],
                "token_distribution": {
                    "small_contexts_under_1k": sum(1 for t in token_counts if t < 1000),
                    "medium_contexts_1k_10k": sum(
                        1 for t in token_counts if 1000 <= t < 10000
                    ),
                    "large_contexts_over_10k": sum(
                        1 for t in token_counts if t >= 10000
                    ),
                },
                "model_capacity_analysis": {
                    "max_model_tokens": token_manager.model_config.max_tokens,
                    "could_fit_all_contexts": total_tokens
                    <= token_manager.model_config.max_tokens,
                    "utilization_if_all_used": min(
                        1.0, total_tokens / token_manager.model_config.max_tokens
                    ),
                    "contexts_that_fit_individually": sum(
                        1
                        for t in token_counts
                        if t <= token_manager.model_config.max_tokens
                    ),
                },
            }

        except Exception as e:
            return {
                "error": f"Token analytics failed: {str(e)}",
                "total_contexts": len(self._contexts),
                "total_tokens": 0,
            }

    def analyze_semantic_patterns(
        self, similarity_threshold: float = 0.85, clustering_algorithm: str = "dbscan"
    ) -> Dict[str, Any]:
        """
        Perform semantic analysis on all stored contexts to identify patterns and optimization opportunities.

        Args:
            similarity_threshold: Threshold for considering contexts semantically similar
            clustering_algorithm: Algorithm to use for clustering ("dbscan", "kmeans", "hierarchical")

        Returns:
            Dictionary with semantic analysis results and recommendations
        """
        if not SEMANTIC_ANALYSIS_AVAILABLE:
            return {
                "error": "Semantic analysis not available",
                "message": "Install sentence-transformers for semantic features: pip install sentence-transformers",
            }

        if not self._contexts:
            return {
                "total_contexts": 0,
                "semantic_duplicates": 0,
                "clusters": 0,
                "space_savings_potential": 0.0,
            }

        try:
            # Create semantic analyzer
            analyzer = create_semantic_analyzer(
                similarity_threshold=similarity_threshold,
                clustering_algorithm=clustering_algorithm,
            )

            # Perform analysis
            result = analyzer.analyze_contexts(self._contexts)

            # Convert result to dictionary format
            return {
                "total_contexts_analyzed": result.total_contexts_analyzed,
                "semantic_duplicates_found": result.duplicates_found,
                "clusters_created": result.clusters_created,
                "space_savings_potential": result.space_savings_potential,
                "quality_improvement_potential": result.quality_improvement_potential,
                "processing_time_ms": result.processing_time_ms,
                "similarity_matches": [
                    {
                        "context_id_1": match.context_id_1,
                        "context_id_2": match.context_id_2,
                        "similarity_score": match.similarity_score,
                        "confidence": match.confidence,
                        "suggested_action": match.suggested_action,
                        "match_reasons": match.match_reasons,
                    }
                    for match in result.similarity_matches
                ],
                "clusters": [
                    {
                        "cluster_id": cluster.cluster_id,
                        "context_ids": cluster.context_ids,
                        "representative_context_id": cluster.representative_context_id,
                        "semantic_theme": cluster.semantic_theme,
                        "quality_score": cluster.quality_score,
                        "size": len(cluster.context_ids),
                    }
                    for cluster in result.clusters
                ],
                "recommendations": result.recommendations,
                "analyzer_configuration": {
                    "similarity_threshold": similarity_threshold,
                    "clustering_algorithm": clustering_algorithm,
                    "embedding_model": "sentence-transformers model (if available)",
                },
            }

        except Exception as e:
            return {
                "error": f"Semantic analysis failed: {str(e)}",
                "total_contexts": len(self._contexts),
                "semantic_duplicates": 0,
            }

    def find_similar_contexts(
        self,
        query_context: str,
        similarity_threshold: float = 0.7,
        max_results: int = 10,
    ) -> Dict[str, Any]:
        """
        Find contexts semantically similar to a query context.

        Args:
            query_context: The context to find similarities for
            similarity_threshold: Minimum similarity score to include in results
            max_results: Maximum number of results to return

        Returns:
            Dictionary with similar contexts and their similarity scores
        """
        if not SEMANTIC_ANALYSIS_AVAILABLE:
            return {
                "error": "Semantic analysis not available",
                "message": "Install sentence-transformers for semantic search: pip install sentence-transformers",
            }

        if not self._contexts:
            return {
                "query_context_length": len(query_context),
                "similar_contexts": [],
                "total_contexts_searched": 0,
            }

        try:
            # Create semantic analyzer
            analyzer = create_semantic_analyzer(
                similarity_threshold=0.0
            )  # Use 0.0 to get all similarities

            # Calculate similarities
            similarities = []

            for context_id, content in self._contexts.items():
                similarity = analyzer.calculate_similarity(query_context, content)

                if similarity >= similarity_threshold:
                    similarities.append(
                        {
                            "context_id": context_id,
                            "similarity_score": similarity,
                            "content_preview": (
                                content[:200] + "..." if len(content) > 200 else content
                            ),
                            "content_length": len(content),
                        }
                    )

            # Sort by similarity score and limit results
            similarities.sort(key=lambda x: x["similarity_score"], reverse=True)
            similar_contexts = similarities[:max_results]

            return {
                "query_context_length": len(query_context),
                "similar_contexts": similar_contexts,
                "total_contexts_searched": len(self._contexts),
                "total_similar_found": len(similarities),
                "returned_results": len(similar_contexts),
                "search_parameters": {
                    "similarity_threshold": similarity_threshold,
                    "max_results": max_results,
                },
            }

        except Exception as e:
            return {
                "error": f"Semantic search failed: {str(e)}",
                "query_context_length": len(query_context),
                "similar_contexts": [],
            }

    def optimize_semantic_storage(
        self, similarity_threshold: float = 0.9, dry_run: bool = True
    ) -> Dict[str, Any]:
        """
        Optimize storage by identifying and potentially merging semantically duplicate contexts.

        Args:
            similarity_threshold: Threshold for considering contexts duplicates
            dry_run: If True, only analyze without making changes

        Returns:
            Dictionary with optimization results and potential savings
        """
        if not SEMANTIC_ANALYSIS_AVAILABLE:
            return {
                "error": "Semantic optimization not available",
                "message": "Install sentence-transformers for semantic optimization: pip install sentence-transformers",
            }

        if not self._contexts:
            return {
                "total_contexts": 0,
                "duplicates_found": 0,
                "potential_savings": 0.0,
            }

        try:
            # Create semantic analyzer
            analyzer = create_semantic_analyzer(
                similarity_threshold=similarity_threshold
            )

            # Find semantic duplicates
            matches = analyzer.find_semantic_duplicates(self._contexts)

            # Analyze potential optimizations
            optimization_plan = []
            potential_savings = 0
            contexts_to_remove = set()

            for match in matches:
                if match.similarity_score >= similarity_threshold:
                    context_1_size = len(self._contexts[match.context_id_1])
                    context_2_size = len(self._contexts[match.context_id_2])

                    # Keep the longer context, remove the shorter one
                    if context_1_size >= context_2_size:
                        keep_context = match.context_id_1
                        remove_context = match.context_id_2
                        savings = context_2_size
                    else:
                        keep_context = match.context_id_2
                        remove_context = match.context_id_1
                        savings = context_1_size

                    if remove_context not in contexts_to_remove:
                        optimization_plan.append(
                            {
                                "action": "merge_duplicate",
                                "keep_context_id": keep_context,
                                "remove_context_id": remove_context,
                                "similarity_score": match.similarity_score,
                                "space_saved_bytes": savings,
                                "confidence": match.confidence,
                            }
                        )

                        potential_savings += savings
                        contexts_to_remove.add(remove_context)

            # Calculate savings percentage
            total_size = sum(len(content) for content in self._contexts.values())
            savings_percentage = (
                (potential_savings / total_size) * 100 if total_size > 0 else 0
            )

            # Execute optimization if not dry run
            actual_savings = 0
            if not dry_run and optimization_plan:
                for plan in optimization_plan:
                    remove_id = plan["remove_context_id"]
                    if remove_id in self._contexts:
                        actual_savings += len(self._contexts[remove_id])
                        del self._contexts[remove_id]
                        if remove_id in self._metadata:
                            del self._metadata[remove_id]
                        if remove_id in self._compression_metadata:
                            del self._compression_metadata[remove_id]

            return {
                "total_contexts_analyzed": len(self._contexts)
                + len(contexts_to_remove),
                "duplicates_found": len(optimization_plan),
                "optimization_plan": optimization_plan,
                "potential_savings_bytes": potential_savings,
                "potential_savings_percentage": savings_percentage,
                "actual_savings_bytes": actual_savings if not dry_run else 0,
                "dry_run": dry_run,
                "contexts_after_optimization": len(self._contexts),
                "similarity_threshold_used": similarity_threshold,
                "recommendations": [
                    (
                        "Run with dry_run=False to apply optimizations"
                        if dry_run and optimization_plan
                        else ""
                    ),
                    (
                        f"Consider lowering similarity threshold to find more duplicates"
                        if len(optimization_plan) < 5
                        else ""
                    ),
                    (
                        f"High similarity matches found - significant savings possible"
                        if savings_percentage > 20
                        else ""
                    ),
                ],
            }

        except Exception as e:
            return {
                "error": f"Semantic optimization failed: {str(e)}",
                "total_contexts": len(self._contexts),
                "duplicates_found": 0,
            }

    def get_semantic_insights(self) -> Dict[str, Any]:
        """
        Get insights about the semantic characteristics of stored contexts.

        Returns:
            Dictionary with semantic insights and patterns
        """
        if not SEMANTIC_ANALYSIS_AVAILABLE:
            return {
                "error": "Semantic insights not available",
                "message": "Install sentence-transformers for semantic insights: pip install sentence-transformers",
            }

        if not self._contexts:
            return {"total_contexts": 0, "insights": []}

        try:
            # Analyze content characteristics
            content_lengths = [len(content) for content in self._contexts.values()]
            content_types = self._analyze_content_types()

            # Basic semantic patterns
            insights = []

            # Length distribution insights
            avg_length = sum(content_lengths) / len(content_lengths)
            if avg_length > 5000:
                insights.append(
                    "Contexts tend to be quite long - consider compression or summarization"
                )
            elif avg_length < 500:
                insights.append(
                    "Contexts are generally short - good for quick processing"
                )

            # Content type insights
            if content_types.get("code_like", 0) > len(self._contexts) * 0.3:
                insights.append("High proportion of code-like content detected")

            if content_types.get("json_like", 0) > len(self._contexts) * 0.3:
                insights.append("High proportion of structured JSON data detected")
            # Diversity insights
            unique_words = set()
            for content in list(self._contexts.values())[
                :100
            ]:  # Sample first 100 contexts
                words = content.lower().split()
                unique_words.update(words)

            if len(unique_words) > 10000:
                insights.append(
                    "High vocabulary diversity - contexts cover diverse topics"
                )
            elif len(unique_words) < 1000:
                insights.append(
                    "Low vocabulary diversity - contexts may be similar in domain"
                )
            return {
                "total_contexts": len(self._contexts),
                "content_statistics": {
                    "average_length": avg_length,
                    "min_length": min(content_lengths) if content_lengths else 0,
                    "max_length": max(content_lengths) if content_lengths else 0,
                    "total_characters": sum(content_lengths),
                },
                "content_type_distribution": content_types,
                "vocabulary_diversity": {
                    "unique_words_sampled": len(unique_words),
                    "estimated_total_vocabulary": len(unique_words)
                    * (len(self._contexts) / min(100, len(self._contexts))),
                },
                "semantic_insights": insights,
                "recommendations": [
                    "Run full semantic analysis for detailed clustering and similarity detection",
                    "Consider semantic search capabilities for better content retrieval",
                    "Use semantic optimization to identify potential duplicates",
                ],
            }

        except Exception as e:
            return {
                "error": f"Semantic insights analysis failed: {str(e)}",
                "total_contexts": len(self._contexts),
            }

    def _analyze_content_types(self) -> Dict[str, int]:
        """Analyze the types of content stored."""
        content_types = {
            "json_like": 0,
            "xml_html_like": 0,
            "code_like": 0,
            "structured_text": 0,
            "plain_text": 0,
        }
        for content in self._contexts.values():
            content_lower = content.lower()
            # JSON-like detection
            if (content.strip().startswith("{") and content.strip().endswith("}")) or (
                content.strip().startswith("[") and content.strip().endswith("]")
            ):
                content_types["json_like"] += 1
            # XML/HTML detection
            elif "<" in content and ">" in content:
                content_types["xml_html_like"] += 1
            # Code detection
            elif any(
                keyword in content
                for keyword in [
                    "def ",
                    "class ",
                    "function",
                    "import ",
                    "from ",
                    "var ",
                    "const ",
                ]
            ):
                content_types["code_like"] += 1
            elif any(
                pattern in content
                for pattern in ["# ", "## ", "* ", "- ", "1. ", "2. "]
            ):
                content_types["structured_text"] += 1
            else:
                content_types["plain_text"] += 1

        return content_types
