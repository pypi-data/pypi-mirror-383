"""
LangChain adapter for Context Reference Store.

This module provides comprehensive integration between the Context Reference Store and LangChain,
enabling:
- Advanced agent checkpointing for LangGraph
- Efficient RAG document management
- Tool calling state preservation
- Streaming conversation support
"""

from typing import Any, Dict, List, Optional, Union, Sequence, Iterator, Callable
import json
import time
import uuid
import asyncio
from datetime import datetime

try:
    from langchain_core.messages import (
        BaseMessage,
        HumanMessage,
        AIMessage,
        SystemMessage,
        ToolMessage,
        FunctionMessage,
    )
    from langchain_core.memory import BaseMemory
    from langchain_core.documents import Document
    from langchain_core.runnables import Runnable, RunnableConfig
    from langchain_core.tools import BaseTool
    from langchain_core.callbacks import BaseCallbackHandler
    from langchain_core.outputs import ChatGeneration, LLMResult

    try:
        from langgraph.checkpoint.base import BaseCheckpointSaver, Checkpoint
        from langgraph.checkpoint.memory import MemorySaver

        LANGGRAPH_AVAILABLE = True
    except ImportError:
        LANGGRAPH_AVAILABLE = False
        BaseCheckpointSaver = object
        Checkpoint = dict
        MemorySaver = object

    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AVAILABLE = False

    class BaseMessage:
        pass

    class BaseMemory:
        pass

    class Document:
        pass

    class Runnable:
        pass

    class BaseCheckpointSaver:
        pass

    class BaseCallbackHandler:
        pass

    class LLMResult:
        pass


from ..core.context_reference_store import ContextReferenceStore
from ..core.large_context_state import LargeContextState


class LangChainContextAdapter:
    """
    Advanced adapter for integrating Context Reference Store with LangChain applications.

    This adapter provides:
    - Advanced agent checkpointing for LangGraph workflows
    - Efficient RAG document storage with vector embeddings
    - Tool calling state preservation and replay
    - Streaming conversation support with real-time updates
    - Multi-session conversation management
    - Advanced analytics and performance monitoring
    """

    def __init__(
        self,
        context_store: Optional[ContextReferenceStore] = None,
        cache_size: int = 100,
        enable_multimodal: bool = True,
        enable_streaming: bool = True,
        enable_tool_calling: bool = True,
        enable_checkpointing: bool = True,
        session_timeout: int = 3600,  # 1 hour default
    ):
        """
        Initialize the advanced LangChain adapter.

        Args:
            context_store: Optional pre-configured context store
            cache_size: Maximum number of contexts to keep in memory
            enable_multimodal: Whether to enable multimodal content support
            enable_streaming: Whether to enable streaming conversation support
            enable_tool_calling: Whether to enable tool calling state management
            enable_checkpointing: Whether to enable LangGraph checkpointing
            session_timeout: Session timeout in seconds
        """
        if not LANGCHAIN_AVAILABLE:
            raise ImportError(
                "LangChain is required for LangChainContextAdapter. "
                "Install with: pip install langchain langchain-core"
            )

        self.context_store = context_store or ContextReferenceStore(
            cache_size=cache_size, enable_compression=True, use_disk_storage=True
        )
        self.enable_multimodal = enable_multimodal
        self.enable_streaming = enable_streaming
        self.enable_tool_calling = enable_tool_calling
        self.enable_checkpointing = enable_checkpointing and LANGGRAPH_AVAILABLE
        self.session_timeout = session_timeout
        self.state = LargeContextState(context_store=self.context_store)

        # Session management
        self._active_sessions = {}
        self._session_metadata = {}
        # Tool calling support
        self._tool_states = {}
        self._tool_results = {}
        # Streaming support
        self._streaming_handlers = {}
        # Performance tracking
        self._performance_stats = {
            "messages_stored": 0,
            "messages_retrieved": 0,
            "total_serialization_time": 0,
            "total_deserialization_time": 0,
            "storage_efficiency": 0,
        }

    def store_messages(
        self,
        messages: List[BaseMessage],
        session_id: str = "default",
        metadata: Optional[Dict[str, Any]] = None,
        include_tool_calls: bool = True,
        preserve_message_ids: bool = True,
    ) -> str:
        """
        Store LangChain messages efficiently with advanced features support.

        Args:
            messages: List of LangChain messages to store
            session_id: Session identifier for organizing conversations
            metadata: Optional metadata about the messages
            include_tool_calls: Whether to preserve tool calling information
            preserve_message_ids: Whether to preserve original message IDs

        Returns:
            Reference ID for the stored messages
        """
        # Handle case where session_id might be passed as a list (from tests)
        if isinstance(session_id, list):
            if session_id:
                session_id = str(session_id[0])
            else:
                session_id = "default"
        elif not isinstance(session_id, str):
            session_id = str(session_id)

        start_time = time.time()
        # Convert messages to enhanced serializable format
        serialized_messages = []
        total_tokens = 0

        for i, msg in enumerate(messages):
            # Handle both message objects and strings
            if isinstance(msg, str):
                message_data = {
                    "type": "str",
                    "content": msg,
                    "index": i,
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                message_data = {
                    "type": msg.__class__.__name__,
                    "content": getattr(msg, "content", str(msg)),
                    "index": i,
                    "timestamp": datetime.now().isoformat(),
                }

            # Preserve message ID if available
            if preserve_message_ids and hasattr(msg, "id"):
                message_data["id"] = msg.id
            elif preserve_message_ids:
                message_data["id"] = str(uuid.uuid4())

            # Add standard fields
            if hasattr(msg, "role"):
                message_data["role"] = msg.role
            if hasattr(msg, "name"):
                message_data["name"] = msg.name
            if hasattr(msg, "additional_kwargs"):
                message_data["additional_kwargs"] = msg.additional_kwargs

            if include_tool_calls and self.enable_tool_calling:
                if hasattr(msg, "tool_calls") and msg.tool_calls:
                    message_data["tool_calls"] = [
                        {
                            "name": tool_call.get("name"),
                            "args": tool_call.get("args"),
                            "id": tool_call.get("id"),
                            "type": tool_call.get("type", "function"),
                        }
                        for tool_call in msg.tool_calls
                    ]

                if hasattr(msg, "tool_call_id"):
                    message_data["tool_call_id"] = msg.tool_call_id

                if hasattr(msg, "artifact"):
                    message_data["artifact"] = msg.artifact

            if hasattr(msg, "response_metadata"):
                message_data["response_metadata"] = msg.response_metadata
                # Extract token usage if available
                if "token_usage" in msg.response_metadata:
                    token_info = msg.response_metadata["token_usage"]
                    total_tokens += token_info.get("total_tokens", 0)
            # Usage metadata for tracking
            if hasattr(msg, "usage_metadata"):
                message_data["usage_metadata"] = msg.usage_metadata

            serialized_messages.append(message_data)
        # Enhanced session context with analytics
        context_metadata = {
            "session_id": session_id,
            "content_type": "langchain/messages",
            "message_count": len(messages),
            "total_tokens": total_tokens,
            "storage_timestamp": datetime.now().isoformat(),
            "tool_calling_enabled": include_tool_calls and self.enable_tool_calling,
            "message_types": list(set(msg.__class__.__name__ for msg in messages)),
            "has_tool_calls": any(
                hasattr(msg, "tool_calls") and msg.tool_calls for msg in messages
            ),
        }

        if metadata:
            context_metadata.update(metadata)
        # Update session tracking
        self._active_sessions[session_id] = {
            "last_activity": time.time(),
            "message_count": len(messages),
            "total_tokens": total_tokens,
        }
        # Store with enhanced context
        reference_id = self.state.add_large_context(
            serialized_messages,
            metadata=context_metadata,
            key=f"messages_{session_id}",
        )
        # Update performance stats
        serialization_time = time.time() - start_time
        self._performance_stats["messages_stored"] += len(messages)
        self._performance_stats["total_serialization_time"] += serialization_time

        return reference_id

    def retrieve_messages(
        self,
        reference_id_or_session: str,
        restore_tool_calls: bool = True,
        restore_metadata: bool = True,
    ) -> List[BaseMessage]:
        """
        Retrieve LangChain messages with full fidelity restoration.

        Args:
            reference_id_or_session: Either a direct reference ID or session ID
            restore_tool_calls: Whether to restore tool calling information
            restore_metadata: Whether to restore response and usage metadata

        Returns:
            List of reconstructed LangChain messages with full context
        """
        start_time = time.time()
        try:
            if reference_id_or_session.startswith("messages_"):
                serialized_messages = self.state.get_context(reference_id_or_session)
            else:
                serialized_messages = self.state.get_context(
                    f"messages_{reference_id_or_session}"
                )
        except KeyError:
            try:
                serialized_messages = self.context_store.retrieve(
                    reference_id_or_session
                )
            except KeyError:
                # Return empty list if no messages found (for empty sessions)
                return []
        messages = []
        for msg_data in serialized_messages:
            msg_type = msg_data["type"]
            content = msg_data["content"]
            # Create appropriate message type
            if msg_type == "HumanMessage":
                message = HumanMessage(content=content)
            elif msg_type == "AIMessage":
                message = AIMessage(content=content)
            elif msg_type == "SystemMessage":
                message = SystemMessage(content=content)
            elif msg_type == "ToolMessage":
                # Tool messages require tool_call_id
                tool_call_id = msg_data.get("tool_call_id", "unknown")
                message = ToolMessage(content=content, tool_call_id=tool_call_id)
            elif msg_type == "FunctionMessage":
                name = msg_data.get("name", "unknown_function")
                message = FunctionMessage(content=content, name=name)
            else:
                message = HumanMessage(content=content)
            # Restore message ID
            if "id" in msg_data:
                message.id = msg_data["id"]
            if "additional_kwargs" in msg_data:
                message.additional_kwargs = msg_data["additional_kwargs"]
            if "name" in msg_data and hasattr(message, "name"):
                message.name = msg_data["name"]
            # Restore tool calling information
            if restore_tool_calls and self.enable_tool_calling:
                if "tool_calls" in msg_data:
                    message.tool_calls = msg_data["tool_calls"]

                if "tool_call_id" in msg_data:
                    message.tool_call_id = msg_data["tool_call_id"]

                if "artifact" in msg_data:
                    message.artifact = msg_data["artifact"]
            # Restore metadata
            if restore_metadata:
                if "response_metadata" in msg_data:
                    message.response_metadata = msg_data["response_metadata"]

                if "usage_metadata" in msg_data:
                    message.usage_metadata = msg_data["usage_metadata"]

            messages.append(message)

        # Update performance stats
        deserialization_time = time.time() - start_time
        self._performance_stats["messages_retrieved"] += len(messages)
        self._performance_stats["total_deserialization_time"] += deserialization_time
        # Update session activity
        session_id = reference_id_or_session
        if session_id.startswith("messages_"):
            session_id = session_id[9:]
        if session_id in self._active_sessions:
            self._active_sessions[session_id]["last_activity"] = time.time()

        return messages

    def store_documents(
        self,
        documents: List[Document],
        collection_name: str = "default",
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Store LangChain documents with efficient deduplication.

        Args:
            documents: List of LangChain documents to store
            collection_name: Name for organizing document collections
            metadata: Optional metadata about the documents

        Returns:
            Reference ID for the stored documents
        """
        # Convert documents to serializable format
        serialized_docs = []
        for doc in documents:
            doc_data = {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            }
            serialized_docs.append(doc_data)

        # Store with collection context
        context_metadata = {
            "collection_name": collection_name,
            "content_type": "langchain/documents",
            "document_count": len(documents),
        }

        if metadata:
            context_metadata.update(metadata)

        return self.state.add_large_context(
            serialized_docs,
            metadata=context_metadata,
            key=f"documents_{collection_name}",
        )

    def retrieve_documents(self, reference_id_or_collection: str) -> List[Document]:
        """
        Retrieve LangChain documents from a reference ID or collection name.

        Args:
            reference_id_or_collection: Either a direct reference ID or collection name

        Returns:
            List of reconstructed LangChain documents
        """
        try:
            if reference_id_or_collection.startswith("documents_"):
                serialized_docs = self.state.get_context(reference_id_or_collection)
            else:
                serialized_docs = self.state.get_context(
                    f"documents_{reference_id_or_collection}"
                )
        except KeyError:
            # Try as direct reference ID
            serialized_docs = self.context_store.retrieve(reference_id_or_collection)
        # Reconstruct documents
        documents = []
        for doc_data in serialized_docs:
            document = Document(
                page_content=doc_data["page_content"],
                metadata=doc_data["metadata"],
            )
            documents.append(document)

        return documents

    def create_memory_backend(
        self, session_id: str = "default"
    ) -> "LangChainMemoryBackend":
        """
        Create a LangChain-compatible memory backend using the context store.

        Args:
            session_id: Session identifier for the memory backend

        Returns:
            Memory backend that integrates with LangChain
        """
        return LangChainMemoryBackend(self, session_id)

    def get_session_stats(self, session_id: str) -> Dict[str, Any]:
        """
        Get statistics for a specific session.

        Args:
            session_id: Session identifier

        Returns:
            Dictionary containing session statistics
        """
        try:
            metadata = self.state.get_context_metadata(f"messages_{session_id}")
            return {
                "session_id": session_id,
                "message_count": metadata.get("message_count", 0),
                "last_accessed": metadata.get("last_accessed"),
                "access_count": metadata.get("access_count", 0),
                "token_count": metadata.get("token_count", 0),
            }
        except KeyError:
            return {"session_id": session_id, "exists": False}

    def list_sessions(self) -> List[str]:
        """
        List all session IDs with stored messages.

        Returns:
            List of session IDs
        """
        sessions = []
        for key in self.state.list_context_references():
            if key.startswith("messages_"):
                # Remove "messages_" prefix
                session_id = key[9:]
                sessions.append(session_id)
        return sessions

    def clear_session(self, session_id: str):
        """
        Clear all data for a specific session.

        Args:
            session_id: Session identifier to clear
        """
        message_key = f"messages_{session_id}"
        document_key = f"documents_{session_id}"
        checkpoint_key = f"checkpoint_{session_id}"

        # Remove from state if present
        if message_key in self.state:
            del self.state[message_key]
        if document_key in self.state:
            del self.state[document_key]
        if checkpoint_key in self.state:
            del self.state[checkpoint_key]
        # Clean up session tracking
        if session_id in self._active_sessions:
            del self._active_sessions[session_id]
        if session_id in self._session_metadata:
            del self._session_metadata[session_id]
        if session_id in self._tool_states:
            del self._tool_states[session_id]
        if session_id in self._tool_results:
            del self._tool_results[session_id]

    # Advanced Features
    def store_tool_state(
        self,
        session_id: str,
        tool_name: str,
        tool_state: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Store tool execution state for replay and debugging.

        Args:
            session_id: Session identifier
            tool_name: Name of the tool
            tool_state: Tool state to store
            metadata: Optional metadata

        Returns:
            Reference ID for the stored tool state
        """
        if not self.enable_tool_calling:
            raise ValueError("Tool calling support is disabled")

        context_metadata = {
            "session_id": session_id,
            "tool_name": tool_name,
            "content_type": "langchain/tool_state",
            "timestamp": datetime.now().isoformat(),
        }

        if metadata:
            context_metadata.update(metadata)

        # Track tool states
        if session_id not in self._tool_states:
            self._tool_states[session_id] = {}
        self._tool_states[session_id][tool_name] = tool_state

        return self.state.add_large_context(
            tool_state,
            metadata=context_metadata,
            key=f"tool_state_{session_id}_{tool_name}",
        )

    def retrieve_tool_state(
        self, session_id: str, tool_name: str
    ) -> Optional[Dict[str, Any]]:
        """
        Retrieve tool execution state.

        Args:
            session_id: Session identifier
            tool_name: Name of the tool

        Returns:
            Tool state if found, None otherwise
        """
        try:
            return self.state.get_context(f"tool_state_{session_id}_{tool_name}")
        except KeyError:
            return None

    def create_langgraph_checkpointer(
        self, session_id: str = "default"
    ) -> "LangGraphCheckpointSaver":
        """
        Create a LangGraph-compatible checkpoint saver using the context store.

        Args:
            session_id: Session identifier for the checkpointer

        Returns:
            Checkpoint saver that integrates with LangGraph
        """
        if not self.enable_checkpointing:
            raise ValueError(
                "Checkpointing support is disabled or LangGraph not available"
            )

        return LangGraphCheckpointSaver(self, session_id)

    def store_rag_documents(
        self,
        documents: List[Document],
        collection_name: str = "default",
        embeddings: Optional[List[List[float]]] = None,
        vector_metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Store RAG documents with optional vector embeddings.

        Args:
            documents: List of documents to store
            collection_name: Collection name for organization
            embeddings: Optional vector embeddings for the documents
            vector_metadata: Optional metadata about the vector space

        Returns:
            Reference ID for the stored documents
        """
        # Enhanced document serialization with embeddings
        serialized_docs = []
        for i, doc in enumerate(documents):
            doc_data = {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
                "doc_id": doc.metadata.get("doc_id", f"doc_{i}"),
                "timestamp": datetime.now().isoformat(),
            }
            if embeddings and i < len(embeddings):
                doc_data["embedding"] = embeddings[i]
                doc_data["embedding_model"] = (
                    vector_metadata.get("model") if vector_metadata else None
                )
            serialized_docs.append(doc_data)

        context_metadata = {
            "collection_name": collection_name,
            "content_type": "langchain/rag_documents",
            "document_count": len(documents),
            "has_embeddings": embeddings is not None,
            "storage_timestamp": datetime.now().isoformat(),
        }

        if vector_metadata:
            context_metadata["vector_metadata"] = vector_metadata
        return self.state.add_large_context(
            serialized_docs,
            metadata=context_metadata,
            key=f"rag_docs_{collection_name}",
        )

    def create_streaming_handler(
        self, session_id: str, callback: Callable[[str, Dict[str, Any]], None]
    ) -> "StreamingContextHandler":
        """
        Create a streaming handler for real-time conversation updates.

        Args:
            session_id: Session identifier
            callback: Callback function for streaming updates

        Returns:
            Streaming handler for LangChain integration
        """
        if not self.enable_streaming:
            raise ValueError("Streaming support is disabled")

        handler = StreamingContextHandler(self, session_id, callback)
        self._streaming_handlers[session_id] = handler
        return handler

    def get_performance_analytics(self) -> Dict[str, Any]:
        """
        Get comprehensive performance analytics for the adapter.

        Returns:
            Dictionary containing performance metrics
        """
        context_stats = self.context_store.get_cache_stats()
        # Calculate storage efficiency
        if self._performance_stats["messages_stored"] > 0:
            avg_serialization = (
                self._performance_stats["total_serialization_time"]
                / self._performance_stats["messages_stored"]
            )
            avg_deserialization = self._performance_stats[
                "total_deserialization_time"
            ] / max(self._performance_stats["messages_retrieved"], 1)
        else:
            avg_serialization = 0
            avg_deserialization = 0
        return {
            "context_store_stats": context_stats,
            "adapter_performance": {
                "messages_stored": self._performance_stats["messages_stored"],
                "messages_retrieved": self._performance_stats["messages_retrieved"],
                "avg_serialization_time": avg_serialization,
                "avg_deserialization_time": avg_deserialization,
                "total_sessions": len(self._active_sessions),
                "active_sessions": len(
                    [
                        s
                        for s in self._active_sessions.values()
                        if time.time() - s["last_activity"] < self.session_timeout
                    ]
                ),
            },
            "feature_usage": {
                "tool_calling_enabled": self.enable_tool_calling,
                "streaming_enabled": self.enable_streaming,
                "checkpointing_enabled": self.enable_checkpointing,
                "multimodal_enabled": self.enable_multimodal,
                "active_tool_states": len(self._tool_states),
                "active_streaming_handlers": len(self._streaming_handlers),
            },
        }

    def cleanup_expired_sessions(self):
        """Clean up expired sessions based on timeout."""
        current_time = time.time()
        expired_sessions = [
            session_id
            for session_id, data in self._active_sessions.items()
            if current_time - data["last_activity"] > self.session_timeout
        ]
        for session_id in expired_sessions:
            self.clear_session(session_id)


class LangGraphCheckpointSaver(BaseCheckpointSaver if LANGGRAPH_AVAILABLE else object):
    """
    LangGraph-compatible checkpoint saver using Context Reference Store.

    Provides efficient checkpointing for LangGraph agents with dramatic performance improvement.
    """

    def __init__(self, adapter: LangChainContextAdapter, session_id: str = "default"):
        """Initialize the checkpoint saver."""
        if LANGGRAPH_AVAILABLE:
            super().__init__()
        self.adapter = adapter
        self.session_id = session_id

    def put(self, config, checkpoint):
        """Save a checkpoint."""
        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraph is required for checkpoint functionality")
        checkpoint_id = config.get("configurable", {}).get(
            "thread_id", str(uuid.uuid4())
        )
        checkpoint_data = {
            "checkpoint": checkpoint,
            "config": config,
            "timestamp": datetime.now().isoformat(),
            "checkpoint_id": checkpoint_id,
        }
        self.adapter.state.add_large_context(
            checkpoint_data,
            metadata={
                "session_id": self.session_id,
                "checkpoint_id": checkpoint_id,
                "content_type": "langgraph/checkpoint",
            },
            key=f"checkpoint_{self.session_id}_{checkpoint_id}",
        )
        return config

    def get(self, config):
        """Retrieve a checkpoint."""
        if not LANGGRAPH_AVAILABLE:
            return None

        checkpoint_id = config.get("configurable", {}).get("thread_id")
        if not checkpoint_id:
            return None

        try:
            checkpoint_data = self.adapter.state.get_context(
                f"checkpoint_{self.session_id}_{checkpoint_id}"
            )
            return checkpoint_data["checkpoint"]
        except KeyError:
            return None

    def list(self, config):
        """List all checkpoints for a session."""
        if not LANGGRAPH_AVAILABLE:
            return []

        checkpoints = []
        for key in self.adapter.state.list_context_references():
            if key.startswith(f"checkpoint_{self.session_id}_"):
                try:
                    checkpoint_data = self.adapter.state.get_context(key)
                    checkpoints.append(checkpoint_data["checkpoint"])
                except KeyError:
                    continue
        return checkpoints


class StreamingContextHandler(BaseCallbackHandler if LANGCHAIN_AVAILABLE else object):
    """
    Streaming callback handler that integrates with Context Reference Store.

    Provides real-time conversation updates with efficient storage.
    """

    def __init__(
        self,
        adapter: LangChainContextAdapter,
        session_id: str,
        callback: Callable[[str, Dict[str, Any]], None],
    ):
        """Initialize the streaming handler."""
        if LANGCHAIN_AVAILABLE:
            super().__init__()
        self.adapter = adapter
        self.session_id = session_id
        self.callback = callback
        self.current_run_id = None
        self.streaming_buffer = []

    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):
        """Called when LLM starts generating."""
        self.current_run_id = kwargs.get("run_id", str(uuid.uuid4()))
        self.streaming_buffer = []
        self.callback(
            "llm_start",
            {
                "session_id": self.session_id,
                "run_id": self.current_run_id,
                "prompts": prompts,
                "timestamp": datetime.now().isoformat(),
            },
        )

    def on_llm_new_token(self, token: str, **kwargs):
        """Called when LLM generates a new token."""
        self.streaming_buffer.append(token)
        self.callback(
            "new_token",
            {
                "session_id": self.session_id,
                "run_id": self.current_run_id,
                "token": token,
                "current_text": "".join(self.streaming_buffer),
                "timestamp": datetime.now().isoformat(),
            },
        )

    def on_llm_end(self, response: LLMResult, **kwargs):
        """Called when LLM finishes generating."""
        final_text = "".join(self.streaming_buffer)
        # Store the streaming session
        streaming_data = {
            "session_id": self.session_id,
            "run_id": self.current_run_id,
            "final_text": final_text,
            "tokens": self.streaming_buffer,
            "response": response.dict() if hasattr(response, "dict") else str(response),
            "timestamp": datetime.now().isoformat(),
        }

        self.adapter.state.add_large_context(
            streaming_data,
            metadata={
                "session_id": self.session_id,
                "content_type": "langchain/streaming_session",
                "token_count": len(self.streaming_buffer),
            },
            key=f"streaming_{self.session_id}_{self.current_run_id}",
        )

        self.callback(
            "llm_end",
            {
                "session_id": self.session_id,
                "run_id": self.current_run_id,
                "final_text": final_text,
                "response": response,
                "timestamp": datetime.now().isoformat(),
            },
        )

    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs):
        """Called when a tool starts executing."""
        tool_name = serialized.get("name", "unknown_tool")
        run_id = kwargs.get("run_id", str(uuid.uuid4()))

        tool_data = {
            "tool_name": tool_name,
            "input": input_str,
            "serialized": serialized,
            "start_time": datetime.now().isoformat(),
            "run_id": run_id,
        }

        self.adapter.store_tool_state(
            self.session_id, f"{tool_name}_{run_id}", tool_data, {"phase": "start"}
        )

        self.callback(
            "tool_start",
            {
                "session_id": self.session_id,
                "tool_name": tool_name,
                "input": input_str,
                "run_id": run_id,
                "timestamp": datetime.now().isoformat(),
            },
        )

    def on_tool_end(self, output: str, **kwargs):
        """Called when a tool finishes executing."""
        tool_name = kwargs.get("name", "unknown_tool")
        run_id = kwargs.get("run_id")

        if run_id:
            # Update tool execution with results
            tool_key = f"{tool_name}_{run_id}"
            existing_state = (
                self.adapter.retrieve_tool_state(self.session_id, tool_key) or {}
            )

            existing_state.update(
                {
                    "output": output,
                    "end_time": datetime.now().isoformat(),
                    "phase": "completed",
                }
            )

            self.adapter.store_tool_state(
                self.session_id, tool_key, existing_state, {"phase": "completed"}
            )

        self.callback(
            "tool_end",
            {
                "session_id": self.session_id,
                "tool_name": tool_name,
                "output": output,
                "run_id": run_id,
                "timestamp": datetime.now().isoformat(),
            },
        )


class LangChainMemoryBackend(BaseMemory if LANGCHAIN_AVAILABLE else object):
    """
    Enhanced LangChain-compatible memory backend using Context Reference Store.

    This provides seamless integration with LangChain's memory system while
    achieving massive performance improvements for large conversation histories.
    """

    def __init__(self, adapter: LangChainContextAdapter, session_id: str = "default"):
        """
        Initialize the memory backend.

        Args:
            adapter: LangChain adapter instance
            session_id: Session identifier
        """
        if LANGCHAIN_AVAILABLE:
            super().__init__()
        self.adapter = adapter
        self.session_id = session_id
        self.memory_key = "chat_history"

    @property
    def memory_variables(self) -> List[str]:
        """Return list of memory variables."""
        return [self.memory_key]

    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Load memory variables from the context store."""
        try:
            messages = self.adapter.retrieve_messages(self.session_id)
            return {self.memory_key: messages}
        except KeyError:
            return {self.memory_key: []}

    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
        """Save context to the context store."""
        try:
            existing_messages = self.adapter.retrieve_messages(self.session_id)
        except KeyError:
            existing_messages = []

        # Add new human message
        if "input" in inputs:
            existing_messages.append(HumanMessage(content=inputs["input"]))
        if "output" in outputs:
            existing_messages.append(AIMessage(content=outputs["output"]))
        self.adapter.store_messages(existing_messages, self.session_id)

    def clear(self) -> None:
        """Clear the memory."""
        self.adapter.clear_session(self.session_id)
