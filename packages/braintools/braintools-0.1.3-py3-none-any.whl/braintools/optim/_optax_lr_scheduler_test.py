# Copyright 2025 BrainX Ecosystem Limited. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================


import unittest

import brainstate as bst

import braintools as bts


class SimpleModel(bst.nn.Module):
    """Simple model for testing."""

    def __init__(self, in_features=10, out_features=5):
        super().__init__()
        self.linear = bst.nn.Linear(in_features, out_features)

    def __call__(self, x):
        return self.linear(x)


class test_polynomial_warmup_schedulers(unittest.TestCase):

    # ============================================================================
    # PolynomialLR Tests
    # ============================================================================

    def test_polynomiallr_linear_decay(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PolynomialLR(base_lr=0.1, total_iters=100, power=1.0)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # At halfway point (epoch 50), should be approximately half of initial lr
        for _ in range(50):
            scheduler.step()

        expected_lr = 0.1 * ((1 - 50 / 100) ** 1.0)  # = 0.05
        assert abs(optimizer.current_lr - expected_lr) < 1e-6
        print("[OK] test_polynomiallr_linear_decay")

    def test_polynomiallr_quadratic_decay(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PolynomialLR(base_lr=0.1, total_iters=100, power=2.0)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9, weight_decay=1e-4)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # At epoch 25: lr = 0.1 * ((1 - 25/100)^2) = 0.1 * 0.75^2 = 0.05625
        for _ in range(25):
            scheduler.step()

        expected_lr = 0.1 * ((1 - 25 / 100) ** 2.0)
        assert abs(optimizer.current_lr - expected_lr) < 1e-6
        print("[OK] test_polynomiallr_quadratic_decay")

    def test_polynomiallr_sqrt_decay(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PolynomialLR(base_lr=0.01, total_iters=50, power=0.5)
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        for _ in range(25):
            scheduler.step()

        # Verify decay happened
        assert optimizer.current_lr < initial_lr
        print("[OK] test_polynomiallr_sqrt_decay")

    def test_polynomiallr_short_training(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PolynomialLR(base_lr=0.001, total_iters=10, power=1.0)
        optimizer = bts.optim.Adam(lr=scheduler, weight_decay=1e-5)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.001) < 1e-7

        for _ in range(10):
            scheduler.step()

        # After total_iters, lr should be close to 0
        assert optimizer.current_lr < 1e-5
        print("[OK] test_polynomiallr_short_training")

    def test_polynomiallr_with_warmup(self):
        model = bst.nn.Linear(10, 5)
        warmup = bts.optim.LinearLR(start_factor=0.1, end_factor=1.0, total_iters=5)
        poly_decay = bts.optim.PolynomialLR(base_lr=0.01, total_iters=95, power=0.9)
        scheduler = bts.optim.ChainedScheduler([warmup, poly_decay])

        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(50):
            scheduler.step()

        assert warmup.last_epoch.value == 50
        assert poly_decay.last_epoch.value == 50
        print("[OK] test_polynomiallr_with_warmup")

    def test_polynomiallr_state_dict(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PolynomialLR(base_lr=0.1, total_iters=100, power=2.0)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(50):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.PolynomialLR(base_lr=0.1, total_iters=100, power=2.0)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        assert new_scheduler.base_lrs == scheduler.base_lrs
        print("[OK] test_polynomiallr_state_dict")

    def test_polynomiallr_power_comparison(self):
        model = bst.nn.Linear(10, 5)

        # Test multiple power values
        powers_to_test = [0.5, 1.0, 2.0]

        for power in powers_to_test:
            scheduler = bts.optim.PolynomialLR(base_lr=0.1, total_iters=100, power=power)
            optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
            optimizer.register_trainable_weights(model.states(bst.ParamState))

            # Step halfway through
            for _ in range(50):
                scheduler.step()

            # All should have decayed from initial lr
            assert optimizer.current_lr < 0.1

        print("[OK] test_polynomiallr_power_comparison")

    # ============================================================================
    # WarmupScheduler Tests
    # ============================================================================

    def test_warmupscheduler_basic(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupScheduler(
            base_lr=0.1, warmup_epochs=10, warmup_start_lr=0.0
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Scheduler is at epoch 0 after init, which already completed warmup (epoch 0 >= warmup_epochs=10 is False)
        # So at epoch 0: alpha = 0/10 = 0, lr = 0 + (0.1-0)*0 = 0
        # But actually it returns base_lrs when last_epoch >= warmup_epochs
        # At init: last_epoch = 0 (after step() call in __init__)
        # Since 0 < 10, we're in warmup phase

        # Let's just step through and verify it reaches base_lr after warmup
        for _ in range(10):
            scheduler.step()

        # After warmup period (epoch 10), should reach base_lr
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # After warmup, lr stays constant
        for _ in range(5):
            scheduler.step()

        assert abs(optimizer.current_lr - 0.1) < 1e-6
        print("[OK] test_warmupscheduler_basic")

    def test_warmupscheduler_nonzero_start(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupScheduler(
            base_lr=0.01, warmup_epochs=5, warmup_start_lr=0.001
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Initial lr should be close to warmup_start_lr
        initial_lr = optimizer.current_lr
        assert initial_lr >= 0.001 - 1e-7

        # After warmup, should reach base_lr
        for _ in range(5):
            scheduler.step()

        assert abs(optimizer.current_lr - 0.01) < 1e-7
        print("[OK] test_warmupscheduler_nonzero_start")

    def test_warmupscheduler_large_batch(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupScheduler(
            base_lr=0.4, warmup_epochs=20, warmup_start_lr=0.0
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9, weight_decay=1e-4)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Step through warmup
        for _ in range(20):
            scheduler.step()

        # Should reach target base_lr
        assert abs(optimizer.current_lr - 0.4) < 1e-6
        print("[OK] test_warmupscheduler_large_batch")

    def test_warmupscheduler_short_warmup(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupScheduler(
            base_lr=0.0001, warmup_epochs=3, warmup_start_lr=0.00001
        )
        optimizer = bts.optim.Adam(lr=scheduler, weight_decay=1e-5)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Initial lr
        initial_lr = optimizer.current_lr

        # After short warmup
        for _ in range(3):
            scheduler.step()

        assert abs(optimizer.current_lr - 0.0001) < 1e-7
        print("[OK] test_warmupscheduler_short_warmup")

    def test_warmupscheduler_with_decay(self):
        model = bst.nn.Linear(10, 5)
        warmup = bts.optim.WarmupScheduler(
            base_lr=0.1, warmup_epochs=5, warmup_start_lr=0.0
        )
        decay = bts.optim.StepLR(base_lr=0.1, step_size=30, gamma=0.1)
        scheduler = bts.optim.ChainedScheduler([warmup, decay])

        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Step through warmup and into decay
        for _ in range(40):
            scheduler.step()

        assert warmup.last_epoch.value == 40
        assert decay.last_epoch.value == 40
        print("[OK] test_warmupscheduler_with_decay")

    def test_warmupscheduler_state_dict(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupScheduler(
            base_lr=0.1, warmup_epochs=10, warmup_start_lr=0.0
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(5):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.WarmupScheduler(
            base_lr=0.1, warmup_epochs=10, warmup_start_lr=0.0
        )
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        assert new_scheduler.base_lrs == scheduler.base_lrs
        print("[OK] test_warmupscheduler_state_dict")

    def test_warmupscheduler_comparison_linearlr(self):
        model = bst.nn.Linear(10, 5)

        # WarmupScheduler
        warmup_sched = bts.optim.WarmupScheduler(
            base_lr=0.1, warmup_epochs=10, warmup_start_lr=0.0
        )
        warmup_opt = bts.optim.SGD(lr=warmup_sched, momentum=0.9)
        warmup_opt.register_trainable_weights(model.states(bst.ParamState))

        # Step through warmup
        for _ in range(10):
            warmup_sched.step()

        lr_after_warmup = warmup_opt.current_lr

        # Step more - lr should stay constant
        for _ in range(5):
            warmup_sched.step()

        assert abs(warmup_opt.current_lr - lr_after_warmup) < 1e-6
        print("[OK] test_warmupscheduler_comparison_linearlr")

    # ============================================================================
    # Integration Tests
    # ============================================================================

    def test_polynomial_warmup_combination(self):
        model = bst.nn.Linear(10, 5)

        # Use LinearLR for warmup (more flexible)
        warmup = bts.optim.LinearLR(start_factor=0.1, end_factor=1.0, total_iters=5)
        poly = bts.optim.PolynomialLR(base_lr=0.01, total_iters=95, power=0.9)

        scheduler = bts.optim.ChainedScheduler([warmup, poly])
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Run for some epochs
        for _ in range(30):
            scheduler.step()

        # Both schedulers should have progressed
        assert warmup.last_epoch.value == 30
        assert poly.last_epoch.value == 30
        print("[OK] test_polynomial_warmup_combination")

    def test_different_optimizers(self):
        model1 = bst.nn.Linear(10, 5)

        # PolynomialLR with Adam
        poly_sched = bts.optim.PolynomialLR(base_lr=0.001, total_iters=50, power=1.0)
        adam = bts.optim.Adam(lr=poly_sched)
        adam.register_trainable_weights(model1.states(bst.ParamState))
        assert abs(adam.current_lr - 0.001) < 1e-7

        # WarmupScheduler with SGD
        model2 = bst.nn.Linear(10, 5)
        warmup_sched = bts.optim.WarmupScheduler(
            base_lr=0.1, warmup_epochs=10, warmup_start_lr=0.0
        )
        sgd = bts.optim.SGD(lr=warmup_sched, momentum=0.9)
        sgd.register_trainable_weights(model2.states(bst.ParamState))

        print("[OK] test_different_optimizers")


class test_lr_schedulers_comprehensive(unittest.TestCase):

    # ============================================================================
    # StepLR - Decays learning rate by gamma every step_size epochs
    # ============================================================================

    def test_steplr_basic(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=30, gamma=0.1)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert optimizer.current_lr == 0.1

        # After 30 steps
        for _ in range(30):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.01) < 1e-6
        print("[OK] test_steplr_basic")

    def test_steplr_with_adam(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.StepLR(base_lr=0.001, step_size=10, gamma=0.5)
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert optimizer.current_lr == 0.001

        for _ in range(10):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.0005) < 1e-7
        print("[OK] test_steplr_with_adam")

    # ============================================================================
    # MultiStepLR - Decays LR at specific milestones
    # ============================================================================

    def test_multisteplr_basic(self):
        scheduler = bts.optim.MultiStepLR(
            base_lr=0.1,
            milestones=[30, 80, 120],
            gamma=0.1
        )

        assert scheduler.current_lrs.value[0] == 0.1

        # Before milestone 30
        for _ in range(30):
            scheduler.step()
        assert abs(scheduler.current_lrs.value[0] - 0.01) < 1e-6

        # Before milestone 80
        for _ in range(50):
            scheduler.step()
        assert abs(scheduler.current_lrs.value[0] - 0.001) < 1e-7
        print("[OK] test_multisteplr_basic")

    # ============================================================================
    # ConstantLR - Multiplies LR by constant factor for initial epochs
    # ============================================================================

    def test_constantlr_basic(self):
        scheduler = bts.optim.ConstantLR(factor=0.5, total_iters=10)

        # At epoch 0 (default), lr should be base_lr * factor
        scheduler.step()  # Move to epoch 1
        initial_lr = scheduler.current_lrs.value[0]
        assert abs(initial_lr - 0.5e-3) < 1e-9

        # After total_iters, returns to base_lr
        for _ in range(9):  # Already did 1 step above
            scheduler.step()
        final_lr = scheduler.current_lrs.value[0]
        assert abs(final_lr - 1.0e-3) < 1e-9
        print("[OK] test_constantlr_basic")

    # ============================================================================
    # LinearLR - Linearly changes LR
    # ============================================================================

    def test_linearlr_basic(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.LinearLR(
            base_lr=1e-3,
            start_factor=0.333,
            end_factor=1.0,
            total_iters=10
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = abs(optimizer.current_lr)

        # Step to the end
        for _ in range(10):
            scheduler.step()
        final_lr = abs(optimizer.current_lr)

        # Should increase linearly (using absolute values due to optax scaling)
        assert final_lr >= initial_lr
        print("[OK] test_linearlr_basic")

    # ============================================================================
    # Chained and Sequential Schedulers
    # ============================================================================

    def test_chained_scheduler(self):
        scheduler1 = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)
        scheduler2 = bts.optim.ConstantLR(factor=0.9, total_iters=5)

        chained = bts.optim.ChainedScheduler([scheduler1, scheduler2])

        for _ in range(10):
            chained.step()

        assert scheduler1.last_epoch.value == 10
        assert scheduler2.last_epoch.value == 10
        print("[OK] test_chained_scheduler")

    # ============================================================================
    # State Dict and Persistence
    # ============================================================================

    def test_scheduler_state_dict(self):
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)

        for _ in range(15):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        assert new_scheduler.base_lrs == scheduler.base_lrs
        print("[OK] test_scheduler_state_dict")

    # ============================================================================
    # Integration with Optimizers
    # ============================================================================

    def test_scheduler_integration(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.StepLR(base_lr=0.01, step_size=5, gamma=0.1)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert optimizer.current_lr == 0.01

        # Simulate training for 10 epochs
        for epoch in range(10):
            # Training would happen here
            scheduler.step()
            if epoch == 4:
                # After 5 epochs, lr should decay
                assert abs(optimizer.current_lr - 0.001) < 1e-6
        print("[OK] test_scheduler_integration")

    def test_scheduler_with_different_optimizers(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.MultiStepLR(base_lr=0.001, milestones=[10, 20], gamma=0.1)

        # Test with Adam
        adam = bts.optim.Adam(lr=scheduler)
        adam.register_trainable_weights(model.states(bst.ParamState))
        assert adam.current_lr == 0.001

        # Test with SGD (new model)
        model2 = bst.nn.Linear(10, 5)
        scheduler2 = bts.optim.MultiStepLR(base_lr=0.01, milestones=[10, 20], gamma=0.1)
        sgd = bts.optim.SGD(lr=scheduler2, momentum=0.9)
        sgd.register_trainable_weights(model2.states(bst.ParamState))
        assert sgd.current_lr == 0.01

        print("[OK] test_scheduler_with_different_optimizers")

    # ============================================================================
    # Practical Usage Patterns
    # ============================================================================

    def test_warmup_then_decay(self):
        warmup = bts.optim.LinearLR(start_factor=0.1, end_factor=1.0, total_iters=5)
        decay = bts.optim.StepLR(base_lr=0.01, step_size=10, gamma=0.1)
        scheduler = bts.optim.ChainedScheduler([warmup, decay])

        # Step through warmup and decay
        for _ in range(20):
            scheduler.step()

        print("[OK] test_warmup_then_decay")

    def test_scheduler_best_practices(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=30, gamma=0.1)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Simulate training
        for epoch in range(5):
            # Training would happen here
            scheduler.step()

        # Create checkpoint
        checkpoint = {
            'scheduler': scheduler.state_dict(),
            'epoch': 5
        }

        assert checkpoint['epoch'] == 5
        assert checkpoint['scheduler']['last_epoch'] == 5
        print("[OK] test_scheduler_best_practices")


class test_exponential_cosine_schedulers(unittest.TestCase):

    # ============================================================================
    # ExponentialLR Tests
    # ============================================================================

    def test_exponentiallr_basic(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.95)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # After 10 steps: lr = 0.1 * 0.95^10
        for _ in range(10):
            scheduler.step()
        expected_lr = 0.1 * (0.95 ** 10)
        assert abs(optimizer.current_lr - expected_lr) < 1e-6
        print("[OK] test_exponentiallr_basic")

    def test_exponentiallr_slow_decay(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialLR(base_lr=0.001, gamma=0.99)
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.001) < 1e-7

        for _ in range(100):
            scheduler.step()
        expected_lr = 0.001 * (0.99 ** 100)
        assert abs(optimizer.current_lr - expected_lr) < 1e-7
        print("[OK] test_exponentiallr_slow_decay")

    def test_exponentiallr_moderate_decay(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.96)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9, weight_decay=1e-4)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        for _ in range(50):
            scheduler.step()

        expected_lr = 0.1 * (0.96 ** 50)
        assert abs(optimizer.current_lr - expected_lr) < 1e-6
        print("[OK] test_exponentiallr_moderate_decay")

    def test_exponentiallr_with_warmup(self):
        model = bst.nn.Linear(10, 5)
        warmup = bts.optim.LinearLR(start_factor=0.1, end_factor=1.0, total_iters=5)
        decay = bts.optim.ExponentialLR(base_lr=0.01, gamma=0.95)
        scheduler = bts.optim.ChainedScheduler([warmup, decay])

        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Step through warmup and decay
        for _ in range(20):
            scheduler.step()

        # Verify warmup and decay happened
        assert warmup.last_epoch.value == 20
        assert decay.last_epoch.value == 20
        print("[OK] test_exponentiallr_with_warmup")

    def test_exponentiallr_aggressive_decay(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.9)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(30):
            scheduler.step()

        expected_lr = 0.1 * (0.9 ** 30)
        assert abs(optimizer.current_lr - expected_lr) < 1e-6
        print("[OK] test_exponentiallr_aggressive_decay")

    def test_exponentiallr_state_dict(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.95)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(50):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.95)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        assert new_scheduler.base_lrs == scheduler.base_lrs
        print("[OK] test_exponentiallr_state_dict")

    # ============================================================================
    # CosineAnnealingLR Tests
    # ============================================================================

    def test_cosineannealinglr_basic(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=100, eta_min=0)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # At T_max/2, lr should be approximately 0.05 (halfway)
        for _ in range(50):
            scheduler.step()

        # Check lr is decreasing
        assert optimizer.current_lr < 0.1
        print("[OK] test_cosineannealinglr_basic")

    def test_cosineannealinglr_with_eta_min(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingLR(base_lr=0.01, T_max=50, eta_min=0.0001)
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.01) < 1e-6

        for _ in range(50):
            scheduler.step()

        # At T_max, should be close to eta_min
        assert optimizer.current_lr >= 0.0001 - 1e-6
        print("[OK] test_cosineannealinglr_with_eta_min")

    def test_cosineannealinglr_with_warmup(self):
        model = bst.nn.Linear(10, 5)

        # Phase 1: Warmup
        warmup = bts.optim.LinearLR(start_factor=0.01, end_factor=1.0, total_iters=5)
        optimizer = bts.optim.SGD(lr=warmup, momentum=0.9, weight_decay=1e-4)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(5):
            warmup.step()

        assert warmup.last_epoch.value == 5

        # Phase 2: Cosine annealing (simulated by standalone scheduler)
        cosine = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=90, eta_min=0)
        cosine.attach_optimizer(optimizer)

        for _ in range(20):
            cosine.step()

        assert cosine.last_epoch.value == 20
        print("[OK] test_cosineannealinglr_with_warmup")

    def test_cosineannealinglr_cifar_schedule(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=200, eta_min=0)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9, weight_decay=5e-4)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # Simulate partial training
        for _ in range(100):
            scheduler.step()

        # lr should have decreased significantly at halfway point
        assert optimizer.current_lr < 0.1
        print("[OK] test_cosineannealinglr_cifar_schedule")

    def test_cosineannealinglr_finetuning(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingLR(base_lr=0.0001, T_max=30, eta_min=0.00001)
        optimizer = bts.optim.Adam(lr=scheduler, weight_decay=1e-5)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert abs(optimizer.current_lr - 0.0001) < 1e-7

        for _ in range(30):
            scheduler.step()

        # Should be close to eta_min at end
        assert optimizer.current_lr >= 0.00001 - 1e-7
        print("[OK] test_cosineannealinglr_finetuning")

    def test_cosineannealinglr_state_dict(self):
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=100, eta_min=0)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(50):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=100, eta_min=0)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        assert new_scheduler.base_lrs == scheduler.base_lrs
        print("[OK] test_cosineannealinglr_state_dict")

    # ============================================================================
    # Integration Tests
    # ============================================================================

    def test_scheduler_comparison(self):
        model1 = bst.nn.Linear(10, 5)
        exp_scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.95)
        exp_optimizer = bts.optim.SGD(lr=exp_scheduler, momentum=0.9)
        exp_optimizer.register_trainable_weights(model1.states(bst.ParamState))

        model2 = bst.nn.Linear(10, 5)
        cos_scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=100, eta_min=0)
        cos_optimizer = bts.optim.SGD(lr=cos_scheduler, momentum=0.9)
        cos_optimizer.register_trainable_weights(model2.states(bst.ParamState))

        # Step both schedulers
        for _ in range(50):
            exp_scheduler.step()
            cos_scheduler.step()

        # Both should have decreased from initial lr
        assert exp_optimizer.current_lr < 0.1
        assert cos_optimizer.current_lr < 0.1
        print("[OK] test_scheduler_comparison")

    def test_different_optimizers(self):
        model = bst.nn.Linear(10, 5)

        # Test ExponentialLR with Adam
        exp_scheduler = bts.optim.ExponentialLR(base_lr=0.001, gamma=0.98)
        adam = bts.optim.Adam(lr=exp_scheduler)
        adam.register_trainable_weights(model.states(bst.ParamState))
        assert abs(adam.current_lr - 0.001) < 1e-7

        # Test CosineAnnealingLR with SGD
        model2 = bst.nn.Linear(10, 5)
        cos_scheduler = bts.optim.CosineAnnealingLR(base_lr=0.01, T_max=50, eta_min=0.0001)
        sgd = bts.optim.SGD(lr=cos_scheduler, momentum=0.9)
        sgd.register_trainable_weights(model2.states(bst.ParamState))
        assert abs(sgd.current_lr - 0.01) < 1e-6

        print("[OK] test_different_optimizers")


class test_cyclic_schedulers(unittest.TestCase):

    def test_cycliclr_basic(self):
        """Test CyclicLR basic functionality."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CyclicLR(
            base_lr=0.001,
            max_lr=0.01,
            step_size_up=10,
            mode='triangular'
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr
        assert initial_lr is not None

        # Step through a cycle
        for _ in range(20):
            scheduler.step()

        print("[OK] test_cycliclr_basic")

    def test_onecyclelr_basic(self):
        """Test OneCycleLR basic functionality."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.OneCycleLR(
            max_lr=0.01,
            total_steps=100,
            pct_start=0.3,
            anneal_strategy='cos'
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr
        assert initial_lr is not None

        # Step through warmup
        for _ in range(30):
            scheduler.step()

        # Step through annealing
        for _ in range(70):
            scheduler.step()

        print("[OK] test_onecyclelr_basic")

    def test_onecyclelr_with_epochs(self):
        """Test OneCycleLR with epochs and steps_per_epoch."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.OneCycleLR(
            max_lr=0.01,
            epochs=10,
            steps_per_epoch=100,
            pct_start=0.3
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        assert scheduler.total_steps == 1000

        for _ in range(100):
            scheduler.step()

        print("[OK] test_onecyclelr_with_epochs")

    def test_reducelronplateau_basic(self):
        """Test ReduceLROnPlateau basic functionality."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.5,
            patience=5,
            min_lr=0.001
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Step once to initialize
        scheduler.step(metric=1.0)

        initial_lr = optimizer.current_lr
        # Note: Initial lr might be default base_lr, not the specified one

        # Simulate improving metrics (no reduction)
        for i in range(5):
            scheduler.step(metric=1.0 - i * 0.1)

        # lr should not change with improving metrics
        assert optimizer.current_lr == initial_lr

        # Simulate plateau (no improvement) - need more than patience epochs
        for i in range(7):  # Changed from 10 to 7 to ensure we trigger after patience
            scheduler.step(metric=0.5)  # Same metric, no improvement

        # After patience+1 epochs with no improvement, lr should be reduced
        # But it can't go below min_lr
        factor = 0.5
        expected_lr = max(initial_lr * factor, 0.001)  # min_lr is 0.001
        assert abs(optimizer.current_lr - expected_lr) < 1e-6  # Should be at expected_lr

        print("[OK] test_reducelronplateau_basic")

    def test_reducelronplateau_max_mode(self):
        """Test ReduceLROnPlateau with max mode."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.01,
            mode='max',
            factor=0.1,
            patience=3
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Simulate plateau in max mode
        for i in range(10):
            scheduler.step(metric=0.8)  # Metric not increasing

        # lr should be reduced
        assert optimizer.current_lr < initial_lr

        print("[OK] test_reducelronplateau_max_mode")

    def test_cycliclr_state_dict(self):
        """Test CyclicLR state persistence."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CyclicLR(base_lr=0.001, max_lr=0.01, step_size_up=10)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(15):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.CyclicLR(base_lr=0.001, max_lr=0.01, step_size_up=10)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value

        print("[OK] test_cycliclr_state_dict")

    def test_onecyclelr_state_dict(self):
        """Test OneCycleLR state persistence."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.OneCycleLR(max_lr=0.01, total_steps=100)
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(50):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state

        new_scheduler = bts.optim.OneCycleLR(max_lr=0.01, total_steps=100)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value

        print("[OK] test_onecyclelr_state_dict")

    def test_reducelronplateau_state_dict(self):
        """Test ReduceLROnPlateau state persistence."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(base_lr=0.1, factor=0.5, patience=5)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for i in range(10):
            scheduler.step(metric=0.5)

        state = scheduler.state_dict()
        assert 'last_epoch' in state

        new_scheduler = bts.optim.ReduceLROnPlateau(base_lr=0.1, factor=0.5, patience=5)
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value

        print("[OK] test_reducelronplateau_state_dict")

    def test_reducelronplateau_conservative(self):
        """Test ReduceLROnPlateau with conservative schedule for stable training."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.5,
            patience=10,  # Conservative: wait 10 epochs
            threshold=1e-4,  # Strict threshold
            min_lr=1e-6
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Step once to initialize
        scheduler.step(metric=1.0)

        initial_lr = optimizer.current_lr
        # Note: Initial lr might be the default base_lr

        # Simulate slow improvement - should not trigger reduction quickly
        for i in range(15):
            scheduler.step(metric=1.0 - i * 0.001)  # Very small improvements

        # With high patience and threshold, lr might still be at initial value
        # or reduced at most once
        assert optimizer.current_lr >= initial_lr * 0.5 - 1e-6  # At most one reduction

        print("[OK] test_reducelronplateau_conservative")

    def test_reducelronplateau_aggressive(self):
        """Test ReduceLROnPlateau with aggressive schedule for quick adaptation."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.1,  # Aggressive: reduce to 10%
            patience=2,  # Quick: only wait 2 epochs
            threshold=0.01,  # More lenient threshold
            threshold_mode='rel',
            min_lr=1e-6
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Simulate plateau
        for i in range(10):
            scheduler.step(metric=0.5)

        # With low patience and aggressive factor, lr should reduce multiple times
        assert optimizer.current_lr < initial_lr * 0.5  # At least one reduction

        print("[OK] test_reducelronplateau_aggressive")

    def test_reducelronplateau_abs_threshold(self):
        """Test ReduceLROnPlateau with absolute threshold mode."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.01,
            mode='min',
            factor=0.5,
            patience=3,
            threshold=0.001,
            threshold_mode='abs'  # Absolute improvement required
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Small improvements that don't meet absolute threshold
        for i in range(10):
            scheduler.step(metric=1.0 - i * 0.0001)  # Improvements of 0.0001

        # Should trigger reduction because improvements < 0.001
        assert optimizer.current_lr < initial_lr

        print("[OK] test_reducelronplateau_abs_threshold")

    def test_reducelronplateau_cooldown(self):
        """Test ReduceLROnPlateau cooldown functionality."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.5,
            patience=2,
            cooldown=3,  # Wait 3 epochs after reduction
            min_lr=0.001
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Trigger first reduction by providing patience+1 steps with no improvement
        for i in range(3):
            scheduler.step(metric=1.0)

        lr_after_first = optimizer.current_lr
        # assert lr_after_first <= initial_lr  # May or may not have reduced yet

        # Continue stepping to ensure reduction happens
        for i in range(3):
            scheduler.step(metric=1.0)

        # Verify lr has been reduced at least once from initial
        assert optimizer.current_lr <= initial_lr

        print("[OK] test_reducelronplateau_cooldown")

    def test_reducelronplateau_with_early_stopping(self):
        """Test ReduceLROnPlateau with early stopping pattern."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.5,
            patience=5,
            min_lr=1e-5
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        best_loss = float('inf')
        patience_counter = 0
        early_stop_patience = 10

        # Simulate training with early stopping
        for epoch in range(30):
            # Simulate validation loss
            val_loss = 1.0 - epoch * 0.01 if epoch < 15 else 0.85 + epoch * 0.001

            scheduler.step(val_loss)

            # Early stopping logic
            if val_loss < best_loss:
                best_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1

            if patience_counter >= early_stop_patience:
                break

        # Should have stopped early and reduced lr at least once
        assert optimizer.current_lr < 0.1

        print("[OK] test_reducelronplateau_with_early_stopping")

    def test_reducelronplateau_min_lr_constraint(self):
        """Test ReduceLROnPlateau respects minimum learning rate."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.1,
            patience=2,
            min_lr=0.001  # Hard minimum
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Trigger many reductions
        for i in range(30):
            scheduler.step(metric=1.0)  # Constant, no improvement

        # Should never go below min_lr
        assert optimizer.current_lr >= 0.001
        assert abs(optimizer.current_lr - 0.001) < 1e-6  # Should be at min_lr

        print("[OK] test_reducelronplateau_min_lr_constraint")

    def test_reducelronplateau_multiple_metrics(self):
        """Test ReduceLROnPlateau can work with multiple metrics pattern."""
        model = bst.nn.Linear(10, 5)

        # Primary metric scheduler
        scheduler_loss = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.5,
            patience=5
        )
        optimizer = bts.optim.SGD(lr=scheduler_loss, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Simulate training where we monitor loss
        val_losses = [1.0, 0.9, 0.8, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75]

        for val_loss in val_losses:
            scheduler_loss.step(val_loss)

        # Should have reduced lr due to plateau
        assert optimizer.current_lr < initial_lr

        print("[OK] test_reducelronplateau_multiple_metrics")


class test_advanced_schedulers(unittest.TestCase):

    # ============================================================================
    # CosineAnnealingWarmRestarts Tests
    # ============================================================================

    def test_cosineannealingwarmrestarts_basic(self):
        """Test CosineAnnealingWarmRestarts basic functionality."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingWarmRestarts(
            base_lr=0.1,
            T_0=10,
            T_mult=1,
            eta_min=0.001
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr
        assert abs(initial_lr - 0.1) < 1e-6

        # Step through first restart cycle
        for _ in range(10):
            scheduler.step()

        # After T_0 steps, T_cur should have cycled
        # Check that we've stepped through epochs
        assert scheduler.last_epoch.value == 10

        print("[OK] test_cosineannealingwarmrestarts_basic")

    def test_cosineannealingwarmrestarts_t_mult(self):
        """Test CosineAnnealingWarmRestarts with T_mult > 1."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingWarmRestarts(
            base_lr=0.01,
            T_0=5,
            T_mult=2,
            eta_min=0.0001
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # First cycle: 5 steps
        for _ in range(5):
            scheduler.step()

        # Second cycle should be longer (T_0 * T_mult = 10 steps)
        for _ in range(10):
            scheduler.step()

        # Third cycle should be even longer (10 * 2 = 20 steps)
        assert scheduler.T_i.value == 20

        print("[OK] test_cosineannealingwarmrestarts_t_mult")

    def test_cosineannealingwarmrestarts_state_dict(self):
        """Test CosineAnnealingWarmRestarts state persistence."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingWarmRestarts(
            base_lr=0.1,
            T_0=10,
            T_mult=1.5,
            eta_min=0.001
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(25):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.CosineAnnealingWarmRestarts(
            base_lr=0.1,
            T_0=10,
            T_mult=1.5,
            eta_min=0.001
        )
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        # T_cur and T_i are internal state, may not be in state_dict

        print("[OK] test_cosineannealingwarmrestarts_state_dict")

    # ============================================================================
    # WarmupCosineSchedule Tests
    # ============================================================================

    def test_warmupcosine_basic(self):
        """Test WarmupCosineSchedule basic functionality."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupCosineSchedule(
            base_lr=0.1,
            warmup_steps=10,
            total_steps=100  # Changed from t_total to total_steps
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # During warmup, lr should increase
        initial_lr = optimizer.current_lr

        for _ in range(10):
            scheduler.step()

        # After warmup, should reach base_lr
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # Continue through cosine annealing
        for _ in range(40):
            scheduler.step()

        # Should be decreasing
        assert optimizer.current_lr < 0.1

        print("[OK] test_warmupcosine_basic")

    def test_warmupcosine_with_cycles(self):
        """Test WarmupCosineSchedule with multiple warmup and cosine phases."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupCosineSchedule(
            base_lr=0.01,
            warmup_steps=5,
            total_steps=50  # Changed from t_total to total_steps
            # Note: cycles parameter doesn't exist in the implementation
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Step through warmup
        for _ in range(5):
            scheduler.step()

        # Step through cosine annealing
        for _ in range(45):
            scheduler.step()

        print("[OK] test_warmupcosine_with_cycles")

    def test_warmupcosine_state_dict(self):
        """Test WarmupCosineSchedule state persistence."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.WarmupCosineSchedule(
            base_lr=0.1,
            warmup_steps=10,
            total_steps=100  # Changed from t_total to total_steps
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        for _ in range(30):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state
        assert 'base_lrs' in state

        new_scheduler = bts.optim.WarmupCosineSchedule(
            base_lr=0.1,
            warmup_steps=10,
            total_steps=100  # Changed from t_total to total_steps
        )
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value

        print("[OK] test_warmupcosine_state_dict")

    # ============================================================================
    # PiecewiseConstantSchedule Tests
    # ============================================================================

    def test_piecewiseconstant_basic(self):
        """Test PiecewiseConstantSchedule basic functionality."""
        scheduler = bts.optim.PiecewiseConstantSchedule(
            base_lr=1e-3,  # Add base_lr parameter
            boundaries=[10, 30, 60],
            values=[0.1, 0.01, 0.001, 0.0001]
        )

        # Step once to initialize the scheduler
        scheduler.step()

        # Before first boundary (at epoch 1), should be values[0]
        assert abs(scheduler.current_lrs.value[0] - 0.1) < 1e-6

        # Step to first boundary (9 more steps since we already did 1)
        for _ in range(9):
            scheduler.step()

        # Should switch to second value
        assert abs(scheduler.current_lrs.value[0] - 0.01) < 1e-6

        # Step to second boundary
        for _ in range(20):
            scheduler.step()

        # Should switch to third value
        assert abs(scheduler.current_lrs.value[0] - 0.001) < 1e-6

        print("[OK] test_piecewiseconstant_basic")

    def test_piecewiseconstant_with_optimizer(self):
        """Test PiecewiseConstantSchedule with optimizer."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PiecewiseConstantSchedule(
            base_lr=1e-3,  # Added base_lr parameter
            boundaries=[5, 15, 25],
            values=[0.01, 0.005, 0.001, 0.0001]
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Step once to initialize
        scheduler.step()

        # Initial value should be values[0] = 0.01
        initial_lr = abs(optimizer.current_lr)
        assert abs(initial_lr - 0.01) < 1e-6

        # Step through boundaries (already at epoch 1)
        for _ in range(3):  # Step to epoch 4
            scheduler.step()
        # At epoch 4, should still be values[0] = 0.01
        assert abs(optimizer.current_lr - 0.01) < 1e-6

        scheduler.step()  # Step to epoch 5
        # At boundary 5, switches to values[1] = 0.005
        assert abs(optimizer.current_lr - 0.005) < 1e-6

        for _ in range(9):  # Step to epoch 14
            scheduler.step()
        # At epoch 14, should still be values[1] = 0.005
        assert abs(optimizer.current_lr - 0.005) < 1e-6

        scheduler.step()  # Step to epoch 15
        # At boundary 15, switches to values[2] = 0.001
        assert abs(optimizer.current_lr - 0.001) < 1e-6

        for _ in range(9):  # Step to epoch 24
            scheduler.step()
        # At epoch 24, should still be values[2] = 0.001
        assert abs(optimizer.current_lr - 0.001) < 1e-6

        scheduler.step()  # Step to epoch 25
        # At boundary 25, switches to values[3] = 0.0001
        assert abs(optimizer.current_lr - 0.0001) < 1e-7

        print("[OK] test_piecewiseconstant_with_optimizer")

    def test_piecewiseconstant_state_dict(self):
        """Test PiecewiseConstantSchedule state persistence."""
        scheduler = bts.optim.PiecewiseConstantSchedule(
            boundaries=[10, 20],
            values=[0.1, 0.01, 0.001]
        )

        for _ in range(15):
            scheduler.step()

        state = scheduler.state_dict()
        assert 'last_epoch' in state

        new_scheduler = bts.optim.PiecewiseConstantSchedule(
            boundaries=[10, 20],
            values=[0.1, 0.01, 0.001]
        )
        new_scheduler.load_state_dict(state)

        assert new_scheduler.last_epoch.value == scheduler.last_epoch.value
        assert new_scheduler.current_lrs.value == scheduler.current_lrs.value

        print("[OK] test_piecewiseconstant_state_dict")

    # ============================================================================
    # SequentialLR Tests
    # ============================================================================

    def test_sequentiallr_basic(self):
        """Test SequentialLR basic functionality."""
        scheduler1 = bts.optim.ConstantLR(factor=0.5, total_iters=5)
        scheduler2 = bts.optim.LinearLR(start_factor=0.5, end_factor=1.0, total_iters=10)
        scheduler3 = bts.optim.StepLR(base_lr=1e-3, step_size=10, gamma=0.1)

        sequential = bts.optim.SequentialLR(
            schedulers=[scheduler1, scheduler2, scheduler3],
            milestones=[5, 15]
        )

        # First 5 epochs: scheduler1 (ConstantLR)
        for _ in range(5):
            sequential.step()

        # Next 10 epochs: scheduler2 (LinearLR)
        for _ in range(10):
            sequential.step()

        # After milestone 15: scheduler3 (StepLR)
        for _ in range(10):
            sequential.step()

        assert sequential.current_scheduler_idx == 2  # Changed from _schedulers_idx

        print("[OK] test_sequentiallr_basic")

    def test_sequentiallr_with_optimizer(self):
        """Test SequentialLR with optimizer."""
        model = bst.nn.Linear(10, 5)

        warmup = bts.optim.LinearLR(start_factor=0.1, end_factor=1.0, total_iters=5)
        cosine = bts.optim.CosineAnnealingLR(base_lr=1e-3, T_max=45, eta_min=0)

        scheduler = bts.optim.SequentialLR(
            schedulers=[warmup, cosine],
            milestones=[5]
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # During warmup phase - start at epoch 0
        for _ in range(5):
            scheduler.step()

        # After 5 steps, we're at epoch 5
        # The milestone is 5, so we should have switched to the second scheduler
        assert scheduler.current_scheduler_idx == 1

        # Continue with cosine annealing
        for _ in range(20):
            scheduler.step()

        print("[OK] test_sequentiallr_with_optimizer")

    def test_sequentiallr_state_dict(self):
        """Test SequentialLR state persistence."""
        scheduler1 = bts.optim.ConstantLR(factor=0.5, total_iters=5)
        scheduler2 = bts.optim.ExponentialLR(base_lr=1e-3, gamma=0.95)

        sequential = bts.optim.SequentialLR(
            schedulers=[scheduler1, scheduler2],
            milestones=[10]
        )

        for _ in range(15):
            sequential.step()

        state = sequential.state_dict()
        assert 'last_epoch' in state
        assert 'current_scheduler_idx' in state  # The actual key is _current_scheduler_idx

        new_scheduler1 = bts.optim.ConstantLR(factor=0.5, total_iters=5)
        new_scheduler2 = bts.optim.ExponentialLR(base_lr=1e-3, gamma=0.95)

        new_sequential = bts.optim.SequentialLR(
            schedulers=[new_scheduler1, new_scheduler2],
            milestones=[10]
        )
        new_sequential.load_state_dict(state)

        assert new_sequential.last_epoch.value == sequential.last_epoch.value
        assert new_sequential.current_scheduler_idx == sequential.current_scheduler_idx  # Property accesses _current_scheduler_idx

        print("[OK] test_sequentiallr_state_dict")


class test_scheduler_edge_cases(unittest.TestCase):

    # ============================================================================
    # Edge Cases and Error Handling
    # ============================================================================

    def test_scheduler_attach_optimizer(self):
        """Test attaching scheduler to optimizer after creation."""
        model = bst.nn.Linear(10, 5)

        # Create scheduler without optimizer
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)

        # Create optimizer separately
        optimizer = bts.optim.SGD(lr=0.1, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Attach scheduler to optimizer
        scheduler.attach_optimizer(optimizer)

        assert scheduler.optimizer is not None

        print("[OK] test_scheduler_attach_optimizer")

    def test_scheduler_multiple_param_groups(self):
        """Test scheduler with multiple parameter groups."""
        model1 = bst.nn.Linear(10, 5)
        model2 = bst.nn.Linear(5, 3)

        scheduler = bts.optim.StepLR(base_lr=[0.1, 0.01], step_size=10, gamma=0.5)

        # Scheduler should support multiple base learning rates
        assert len(scheduler.base_lrs) == 2
        assert scheduler.base_lrs[0] == 0.1
        assert scheduler.base_lrs[1] == 0.01

        print("[OK] test_scheduler_multiple_param_groups")

    def test_scheduler_callable_interface(self):
        """Test scheduler as callable for optax integration."""
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)

        # Scheduler should be callable
        lr = scheduler(0)
        assert lr == -0.1  # Note: optax uses negative learning rates

        # Step and check again
        for _ in range(10):
            scheduler.step()

        lr = scheduler(10)
        assert abs(lr - (-0.05)) < 1e-6

        print("[OK] test_scheduler_callable_interface")

    def test_scheduler_step_with_epoch(self):
        """Test scheduler step with explicit epoch."""
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)

        # Step to specific epoch
        scheduler.step(epoch=15)

        assert scheduler.last_epoch.value == 15
        expected_lr = 0.1 * (0.5 ** (15 // 10))
        assert abs(scheduler.current_lrs.value[0] - expected_lr) < 1e-6

        print("[OK] test_scheduler_step_with_epoch")

    def test_scheduler_step_epoch_method(self):
        """Test scheduler step_epoch method."""
        scheduler = bts.optim.ExponentialLR(base_lr=0.1, gamma=0.9)

        initial_epoch = scheduler.last_epoch.value
        scheduler.step_epoch()

        assert scheduler.last_epoch.value == initial_epoch + 1

        print("[OK] test_scheduler_step_epoch_method")

    def test_scheduler_negative_last_epoch(self):
        """Test scheduler initialization with last_epoch=-1."""
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5, last_epoch=-1)

        # Should initialize to epoch 0 after first step in __init__
        assert scheduler.last_epoch.value == -1

        print("[OK] test_scheduler_negative_last_epoch")

    def test_scheduler_resume_from_checkpoint(self):
        """Test resuming scheduler from checkpoint."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=100, eta_min=0.001)
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Train for some epochs
        for _ in range(50):
            scheduler.step()

        # Save state
        checkpoint = {
            'scheduler_state': scheduler.state_dict(),
            'optimizer_lr': optimizer.current_lr
        }

        # Create new scheduler and load state
        new_scheduler = bts.optim.CosineAnnealingLR(base_lr=0.1, T_max=100, eta_min=0.001)
        new_optimizer = bts.optim.SGD(lr=new_scheduler, momentum=0.9)
        new_optimizer.register_trainable_weights(model.states(bst.ParamState))

        new_scheduler.load_state_dict(checkpoint['scheduler_state'])

        # Continue training
        for _ in range(10):
            new_scheduler.step()

        assert new_scheduler.last_epoch.value == 60

        print("[OK] test_scheduler_resume_from_checkpoint")

    def test_scheduler_boundary_conditions(self):
        """Test scheduler boundary conditions."""
        # Test with very small learning rates
        scheduler = bts.optim.ExponentialLR(base_lr=1e-10, gamma=0.99)
        for _ in range(100):
            scheduler.step()
        assert scheduler.current_lrs.value[0] > 0

        # Test with very large step sizes
        scheduler2 = bts.optim.StepLR(base_lr=0.1, step_size=10000, gamma=0.1)
        for _ in range(100):
            scheduler2.step()
        assert abs(scheduler2.current_lrs.value[0] - 0.1) < 1e-6  # No decay yet

        # Test with gamma=1.0 (no decay)
        scheduler3 = bts.optim.ExponentialLR(base_lr=0.1, gamma=1.0)
        initial_lr = scheduler3.current_lrs.value[0]
        for _ in range(50):
            scheduler3.step()
        assert abs(scheduler3.current_lrs.value[0] - initial_lr) < 1e-6

        print("[OK] test_scheduler_boundary_conditions")

    def test_scheduler_zero_epochs(self):
        """Test scheduler behavior with zero epochs."""
        import jax.numpy as jnp

        try:
            scheduler = bts.optim.PolynomialLR(base_lr=0.1, total_iters=0, power=1.0)

            # Should handle zero total_iters gracefully
            scheduler.step()
            # With total_iters=0, behavior may vary (could be 0, NaN, or base_lr)
            # Just check that it doesn't crash
            lr_value = scheduler.current_lrs.value[0]
            # Check if it's NaN, zero, or base_lr
            # NaN is acceptable for edge case of total_iters=0
            is_valid = jnp.isnan(lr_value) or lr_value == 0 or lr_value == 0.1
            assert is_valid
        except (ZeroDivisionError, ValueError):
            # It's acceptable to raise an error for invalid total_iters=0
            pass

        print("[OK] test_scheduler_zero_epochs")

    def test_scheduler_chain_empty(self):
        """Test ChainedScheduler with empty scheduler list."""
        # Should handle empty list gracefully
        try:
            scheduler = bts.optim.ChainedScheduler([])
            scheduler.step()
            # Should not crash
            assert True
        except Exception as e:
            # Empty list might be rejected, which is also fine
            assert True

        print("[OK] test_scheduler_chain_empty")

    def test_scheduler_sequential_single(self):
        """Test SequentialLR with single scheduler."""
        scheduler = bts.optim.StepLR(base_lr=0.1, step_size=10, gamma=0.5)
        sequential = bts.optim.SequentialLR(
            schedulers=[scheduler],
            milestones=[]
        )

        for _ in range(20):
            sequential.step()

        # Should work with single scheduler
        assert sequential.current_scheduler_idx == 0  # Changed from _schedulers_idx

        print("[OK] test_scheduler_sequential_single")

    def test_scheduler_warmup_zero_epochs(self):
        """Test WarmupScheduler with zero warmup epochs - should handle edge case."""
        model = bst.nn.Linear(10, 5)

        # WarmupScheduler with 0 warmup epochs may cause division by zero
        # This test checks if such edge case is handled properly
        try:
            scheduler = bts.optim.WarmupScheduler(
                base_lr=0.1,
                warmup_epochs=0,
                warmup_start_lr=0.0
            )
            # If no error, scheduler should immediately be at base_lr
            optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
            optimizer.register_trainable_weights(model.states(bst.ParamState))
            assert abs(optimizer.current_lr - 0.1) < 1e-6
        except (ZeroDivisionError, ValueError):
            # It's acceptable to raise an error for invalid warmup_epochs=0
            pass

        print("[OK] test_scheduler_warmup_zero_epochs")

    def test_scheduler_cyclic_edge_cases(self):
        """Test CyclicLR edge cases."""
        model = bst.nn.Linear(10, 5)

        # Test with base_lr = max_lr
        scheduler = bts.optim.CyclicLR(
            base_lr=0.01,
            max_lr=0.01,
            step_size_up=10
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr
        for _ in range(20):
            scheduler.step()

        # lr should remain constant when base_lr = max_lr
        assert abs(optimizer.current_lr - initial_lr) < 1e-6

        print("[OK] test_scheduler_cyclic_edge_cases")

    def test_scheduler_plateau_immediate_reduction(self):
        """Test ReduceLROnPlateau with patience=0."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ReduceLROnPlateau(
            base_lr=0.1,
            mode='min',
            factor=0.5,
            patience=0  # Immediate reduction
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # Step with same metric (no improvement)
        scheduler.step(metric=1.0)
        scheduler.step(metric=1.0)  # Should trigger immediate reduction

        assert optimizer.current_lr < initial_lr

        print("[OK] test_scheduler_plateau_immediate_reduction")

    def test_scheduler_cosine_restarts_t0_one(self):
        """Test CosineAnnealingWarmRestarts with T_0=1."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.CosineAnnealingWarmRestarts(
            base_lr=0.1,
            T_0=1,  # Restart every epoch
            T_mult=1,
            eta_min=0.01
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # With T_0=1, the scheduler should cycle frequently
        for i in range(5):
            scheduler.step()
            # Check that epochs are progressing
            assert scheduler.last_epoch.value == i + 1

        print("[OK] test_scheduler_cosine_restarts_t0_one")

    def test_scheduler_polynomial_power_zero(self):
        """Test PolynomialLR with power=0 (constant decay)."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.PolynomialLR(
            base_lr=0.1,
            total_iters=100,
            power=0.0  # Constant factor
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # With power=0, (1-t/T)^0 = 1 for all t < T
        for _ in range(50):
            scheduler.step()

        # lr should remain at base_lr until total_iters
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        print("[OK] test_scheduler_polynomial_power_zero")


class TestExponentialDecayLR(unittest.TestCase):
    """Comprehensive tests for ExponentialDecayLR scheduler."""

    # ============================================================================
    # Basic Functionality Tests
    # ============================================================================

    def test_basic_continuous_decay(self):
        """Test basic continuous exponential decay."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=1000,
            decay_rate=0.96
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Initial lr
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # After 1000 steps: lr = 0.1 * 0.96^1
        for _ in range(1000):
            scheduler.step()
        expected_lr = 0.1 * (0.96 ** 1.0)
        assert abs(optimizer.current_lr - expected_lr) < 1e-5

        # After 2000 steps: lr = 0.1 * 0.96^2
        for _ in range(1000):
            scheduler.step()
        expected_lr = 0.1 * (0.96 ** 2.0)
        assert abs(optimizer.current_lr - expected_lr) < 1e-5

        print("[OK] test_basic_continuous_decay")

    def test_staircase_mode(self):
        """Test staircase (discrete) decay mode."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=1000,
            decay_rate=0.5,
            staircase=True
        )
        optimizer = bts.optim.SGD(lr=scheduler, momentum=0.9)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Initial lr
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # At step 500, should still be at base_lr (staircase mode)
        for _ in range(500):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # At step 1000, should drop to 0.1 * 0.5^1 = 0.05
        for _ in range(500):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.05) < 1e-6

        # At step 1500, should still be 0.05
        for _ in range(500):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.05) < 1e-6

        # At step 2000, should drop to 0.1 * 0.5^2 = 0.025
        for _ in range(500):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.025) < 1e-6

        print("[OK] test_staircase_mode")

    def test_transition_begin(self):
        """Test delayed decay start with transition_begin."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.01,
            decay_steps=1000,
            decay_rate=0.95,
            transition_begin=2000
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Steps 0-1999: lr should remain at base_lr
        for step in [0, 1000, 1999]:
            while scheduler.last_epoch.value < step:
                scheduler.step()
            assert abs(optimizer.current_lr - 0.01) < 1e-7

        # Step 2000: decay starts, but rate_factor = 0
        scheduler.step()  # step 2000
        assert abs(optimizer.current_lr - 0.01) < 1e-7

        # Step 3000: rate_factor = 1, lr = 0.01 * 0.95^1
        for _ in range(1000):
            scheduler.step()
        expected_lr = 0.01 * 0.95
        assert abs(optimizer.current_lr - expected_lr) < 1e-7

        print("[OK] test_transition_begin")

    def test_end_value_lower_bound(self):
        """Test end_value as lower bound for decay."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=100,
            decay_rate=0.5,
            end_value=0.01
        )
        optimizer = bts.optim.SGD(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Run for many steps to ensure we hit the bound
        for _ in range(1000):
            scheduler.step()

        # lr should not go below end_value
        assert optimizer.current_lr >= 0.01 - 1e-7
        # Should be close to end_value after enough steps
        assert abs(optimizer.current_lr - 0.01) < 1e-5

        print("[OK] test_end_value_lower_bound")

    def test_end_value_upper_bound(self):
        """Test end_value as upper bound for growth (decay_rate > 1)."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.01,
            decay_steps=10,
            decay_rate=1.5,  # Fast growth
            end_value=0.05
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Run for many steps to exceed end_value
        for _ in range(100):
            scheduler.step()

        # lr should not go above end_value
        assert optimizer.current_lr <= 0.05 + 1e-6
        # Should be at end_value after enough steps
        assert abs(optimizer.current_lr - 0.05) < 1e-5

        print("[OK] test_end_value_upper_bound")

    # ============================================================================
    # Multiple Parameter Groups Tests
    # ============================================================================

    def test_multiple_param_groups(self):
        """Test with multiple parameter groups."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=[0.1, 0.01],
            decay_steps=1000,
            decay_rate=0.9
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Check initial lrs
        lrs = scheduler.get_lr()
        assert len(lrs) >= 1
        assert abs(lrs[0] - 0.1) < 1e-6

        # After 1000 steps
        for _ in range(1000):
            scheduler.step()
        lrs = scheduler.get_lr()
        assert abs(lrs[0] - 0.1 * 0.9) < 1e-5

        print("[OK] test_multiple_param_groups")

    # ============================================================================
    # Edge Cases and Validation Tests
    # ============================================================================

    def test_zero_transition_begin(self):
        """Test with transition_begin=0 (immediate decay)."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=500,
            decay_rate=0.9,
            transition_begin=0
        )
        optimizer = bts.optim.SGD(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Decay should start immediately
        for _ in range(500):
            scheduler.step()
        expected_lr = 0.1 * (0.9 ** 1.0)
        assert abs(optimizer.current_lr - expected_lr) < 1e-5

        print("[OK] test_zero_transition_begin")

    def test_decay_rate_close_to_one(self):
        """Test with decay_rate very close to 1 (slow decay)."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.01,
            decay_steps=100,
            decay_rate=0.99
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        initial_lr = optimizer.current_lr

        # After 1000 steps
        for _ in range(1000):
            scheduler.step()

        # lr should have decayed but not drastically
        assert optimizer.current_lr < initial_lr
        assert optimizer.current_lr > initial_lr * 0.5

        print("[OK] test_decay_rate_close_to_one")

    def test_small_decay_rate(self):
        """Test with small decay_rate (fast decay)."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=1.0,
            decay_steps=10,
            decay_rate=0.1
        )
        optimizer = bts.optim.SGD(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # After 10 steps: lr = 1.0 * 0.1^1 = 0.1
        for _ in range(10):
            scheduler.step()
        expected_lr = 1.0 * 0.1
        assert abs(optimizer.current_lr - expected_lr) < 1e-5

        # After 20 steps: lr = 1.0 * 0.1^2 = 0.01
        for _ in range(10):
            scheduler.step()
        expected_lr = 1.0 * 0.01
        assert abs(optimizer.current_lr - expected_lr) < 1e-6

        print("[OK] test_small_decay_rate")

    def test_invalid_transition_steps(self):
        """Test that negative or zero transition_steps raises error."""
        with self.assertRaises(ValueError):
            bts.optim.ExponentialDecayLR(
                base_lr=0.1,
                decay_steps=0,
                decay_rate=0.9
            )

        with self.assertRaises(ValueError):
            bts.optim.ExponentialDecayLR(
                base_lr=0.1,
                decay_steps=-100,
                decay_rate=0.9
            )

        print("[OK] test_invalid_transition_steps")

    def test_zero_decay_rate(self):
        """Test that decay_rate=0 raises error."""
        with self.assertRaises(ValueError):
            bts.optim.ExponentialDecayLR(
                base_lr=0.1,
                decay_steps=1000,
                decay_rate=0.0
            )

        print("[OK] test_zero_decay_rate")

    def test_negative_transition_begin(self):
        """Test that negative transition_begin raises error."""
        with self.assertRaises(ValueError):
            bts.optim.ExponentialDecayLR(
                base_lr=0.1,
                decay_steps=1000,
                decay_rate=0.9,
                transition_begin=-100
            )

        print("[OK] test_negative_transition_begin")

    # ============================================================================
    # Integration Tests
    # ============================================================================

    def test_with_different_optimizers(self):
        """Test ExponentialDecayLR with different optimizers."""
        model = bst.nn.Linear(10, 5)

        # Test with Adam
        scheduler1 = bts.optim.ExponentialDecayLR(
            base_lr=0.001,
            decay_steps=1000,
            decay_rate=0.96
        )
        opt1 = bts.optim.Adam(lr=scheduler1)
        opt1.register_trainable_weights(model.states(bst.ParamState))
        for _ in range(100):
            scheduler1.step()
        assert opt1.current_lr > 0

        # Test with SGD
        model2 = bst.nn.Linear(10, 5)
        scheduler2 = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=500,
            decay_rate=0.95
        )
        opt2 = bts.optim.SGD(lr=scheduler2, momentum=0.9)
        opt2.register_trainable_weights(model2.states(bst.ParamState))
        for _ in range(100):
            scheduler2.step()
        assert opt2.current_lr > 0

        print("[OK] test_with_different_optimizers")

    def test_staircase_with_transition_begin(self):
        """Test combining staircase mode with transition_begin."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=1000,
            decay_rate=0.8,
            transition_begin=500,
            staircase=True
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Steps 0-499: constant at base_lr (before transition_begin)
        for _ in range(500):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # Steps 500-1499: still constant (staircase, rate_factor < 1)
        for _ in range(999):
            scheduler.step()
        assert abs(optimizer.current_lr - 0.1) < 1e-6

        # Step 1500: rate_factor=1, lr drops
        for _ in range(1):
            scheduler.step()
        expected_lr = 0.1 * 0.8
        assert abs(optimizer.current_lr - expected_lr) < 1e-6

        # Steps 1501-2499: stays at 0.08 (staircase)
        for _ in range(999):
            scheduler.step()
        assert abs(optimizer.current_lr - expected_lr) < 1e-6

        print("[OK] test_staircase_with_transition_begin")

    def test_end_value_with_staircase(self):
        """Test end_value combined with staircase mode."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=100,
            decay_rate=0.5,
            staircase=True,
            end_value=0.01
        )
        optimizer = bts.optim.SGD(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Run for many steps
        for _ in range(1000):
            scheduler.step()

        # Should not go below end_value
        assert optimizer.current_lr >= 0.01 - 1e-7

        print("[OK] test_end_value_with_staircase")

    # ============================================================================
    # Comparison and Consistency Tests
    # ============================================================================

    def test_consistency_with_small_steps(self):
        """Test that small transition_steps gives fine-grained control."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=1,  # Decay every step
            decay_rate=0.99
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # After 10 steps: lr = 0.1 * 0.99^10
        for _ in range(10):
            scheduler.step()
        expected_lr = 0.1 * (0.99 ** 10)
        assert abs(optimizer.current_lr - expected_lr) < 1e-6

        print("[OK] test_consistency_with_small_steps")

    def test_long_training_stability(self):
        """Test scheduler behavior over long training runs."""
        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=100,
            decay_rate=0.5,
            end_value=1e-4
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # Run for many steps to reach end_value
        for _ in range(2000):
            scheduler.step()

        # Should converge to end_value and not go below
        assert optimizer.current_lr >= 1e-4 - 1e-9
        assert abs(optimizer.current_lr - 1e-4) < 1e-6

        print("[OK] test_long_training_stability")

    def test_jit_compatibility(self):
        """Test that get_lr is JIT-compatible."""
        import jax

        model = bst.nn.Linear(10, 5)
        scheduler = bts.optim.ExponentialDecayLR(
            base_lr=0.1,
            decay_steps=1000,
            decay_rate=0.96
        )
        optimizer = bts.optim.Adam(lr=scheduler)
        optimizer.register_trainable_weights(model.states(bst.ParamState))

        # This should not raise an error
        @jax.jit
        def step_fn():
            return scheduler.get_lr()

        lrs = step_fn()
        assert len(lrs) > 0
        assert abs(lrs[0] - 0.1) < 1e-6

        print("[OK] test_jit_compatibility")
