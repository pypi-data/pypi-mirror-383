from __future__ import annotations
from typing import Optional, Dict, Any
from typing_extensions import override
import logging
import json

from ..config.settings import Settings
from .llm import LLM
from google.genai import Client
from google.genai import types


class GeminiModel(LLM):
    """
    A subclass of LLM that uses Google's Gemini as the backend for text generation.

    This class provides an interface to interact with Google's Generative AI,
    enabling text generation with various models supported by the Gemini API.

    Attributes:
        model_name (str): The name of the model to use with Gemini.
        role (str): The role of the user in the conversation, typically "user".
        system_prompt (str): The system prompt to use for text generation.
        model (Client): The Gemini client configured to interact with the API.
    """

    def __init__(
        self,
        model_name: str,
        system_prompt: Optional[str] = None,
        system_prompt_file: Optional[str] = None,
        api_base: Optional[str] = None,
        role: str = "user",
    ) -> None:
        """
        Initializes an instance of GeminiModel.

        Args:
            model_name (str): The name of the model to use with Gemini.
            system_prompt (Optional[str]): The system prompt to use. If not provided, it will be loaded from system_prompt_file or use the default value.
            system_prompt_file (Optional[str]): The path to the file to load the system prompt from. If provided, it takes precedence over system_prompt.
            role (str): The role of the user in the conversation, defaults to "user".
        """
        self.api_base = api_base or Settings.DEFAULT_GOOGLE_CLIENT
        super().__init__(model_name, system_prompt, system_prompt_file, self.api_base)
        logging.info(f"Using Gemini with {model_name} model ðŸ¤–")
        self.role: str = role

    @override
    def load(self) -> Client:
        """
        Loads the Gemini client using the modern google.generativeai SDK.

        Returns:
            Client: The client object to interact with Gemini API.
        """
        return Client(
            api_key=Settings.GEMINI_API_KEY,
            http_options=types.HttpOptions(base_url=self.api_base),
        )

    @override
    def generate(self, input: Dict[str, Any]) -> str:
        """
        Generates text using the Gemini model.
        It constructs a structured 'contents' payload using the 'types' module
        as requested for proper input management.

        Args:
            input (Dict[str, Any]): The input data for text generation.

        Returns:
            str: The text generated by the model.
        """
        prompt = json.dumps(input)
        # we need to make the prompt to gemini model format

        contents = [
            types.Content(role="model", parts=[types.Part(text=self.system_prompt)]),
            types.Content(role="user", parts=[types.Part(text=prompt)]),
        ]

        try:
            # as per the LLM class the model is already loaded
            # but the text generation is done by the models.generate_content # ref https://ai.google.dev/gemini-api/docs/text-generation
            response = self.model.models.generate_content(
                model=self.model_name,
                contents=contents,
            )
            if not response.candidates:
                logging.warning("Response was blocked. Checking prompt feedback.")
                if response.prompt_feedback:
                    logging.warning(f"Prompt Feedback: {response.prompt_feedback}")
                return "Response blocked due to safety settings."
            return response.text
        except Exception as e:
            logging.error(f"An error occurred during Gemini content generation: {e}")
            return f"Error: {e}"
