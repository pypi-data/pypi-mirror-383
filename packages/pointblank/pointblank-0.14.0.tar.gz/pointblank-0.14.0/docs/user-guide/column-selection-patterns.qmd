---
title: Column Selection Patterns
jupyter: python3
toc-expand: 2
html-table-processing: none
---

```{python}
#| echo: false
#| output: false
import pointblank as pb
pb.config(report_incl_header=False, report_incl_footer=False)
```

Data validation often requires working with columns in flexible ways. Pointblank offers two powerful
approaches:

1. Applying validation rules across multiple columns: validate many columns with a single rule
2. Comparing values between columns: create validations that compare values across different columns

This guide covers both approaches in detail with practical examples.

## Part 1: Applying Rules Across Multiple Columns

Many of Pointblank's validation methods perform column-level checks. These methods provide the
`columns=` parameter, which accepts not just a single column name but multiple columns through
various selection methods.

Why is this useful? Often you'll want to perform the same validation check (e.g., checking that
numerical values are all positive) across multiple columns. Rather than defining the same rules
multiple times, you can map the validation across those columns in a single step.

Let's explore this using the `game_revenue` dataset:

```{python}
#| echo: false
pb.preview(pb.load_dataset(dataset="game_revenue"))
```

### Using a List of Column Names

The simplest way to validate multiple columns is to provide a list to the `columns=` parameter. In
the `game_revenue` dataset, we have two columns with numerical data: `item_revenue` and
`session_duration`. If we expect all values in both columns to be greater than `0`, we can write:

```{python}
import pointblank as pb

(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_gt(
        columns=["item_revenue", "session_duration"],
        value=0
    )
    .interrogate()
)
```

The validation report shows two validation steps were created from a single method call! All
validation parameters are shared across all generated steps, including thresholds and briefs:

```{python}
(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_gt(
        columns=["item_revenue", "session_duration"],
        value=0,
        thresholds=(0.1, 0.2, 0.3),
        brief="`{col}` must be greater than zero."
    )
    .interrogate()
)
```

In this example, you can see that the validation report displays customized briefs for each column
("`item_revenue` must be greater than zero." and "`session_duration` must be greater than zero."),
automatically substituting the column name using the `{col}` placeholder in the brief template. This
feature is particularly helpful when reviewing reports, as it provides clear, human-readable
descriptions of what each validation step is checking. When working with multiple columns through a
single validation call, these dynamically generated briefs make your validation reports more
understandable for both technical and non-technical stakeholders.

### Using Pointblank's Column Selectors

For more advanced column selection, Pointblank provides selector functions that resolve columns
based on:

- text patterns in column names
- column position
- column data type

Two common selectors, `starts_with()` and `ends_with()`, resolve columns based on text patterns in
column names.

The `game_revenue` dataset has three columns starting with "item": `item_type`, `item_name`, and
`item_revenue`. Let's check that these columns contain no missing values:

```{python}
(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_not_null(columns=pb.starts_with("item"))
    .interrogate()
)
```

Three validation steps were automatically created because three columns matched the pattern.

The complete list of column selectors includes:

- `starts_with()`
- `ends_with()`
- `contains()`
- `matches()`
- `everything()`
- `first_n()`
- `last_n()`

### Combining Column Selectors

Column selectors can be combined for more powerful selection. To do this, use the `col()` helper
function with logical operators:

- `&` (*and*)
- `|` (*or*)
- `-` (*difference*)
- `~` (*not*)

For example, to select all columns except the first four:

```{python}
col_selection = pb.col(pb.everything() - pb.first_n(4))

(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_not_null(
        columns=col_selection,
        thresholds=(1, 0.05, 0.1)
    )
    .interrogate()
)
```

This selects every column except the first four, resulting in seven validation steps.

### Narwhals Selectors

Pointblank also supports column selectors from the
[Narwhals](https://narwhals-dev.github.io/narwhals/) library, which include:

- `matches()`
- `by_dtype()`
- `boolean()`
- `categorical()`
- `datetime()`
- `numeric()`
- `string()`

Here's an example selecting all numeric columns:

```{python}
import narwhals.selectors as ncs

(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_gt(
        columns=ncs.numeric(),
        value=0
    )
    .interrogate()
)
```

And selecting all string columns matching "item_":

```{python}
(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_not_null(columns=pb.col(ncs.string() & ncs.matches("item_")))
    .interrogate()
)
```

This example demonstrates the power of combining Narwhals selectors with logical operators. By using
`ncs.string()` to select string columns and then filtering with `ncs.matches("item_")`, we can
precisely target text columns with specific naming patterns. This type of targeted selection is
particularly valuable when working with wide datasets that have consistent column naming
conventions, allowing you to apply appropriate validation rules to logically grouped columns without
explicitly listing each one.

### Caveats for Using Column Selectors

While column selectors are powerful, there are some caveats. If a selector doesn't match any
columns, the validation won't fail but will show an 'explosion' in the report:

```{python}
(
    pb.Validate(data=pb.load_dataset("game_revenue"))
    .col_vals_not_null(columns=pb.starts_with("items"))
    .col_vals_gt(columns="item_revenue", value=0)
    .interrogate()
)
```

Notice that although there was a problem with Step 1 (that should be addressed), the interrogation
did move on to Step 2 without complication.

To mitigate uncertainty, include validation steps that check for the existence of key columns with
`~~Validate.col_exists()` or verify the schema with `~~Validate.col_schema_match()`.

## Part 2: Comparing Values Between Columns

Sometimes you need to compare values across different columns rather than against fixed values.
Pointblank enables this through the `col()` helper function.

Let's look at examples using the `small_table` dataset:

```{python}
# | echo: false
pb.preview(pb.load_dataset(dataset="small_table"), n_head=20, n_tail=20)
```

### Using `col()`{.qd-no-link} to Specify a Comparison Column

While we typically use validation methods to compare column values against fixed values:

```python
...
.col_vals_gt(columns="a", value=2, ...)
...
```

We can also compare values between columns by using `col()` in the `value=` parameter:

```python
...
.col_vals_gt(columns="a", value=pb.col("x"), ...)
...
```

This checks that each value in column `a` is greater than the corresponding value in column `x`.
Here's a concrete example:

```{python}
(
    pb.Validate(data=pb.load_dataset("small_table"))
    .col_vals_gt(
        columns="d",
        value=pb.col("c")
    )
    .interrogate()
)
```

Notice that the validation report shows both column names (`d` and `c`). There are two failing test
units because of missing values in column `c`. When comparing across columns, missing values in
either column can cause failures.

To handle missing values, use `na_pass=True`:

```{python}
(
    pb.Validate(data=pb.load_dataset("small_table"))
    .col_vals_gt(
        columns="d",
        value=pb.col("c"),
        na_pass=True
    )
    .interrogate()
)
```

Now all tests pass.

The following validation methods accept a `col()` expression in their `value=` parameter:

- `~~Validate.col_vals_gt()`
- `~~Validate.col_vals_lt()`
- `~~Validate.col_vals_ge()`
- `~~Validate.col_vals_le()`
- `~~Validate.col_vals_eq()`
- `~~Validate.col_vals_ne()`

### Using `col()` in Range Checks

For range validations via `~~Validate.col_vals_between()` and `~~Validate.col_vals_outside()` you
can use a mix of column references and fixed values:

```{python}
(
    pb.Validate(data=pb.load_dataset("small_table"))
    .col_vals_between(
        columns="d",
        left=pb.col("c"),
        right=10_000,
        na_pass=True
    )
    .interrogate()
)
```

The validation report shows the range as `[c, 10000]`, indicating that the lower bound comes from
column `c` while the upper bound is fixed at `10000`.

## Advanced Examples: Combining Both Approaches

The true power comes from combining both approaches: validating multiple columns and using
cross-column comparisons:

```{python}
validation = (
    pb.Validate(data=pb.load_dataset("small_table"))
    .col_vals_gt(
        columns=["c", "d"],
        value=pb.col("a"),
        na_pass=True
    )
    .interrogate()
)

validation
```

This creates validation steps checking that values in both columns `d` and `e` are greater than
their corresponding values in column `a`.

## Conclusion

Pointblank provides flexible approaches to working with columns:

1. Column selection: validate multiple columns with a single validation rule
2. Cross-column comparison: compare values between columns

These capabilities allow you to:

- write more concise validation code
- apply consistent validation rules across similar columns
- create dynamic validations that check relationships between columns
- build comprehensive data quality checks with minimal code

By getting familiar with these techniques, you can create more elegant and powerful validation plans
while also reducing repetition in your code.
