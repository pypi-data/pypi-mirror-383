# ğŸš€ Synthetic Data Generation Pipeline

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://badge.fury.io/py/Data-Generation-Agents.svg)](https://badge.fury.io/py/Data-Generation-Agents)

An **enterprise-grade, fault-tolerant** synthetic data generation pipeline powered by **LangGraph**, **Google Gemini AI**, and intelligent web scraping. Create high-quality, multilingual training datasets with automatic resumption, state management, and graceful error handling.

## âœ¨ Key Features

### ğŸ”„ **Fault-Tolerant Architecture**
- **Automatic Resumption**: Pipeline saves progress after every operationâ€”stop anytime, resume later
- **Zero Data Loss**: State persistence ensures no work is ever repeated
- **Graceful API Handling**: Quota exhaustion stops cleanly with clear messages
- **Incremental Processing**: Process millions of samples without memory issues

### ğŸŒ **Multilingual & Domain-Agnostic**
- **Languages**: Excellent support for dialects (English, Egyptian Arabic, Brazilian Portuguese, etc.)
- **Domain Categories**: Optional subcategory specification for targeted data generation
- **Automatic Language Detection**: Built-in filtering for content quality

### ğŸ§  **AI-Powered Intelligence**
- **Smart Query Parsing**: Extracts intent, language, domain, and categories from natural language
- **Web Knowledge Integration**: Scrapes and synthesizes information
- **Topic Extraction**: Automatically identifies unique subtopics per domain
- **Context-Aware Generation**: Creates diverse, factually accurate synthetic data

### ğŸ“Š **Production-Ready**
- **LangGraph Orchestration**: Stateful workflow with conditional routing
- **Structured Logging**: Debug-level traceability for troubleshooting
- **Type-Safe**: Pydantic validation throughout the pipeline

---

## ğŸ—ï¸ Architecture

The pipeline uses **8 specialized AI agents** orchestrated by **LangGraph**:


### **Agent Responsibilities**

| Agent | Purpose | Technology |
|-------|---------|------------|
| **Query Parser** | Extract requirements (domain, language, count, categories) | Gemini AI |
| **Query Refiner** | Generate diverse search queries | Gemini AI |
| **Web Search** | Find relevant sources | Tavily API |
| **Filtration** | Filter by language/relevance | AsyncIO + LangDetect |
| **Web Scraping** | Extract content from URLs | ScraperAPI |
| **Chunking** | Split content into 500-word chunks | Custom Service |
| **Topic Extraction** | Identify subtopics from content | Gemini AI |
| **Data Generator** | Create synthetic samples | Gemini AI |

---

## ğŸ“Š Pipeline Flow Visualization

```mermaid
flowchart TD
    Start([Start]) --> UserQuery[User Query]
    UserQuery --> QueryParser[Query Parser]
    QueryParser --> DataGen{Data Generation?}
    
    DataGen -->|No| Exit([Exit])
    DataGen -->|Yes| QueryRefiner[Query Refiner]
    
    QueryRefiner --> WebSearch[Web Search]
    WebSearch --> WebScraping[Web Scraping]
    WebScraping --> TopicExtraction[Topic Extraction]
    
    TopicExtraction --> TopicCheck{Required Topics Met?}
    TopicCheck -->|No| QueryRefiner
    TopicCheck -->|Yes| SyntheticGen[Synthetic Data Generation]
    
    SyntheticGen --> End([End])
    
    style Start fill:#1e4d7b
    style UserQuery fill:#1e4d7b
    style QueryParser fill:#1e4d7b
    style QueryRefiner fill:#1e4d7b
    style WebSearch fill:#1e4d7b
    style WebScraping fill:#1e4d7b
    style TopicExtraction fill:#1e4d7b
    style SyntheticGen fill:#1e4d7b
    style Exit fill:#1e4d7b
    style End fill:#1e4d7b
    style DataGen fill:#2d8659
    style TopicCheck fill:#2d8659
```

---

## âš¡ Quick Start

### Installation

```bash
pip install Data-Generation-Agents
```

### Basic Usage

```python
from Data_Generation_Agents import generate_synthetic_data

# Simple: Just provide your request
generate_synthetic_data(
    "your-prompt"
)
```

### Advanced Usage with Categories

```python
generate_synthetic_data(
    """Generate 5000 medical QA pairs in English covering categories: 
    cardiology, neurology, oncology. Use columns (Question, Answer).
    Include both short (1-2 sentences) and detailed (5-8 sentences) answers.""",
    refined_queries_count=50,      # More search queries for broader coverage
    search_results_per_query=8,    # More pages scraped per query
    rows_per_subtopic=5            # Data points per topic
)
```

### CLI Usage

```bash
synthetic-data "your-prompt"
```

---

## âš™ï¸ Configuration

### Environment Setup

Create `.env` file:

```env
# Required Configuration
GEMINI_API_KEY=your_gemini_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
SCRAPERAPI_API_KEY=your_scraper_api_key_here
OUTPUT_DIR=./data

# Optional Configuration
REFINED_QUERIES_COUNT=50
SEARCH_RESULTS_PER_QUERY=5
ROWS_PER_SUBTOPIC=5
GEMINI_TEMPERATURE=0.7
LOG_LEVEL=INFO
```

### ğŸ”‘ Get API Keys

1. **Gemini AI** 
   â†’ [Google AI Studio](https://makersuite.google.com/app/apikey)

2. **Tavily Search**
   â†’ [Tavily.com](https://tavily.com/)

3. **ScraperAPI**
   â†’ [ScraperAPI.com](https://www.scraperapi.com/)

---

## ğŸ”„ Fault-Tolerant Resumption Example

The pipeline **never loses progress**:

```python
# First run: Generates 1000 samples, interrupted after 400
generate_synthetic_data("Generate 1000 English QA pairs about Python...")

# Console output:
# âœ… Query parsed: 1000 samples, English, domain=programming
# âœ… 50 search queries generated
# âœ… 250 URLs found via Tavily
# âœ… 180 pages scraped successfully
# âœ… 2000 content chunks created
# âœ… 250 topics extracted
# ğŸ”„ Data generation: 400/1000 samples
# âš ï¸ PIPELINE STOPPED - API QUOTA EXHAUSTED
# Progress saved. Resume by running same command after quota resets.

# Second run (next day): Automatically resumes from topic 81 (if the ROWS_PER_SUBTOPIC=5)
generate_synthetic_data("Generate 1000 English QA pairs about Python...")

# Console output:
# â­ï¸ Resuming from: data_generation
# âœ… Loaded 250 topics from file
# ğŸ”„ Resuming from topic 81/250 (400 samples already generated)
# âœ… Data generation: 1000/1000 samples completed
```

**File Structure Created:**
```
data/
â””â”€â”€ {sha256_hash_of_query}/
    â”œâ”€â”€ pipeline_status.json          # Progress tracker
    â”œâ”€â”€ parsed_query.json             # Requirements
    â”œâ”€â”€ refined_queries.json          # Search queries
    â”œâ”€â”€ search_results.json           # Tavily results
    â”œâ”€â”€ filtered_results.json         # Filtered URLs
    â”œâ”€â”€ scraped_content.json          # Scraped pages
    â”œâ”€â”€ all_chunks.json               # Content chunks
    â”œâ”€â”€ extracted_topics.json         # topics
    â””â”€â”€ synthetic_data.json
```

---

## âš ï¸ Topic Extraction Requirement

**Critical**: Data generation requires **sufficient unique topics**.

### How It Works

```
Required Topics = Sample Count / ROWS_PER_SUBTOPIC
```

**Example:**
- Request: 5000 samples
- `ROWS_PER_SUBTOPIC = 5` (default)
- **Required**: 1000 unique topics

### Pipeline Behavior

```
# Pipeline will loop through content chunks until:
# 1. Required topics extracted (1000 topics) â†’ Move to data generation
# 2. All chunks processed but insufficient topics â†’ Restart with new queries
```

### For Large Datasets

```python
generate_synthetic_data(
    "Generate 10000 samples...",
    refined_queries_count=50,      # More search queries
    search_results_per_query=10,   # More pages scraped
    rows_per_subtopic=5            # Or increase to need fewer topics
)
```

**Pro Tip**: For very large datasets (10K+), increase `rows_per_subtopic` to 10 to reduce topic requirements (but may reduce diversity).

---

## ğŸ“Š Output Structure

Generated data saved as structured JSON:

```json
{
  "Question": "What is machine learning?",
  "Answer": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without explicit programming..."
}
```

For custom structures, specify in your query:
```
"Generate classification data with columns: text, category, confidence_score"
```

---

## ğŸ¯ Advanced Features

### Domain-Specific Generation

Pipeline adapts automatically to domains:
- **Technical**: Programming, DevOps, Cloud (e.g., "Python async/await patterns")
- **Medical**: Healthcare, Pharmaceuticals (e.g., "cardiology procedures")
- **Legal**: Law, Compliance (e.g., "GDPR regulations")
- **Financial**: Banking, Investment (e.g., "stock market analysis")
- **Education**: Learning materials (e.g., "calculus tutorials")

### Category Support

```python
# With categories (more targeted):
"Generate 2000 finance QA pairs in categories: cryptocurrency, stocks, bonds...."

# Without categories (broad coverage):
"Generate 2000 finance QA pairs....."
```

### Custom Gemini Models

```python
generate_synthetic_data(
    "Your query",
    gemini_model_name="gemini-1.5-pro"  # Default: gemini-2.5-flash
)
```

---

## ğŸ› ï¸ Development

### Local Setup

```bash
git clone https://github.com/Omar-YYoussef/Data_Gen_Agent
cd Data_Gen_Agent
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -e ".[dev]"
```

### Project Structure

```
src/Data_Generation_Agents/
â”œâ”€â”€ agents/                    # 7 AI agents
â”‚   â”œâ”€â”€ query_parser_agent.py
â”‚   â”œâ”€â”€ topic_extraction_agent.py
â”‚   â””â”€â”€ synthetic_data_generator_agent.py
â”œâ”€â”€ services/                  # External APIs
â”‚   â”œâ”€â”€ gemini_service.py      # Gemini AI
â”‚   â”œâ”€â”€ tavily_service.py      # Web search
â”‚   â””â”€â”€ scraper_api_service.py # Web scraping
â”œâ”€â”€ langgraph_integration/     # LangGraph workflow
â”‚   â”œâ”€â”€ graph_builder.py       # Graph construction
â”‚   â”œâ”€â”€ nodes.py               # Node implementations
â”‚   â””â”€â”€ state.py               # State definition
â”œâ”€â”€ utils/                     # Utilities
â”‚   â”œâ”€â”€ status_manager.py      # Progress tracking
â”‚   â”œâ”€â”€ data_saver.py          # File I/O
â”‚   â””â”€â”€ node_helpers.py        # Helper functions
â”œâ”€â”€ models/                    # Pydantic schemas
â”‚   â””â”€â”€ data_schemas.py
â””â”€â”€ workflows/                 # Orchestration
    â””â”€â”€ main_workflow.py
    â”œâ”€â”€ __main__.py                # CLI entry point
    â””â”€â”€ main.py    
```

## ğŸ¤ Contributing

We welcome contributions!

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push: `git push origin feature/amazing-feature`
5. Open Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file

---

## ğŸ™ Acknowledgments

- **Google Gemini** - Multilingual AI capabilities
- **Tavily** - AI-optimized web search
- **ScraperAPI** - Robust web scraping
- **LangGraph** - Stateful workflow orchestration

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/Omar-YYoussef/Data_Gen_Agent/issues)
- **Email**: omarjooo595@gmail.com
- **Documentation**: [Project Wiki](https://github.com/Omar-YYoussef/Data_Gen_Agent/wiki)

---

**Made with â¤ï¸ by [Omar Youssef](https://github.com/Omar-YYoussef)**

*Building the future of synthetic data generation, one pipeline at a time.*