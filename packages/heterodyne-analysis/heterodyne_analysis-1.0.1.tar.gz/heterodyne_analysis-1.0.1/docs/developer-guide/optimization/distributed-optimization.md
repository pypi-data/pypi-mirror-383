# Distributed Computing & ML-Accelerated Optimization Guide

## Overview

This guide covers the distributed computing and machine learning acceleration features
added to the heterodyne-analysis package. These features provide:

- **10-100x speedup** through multi-node distributed optimization
- **5-50x faster convergence** with ML-accelerated parameter prediction
- **Automatic backend detection** for Ray, MPI, Dask, and multiprocessing
- **Intelligent resource management** with system-aware configuration
- **Transfer learning** capabilities for similar experimental conditions

## Quick Start

### 1. Basic Distributed Optimization

```python
from heterodyne.optimization.utils import quick_setup_distributed_optimization

# Quick setup with automatic backend detection
coordinator = quick_setup_distributed_optimization(num_processes=8)

# Run distributed parameter sweep
parameter_ranges = {
    'D0': (1e-12, 1e-10, 10),
    'alpha': (0.5, 2.0, 5),
    'gamma_dot': (1, 100, 8)
}

results = coordinator.run_distributed_parameter_sweep(
    parameter_ranges,
    optimization_method="Nelder-Mead"
)

print(f"Best parameters: {results['best_result']['parameters']}")
print(f"Best chi-squared: {results['best_result']['objective_value']}")
```

### 2. ML-Accelerated Optimization

```python
from heterodyne.optimization.utils import quick_setup_ml_acceleration
from heterodyne.optimization.classical import ClassicalOptimizer

# Setup ML acceleration
ml_accelerator = quick_setup_ml_acceleration(
    data_path="./ml_optimization_data",
    enable_transfer_learning=True
)

# Enhance existing optimizer
enhanced_optimizer = ml_accelerator.accelerate_optimization(
    classical_optimizer,
    initial_parameters=np.array([1e-11, 1.0, 0.1]),
    experimental_conditions={
        'temperature': 25.0,
        'concentration': 0.08,
        'q_value': 0.12
    }
)
```

### 3. Combined Distributed + ML Optimization

```python
from heterodyne.optimization.utils import IntegrationHelper, OptimizationConfig

# Auto-configure based on system capabilities
config = IntegrationHelper.auto_configure_optimization(
    experimental_conditions={
        'parameter_count': 7,
        'optimization_history_size': 50
    }
)

# Enhance optimizer with both features
enhanced_optimizer = IntegrationHelper.enhance_optimizer(
    classical_optimizer,
    config=config,
    enable_distributed=True,
    enable_ml=True
)
```

## Advanced Configuration

### Configuration File Setup

Create a comprehensive configuration file `distributed_ml_config.json`:

```json
{
  "distributed_optimization": {
    "enabled": true,
    "backend_preference": ["ray", "mpi", "multiprocessing"],
    "ray_config": {
      "num_cpus": 16,
      "memory_mb": 32768
    },
    "load_balancing": {
      "strategy": "dynamic",
      "performance_monitoring": true
    },
    "fault_tolerance": {
      "max_retries": 3,
      "automatic_failover": true
    }
  },
  "ml_acceleration": {
    "enabled": true,
    "predictor_type": "ensemble",
    "enable_transfer_learning": true,
    "ml_model_config": {
      "hyperparameters": {
        "random_forest": {
          "n_estimators": 200,
          "max_depth": 15
        },
        "gradient_boosting": {
          "learning_rate": 0.05,
          "n_estimators": 150
        }
      },
      "enable_hyperparameter_tuning": true
    },
    "prediction_thresholds": {
      "confidence_threshold": 0.7,
      "uncertainty_threshold": 0.2
    }
  }
}
```

### Using Configuration

```python
from heterodyne.optimization.utils import OptimizationConfig

# Load configuration
config = OptimizationConfig("distributed_ml_config.json")

# Create optimized configuration for your system
optimized_config = SystemResourceDetector.optimize_configuration(config.config)
```

## Backend-Specific Setup

### Ray Cluster Setup

```python
# Local Ray cluster
from heterodyne.optimization.distributed import create_distributed_optimizer

ray_config = {
    'ray_config': {
        'num_cpus': 16,
        'memory_mb': 32768
    }
}

coordinator = create_distributed_optimizer(
    ray_config,
    backend_preference=['ray']
)
```

```bash
# Connect to existing Ray cluster
ray start --head --port=6379
export RAY_ADDRESS="ray://localhost:10001"
```

### MPI Setup

```python
# MPI configuration
mpi_config = {
    'mpi_config': {
        'hostfile': 'hostfile.txt',
        'np': 16
    }
}

coordinator = create_distributed_optimizer(
    mpi_config,
    backend_preference=['mpi']
)
```

```bash
# Run with MPI
mpirun -np 16 -hostfile hostfile.txt python optimization_script.py
```

## ML Model Training & Transfer Learning

### Training ML Predictors

```python
from heterodyne.optimization.ml_acceleration import MLAcceleratedOptimizer

# Create ML optimizer
ml_optimizer = MLAcceleratedOptimizer({
    'enable_transfer_learning': True,
    'data_storage_path': './ml_models'
})

# Train on historical optimization data
training_records = load_optimization_history()
training_result = ml_optimizer.train_predictor(training_records)

if training_result['success']:
    print(f"Model trained on {training_result['n_training_records']} records")
    print(f"Training time: {training_result['training_time']:.2f}s")
```

### Transfer Learning

```python
# Enable transfer learning for similar experimental conditions
transfer_config = {
    'enable_transfer_learning': True,
    'similarity_threshold': 0.8,
    'domain_adaptation': True
}

ml_optimizer = MLAcceleratedOptimizer(transfer_config)
```

## Performance Monitoring & Benchmarking

### System Resource Monitoring

```python
from heterodyne.optimization.utils import SystemResourceDetector

# Check system capabilities
capabilities = SystemResourceDetector.detect_system_capabilities()
requirements = SystemResourceDetector.check_system_requirements()

print(f"CPUs: {capabilities['cpu_count']}")
print(f"Memory: {capabilities['memory_total_gb']:.1f}GB")
print(f"Ray recommended: {requirements['ray_recommended']}")
```

### Optimization Benchmarking

```python
from heterodyne.optimization.utils import OptimizationBenchmark

benchmark = OptimizationBenchmark()

# Create benchmark suite
test_cases = [
    {'method': 'Classical', 'distributed': False, 'ml': False},
    {'method': 'Distributed', 'distributed': True, 'ml': False},
    {'method': 'ML-Accelerated', 'distributed': False, 'ml': True},
    {'method': 'Full-Enhanced', 'distributed': True, 'ml': True}
]

results = benchmark.compare_optimizers(test_cases)
print(f"Best method: {results['summary']['best_method']}")
```

## CLI Integration

### Enhanced CLI Commands

```bash
# Run optimization with distributed backend
heterodyne --config config.json --distributed --backend ray --workers 16

# Run with ML acceleration
heterodyne --config config.json --ml-accelerated --ml-config ml_config.json

# Combined distributed + ML
heterodyne --config config.json --distributed --ml-accelerated --auto-optimize

# Parameter sweep with distribution
heterodyne-sweep --parameter-ranges "D0:1e-12:1e-10:10,alpha:0.5:2.0:5" --distributed
```

### Configuration Generation

```bash
# Generate optimized configuration for your system
heterodyne-config --generate-distributed-ml --output distributed_config.json

# Auto-detect and optimize
heterodyne-config --auto-optimize --system-detect
```

## Error Handling & Troubleshooting

### Common Issues

1. **Backend Not Available**

```python
from heterodyne.optimization.distributed import get_available_backends

backends = get_available_backends()
if not backends['ray']:
    print("Ray not available, install with: pip install ray")
```

2. **Insufficient Memory for ML**

```python
from heterodyne.optimization.utils import SystemResourceDetector

requirements = SystemResourceDetector.check_system_requirements()
if not requirements['ml_recommended']:
    print("System may not have sufficient resources for ML acceleration")
```

3. **Configuration Validation**

```python
from heterodyne.optimization.utils import validate_configuration

is_valid, errors = validate_configuration(config)
if not is_valid:
    for error in errors:
        print(f"Configuration error: {error}")
```

### Debugging

```python
from heterodyne.optimization.utils import setup_logging_for_optimization

# Enable detailed logging
setup_logging_for_optimization(
    log_level='DEBUG',
    enable_distributed_logging=True,
    enable_ml_logging=True
)
```

## Performance Tips

### Optimization Strategies

1. **Use appropriate backend for your setup:**

   - **Ray**: Best for heterogeneous clusters, large memory, dynamic workloads
   - **MPI**: Best for HPC environments, homogeneous clusters
   - **Multiprocessing**: Best for single-node, simple parallelization

2. **ML acceleration works best when:**

   - You have >20 previous optimization runs
   - Experimental conditions have some similarity
   - Parameter space is not too high-dimensional (â‰¤10 parameters)

3. **Memory optimization:**

   - Use streaming for large parameter sweeps
   - Enable result compression for distributed storage
   - Configure appropriate batch sizes

### Performance Scaling

Expected performance improvements:

| Configuration | Speedup vs. Classical | Use Case |
|--------------|----------------------|----------| | Multiprocessing (4 cores) | 2-3x |
Local development | | Ray cluster (16 cores) | 8-12x | Medium workloads | | MPI HPC (64
cores) | 30-50x | Large parameter sweeps | | ML acceleration | 5-20x | Repeated similar
optimizations | | Combined distributed + ML | 50-200x | Production workflows |

## Integration Examples

### With Existing Heterodyne Workflow

```python
from heterodyne.analysis.core import HeterodyneAnalysisCore
from heterodyne.optimization.classical import ClassicalOptimizer
from heterodyne.optimization.utils import IntegrationHelper

# Load your existing analysis
core = HeterodyneAnalysisCore('config.json')
optimizer = ClassicalOptimizer(core, config)

# Enhance with new capabilities
enhanced_optimizer = IntegrationHelper.enhance_optimizer(
    optimizer,
    enable_distributed=True,
    enable_ml=True
)

# Run enhanced optimization
result = enhanced_optimizer.run_ml_accelerated_optimization(
    initial_parameters=initial_params,
    experimental_conditions=conditions
)
```

### Batch Processing Multiple Experiments

```python
# Process multiple experimental datasets efficiently
experiments = load_experiment_list()

for experiment in experiments:
    # Automatically configure for each experiment
    auto_config = IntegrationHelper.auto_configure_optimization(
        experiment['conditions']
    )

    # Run optimized analysis
    enhanced_optimizer = IntegrationHelper.create_enhanced_optimizer(
        ClassicalOptimizer,
        experiment['core'],
        experiment['config'],
        auto_config
    )

    results = enhanced_optimizer.run_optimization()
    save_results(experiment['id'], results)
```

## Requirements & Installation

### Required Dependencies

```bash
# Core distributed computing
pip install ray[default]  # Ray cluster support
pip install mpi4py       # MPI support
pip install dask[distributed]  # Dask support

# ML acceleration
pip install scikit-learn>=1.3.0
pip install xgboost
pip install optuna       # Hyperparameter optimization

# Performance monitoring
pip install psutil
```

### Optional Dependencies

```bash
# Advanced ML features
pip install torch        # PyTorch neural networks
pip install lightgbm     # LightGBM models

# Enhanced monitoring
pip install prometheus_client
pip install grafana-api
```

### Cluster Setup

For production clusters, see:

- [Ray Cluster Setup Guide](https://docs.ray.io/en/latest/cluster/getting-started.html)
- [MPI Configuration Guide](https://mpi4py.readthedocs.io/en/stable/)
- [Dask Distributed Setup](https://distributed.dask.org/en/latest/setup.html)

## Support & Contributing

For issues, feature requests, or contributions:

- GitHub Issues:
  [heterodyne-analysis issues](https://github.com/imewei/heterodyne/issues)
- Documentation: [Full API Documentation](https://heterodyne-analysis.readthedocs.io)
- Contact: Wei Chen (wchen@anl.gov), Hongrui He (hhe@anl.gov)

This advanced optimization framework represents a significant leap forward in
computational efficiency for X-ray photon correlation spectroscopy analysis under
nonequilibrium conditions.
