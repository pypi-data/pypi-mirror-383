\label{sec:Executive}
\subsection{Motivation}

% low latency searches -- both GW and EM
The scheduled resumption of data taking in late 2015\cite{LIGO-2013-WhitePaper-CoordinatedEMObserving}, with the second generation gravitational-wave interferometers in Hanford, Washington, and Livingston, Louisiana, are expected to reach sensitivities which are incrementally better than those achieved in earlier data taking runs\cite{s5s6}. The Virgo detector is also expected to resume data taking within a year of this milestone. In preparation for the next run, the LIGO and Virgo Collaborations have implemented and extensively tested a set of low latency gravitational-wave detection pipelines\cite{gstlal,mbta,cwb}, capable of compact binary event detection in latencies of a few minutes or less from the coalescence time. These pipelines trade the ability to accurately determine all but a few of the parameters of the coalescence for speed and breadth of analysis. Having a more accurate estimation of the parameters is valuable not only to gravitational-wave science, but also is useful to provide electromagnetic observatories information to aid in pointing. Many astrophysical phenomena which could create transient gravitational waves will also have electromagnetic signatures that may rapidly decay, and gravitational-wave interferometer networks often cannot localize GW events to better than a few hundred square degrees on the sky\cite{LIGO-2013-WhitePaper-CoordinatedEMObserving,first2years,gwastro-skyloc-Sidery2013,skylocpapers}, thus prompting follow-up observations occur expeditiously after the initial identification as well as localized promptly and accurately. While several Bayesian algorithms for GW parameter estimation have been employed in the past, the time scale of a full parameter estimation analysis remains at a much higher latency than the initial detection.

% MC integration scheme
%Given the inherent serial nature of current Bayesian parameter estimation schemes,  s
Significant reductions in overall computational time seem to  require  reduced the cost per likelihood computation or an
inherently parallel  parallel algorithm from the start. Our scheme accomplishes both.  
%
On the one hand, our scheme precomputes several quantities which appear in the likelihood, allowing us to  efficiently and
repeatedly evaluate it with little cost.  By contrast, most current schemes  require the costly evaluation of an inner
product per likelihood sample.  
%
%
On the other hand, our scheme parallelizes over intrinsic parameters (i.e., masses).    Because all waveforms are
generated once, in parallel, we mitigate the cost of waveform generation, to the point that even waveform families with
significant computational overhead per waveform are no serious obstacle.\footnote{Roughly speaking, more accurate and
complete physical models are more expensive to generate. Several waveform families representing the gravitational radiation from coalescing binaries exist and are in common use,
both for searches and followup parameter
estimation\cite{gw-astro-mergers-approximations-SpinningPNHigherHarmonics,gw-astro-PN-Comparison-AlessandraSathya2009,gw-astro-EOBspin-Tarrachini2012,gw-astro-EOBNR-Calibrated-2009}.}  This method is particularly beneficial as 
the low frequency sensitivity of the second generation interferometers evolves, requiring  waveform simulations to start
at lower frequencies and cover a significantly longer time interval, at correspondingly larger computational cost. 
%
By contrast, current parameter estimation schemes usually require millions of likelihood evaluations
to explore the entire space, with a correspondingly large number of  waveform generations.   Waveform generation cost
can dominate the overall cost of the calculation.

% waveform generation cost

% POINT: Not only accurate
Because we can direct evaluate the expected accuracy in each Monte Carlo integral, using the observed variance we can
insure our algorithm terminates only once  a target accuracy has been achieved.   Our robust error estimates are  vastly less complex than comparable algorithms to derive the evidence from MCMC\cite{gwastro-mergers-HeeSuk-CompareToPE-Aligned,mm-stats-MCMC-GeometricLaddear-Neal,mm-stats-MCMC-GeometricLadderChoices-Liu,mm-stats-EvidenceFromMCMC-Weinberg2009,2007PhRvD..75f2004R,2008ApJ...688L..61V,2009PhRvD..80f3007L, 2009CQGra..26k4007R,2011RvMP...83..943V}.

We also propose to make use of information that is retrieved from both the low-latency gravitational-wave search as well as any other low-latency process to augment our own results. From the gravitational-wave search, we obtain knowledge of the event time --- needed only in a general sense to restrict the amount of data processing involved to tractable levels --- and information regarding the masses, used later to center the search space in the searched intrinsic parameters. Fast algorithms which can produce an accurate estimate of the sky position using only the information provided from the GW search\cite{gw-astro-Bayestar} can also be utilized to speed up computation.

%% % TODO: Can we edit this down further?
%% {\color{blue} Evan thinks this is unclear and Chris agrees. Either edit it to be more concise or remove.}
%% Moreover, it is known\cite{glitch,detchar,glitch_pe} that the quality or stationarity of the data could influence the detection of an astrophysical event, or in the worst cases, trigger detection pipelines from the accidental coincidence of environmental or mechanical phenomenon which are not successfully cleaned from the data stream. The ability of a Bayesian parameter estimation algorithm to deal with such influences is an active area of study\cite{glitchfitting}. Even so, a detailed parameter estimation study can also influence the statistical significance of a putative event by downranking or upranking based on the model support from the parameter estimation itself.

