{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "72cf0370-2350-4eb3-9e76-5850adce7ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade zia-sdk-python[databricks] \n",
    "#%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2048ecbb-1f06-4926-ab51-f5e223936709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "# Base \n",
    "import datetime\n",
    "\n",
    "# Import data handling dependencies \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import zia-sdk depdendencies \n",
    "from neurolabszia import Zia, NLIRResult \n",
    "from neurolabszia.utils import to_spark_dataframe\n",
    "\n",
    "# 1. Get API key securely from Databricks secrets (should first be set)\n",
    "try:\n",
    "    api_key = dbutils.secrets.get(scope=\"neurolabs-api\", key=\"api_key\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to retrieve API key from Databricks secrets. Make sure the secret scope and key are set up.\") from e\n",
    "\n",
    "async def get_paginated_results(\n",
    "    client: Zia, task_uuid: str, batch_size: int = 10, max_offset: int = 800\n",
    ") -> list[NLIRResult]:\n",
    "    \"\"\"\n",
    "    Get all results from a task using pagination.\n",
    "\n",
    "    Args:\n",
    "        client: Zia client instance\n",
    "        task_uuid: The UUID of the task\n",
    "        batch_size: Number of results to fetch per request (default: 10)\n",
    "        max_offset: Maximum number of images to fetch results for (default: 100)\n",
    "    Returns:\n",
    "        List of all NLIRResult objects\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    offset = 0\n",
    "\n",
    "    print(f\"üîç Fetching paginated results for task: {task_uuid}\")\n",
    "    print(f\"üì¶ Batch size: {batch_size}\")\n",
    "\n",
    "    while True:\n",
    "        print(f\"\\nüìÑ Fetching batch at offset {offset}...\")\n",
    "\n",
    "        # Get a batch of results\n",
    "        batch = await client.result_management.get_task_results(\n",
    "            task_uuid=task_uuid, limit=batch_size, offset=offset\n",
    "        )\n",
    "\n",
    "        if not batch:\n",
    "            print(f\"‚úÖ No more results found at offset {offset}\")\n",
    "            break\n",
    "\n",
    "        print(f\"‚úÖ Retrieved {len(batch)} results\")\n",
    "        all_results.extend(batch)\n",
    "\n",
    "        # If we got fewer results than requested, we've reached the end\n",
    "        if len(batch) < batch_size or offset >= max_offset:\n",
    "            print(f\"‚úÖ Reached end of results (got {len(batch)} < {batch_size})\")\n",
    "            break\n",
    "\n",
    "        offset += batch_size\n",
    "\n",
    "    print(f\"\\nüéâ Total results retrieved: {len(all_results)}\")\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Databricks redacts secrets by default, print the first and last 4 characters of the API key \n",
    "#print(f\"API Key: {api_key[:4]}{'*' * (len(api_key) - 8)}{api_key[-4:]}\")\n",
    "\n",
    "#def load_json(path: str | Path):\n",
    "#    p = Path(path)\n",
    "#    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "#        return json.load(f)\n",
    "# results_path = [f\"/Volumes/catalog_integration/neurolabs_catalog/ir_results/ab_results20_{i}.json\" for i in range(0,7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910834f6-528e-45f6-af62-792387f7b9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run `get_paginated_results` when you want to get new data via Neurolabs API \n",
    "# task-uuid = \"xyz\"\n",
    "# client = Zia(api_key)\n",
    "# Before you update the API key via DB secrets, you can try here first. \n",
    "api_key = \"your-api-key\"\n",
    "task_uuid = \"your-task-uuid\"\n",
    "\n",
    "async def get_all():\n",
    "    \"\"\"Main function demonstrating paginated results usage.\"\"\"\n",
    "\n",
    "    print(\"üöÄ Zia SDK - Paginated Results Example\")\n",
    "    print(\"=\" * 60)\n",
    "    all_results = []\n",
    "    # Initialize client once and reuse it\n",
    "    async with Zia(api_key) as client:\n",
    "        # Example 1: Get all results with pagination\n",
    "        print(\"\\n1Ô∏è‚É£ Getting all paginated results:\")\n",
    "        try:\n",
    "            all_results = await get_paginated_results(client, task_uuid, batch_size=20)\n",
    "            print(f\"‚úÖ Successfully retrieved {len(all_results)} total results\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting all results: {e}\")\n",
    "\n",
    "    return all_results \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ Paginated results example complete!\")\n",
    "\n",
    "\n",
    "#all_results = []\n",
    "#for results in results_path: \n",
    "#    data = load_json(results)\n",
    "    # Parse into our NLB\n",
    "    # results = [NLIRResult.model_validate(result) for result in data[\"items\"]]\n",
    "    # all_results.extend(results)\n",
    "\n",
    "#print(f\"Total results retrieved: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8489760d-16ae-4ce2-a205-448a868c5c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_results = await get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f378d5-540f-45b6-8fa9-12fd2813e0d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f56d39-42b7-402f-b08b-57f2748be922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Convert IR Results into Spark Dataframe and Upload to Unity Catalog \n",
    "\n",
    "print(\"Execution started at:\", datetime.now())\n",
    "\n",
    "# 1. Create Spark session\n",
    "spark = SparkSession.builder.appName(\"NLIRResultsIngestion\").getOrCreate()\n",
    "\n",
    "# 2. Create Catalog, Schema & Table \n",
    "catalog_name = \"catalog_integration\"\n",
    "schema_name = \"neurolabs_ir_results_examples\" \n",
    "table_name = \"ir_results_customer_account_name\"\n",
    "\n",
    "# 3. Convert NLIRResults -> pd.Dataframe -> Spark Dataframe\n",
    "#pdf = ir_results_to_dataframe(all_results)\n",
    "#ir_results_schema = get_spark_schema_from_dataframe(pdf)\n",
    "df_spark = to_spark_dataframe(all_results, spark)\n",
    "# df_spark.head(2)\n",
    "\n",
    "# 4. Write to Databricks Delta table\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{table_name}\")\n",
    "\n",
    "print(f\"Successfully wrote {df_spark.count()} records to table {schema_name}.{table_name}.\") "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IR Results Neurolabs Ingestion [DB Integration]",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
