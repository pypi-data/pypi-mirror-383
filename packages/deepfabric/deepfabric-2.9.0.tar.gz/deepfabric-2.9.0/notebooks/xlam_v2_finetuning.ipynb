{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLAM 2.0 Function Calling Fine-tuning\n",
    "\n",
    "Fine-tune Qwen 2.5 Instruct on XLAM 2.0 (APIGen-MT) format dataset for improved function calling.\n",
    "\n",
    "**Model**: Qwen/Qwen2.5-7B-Instruct  \n",
    "**Method**: LoRA with 4-bit quantization  \n",
    "**Framework**: Unsloth for 2x faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q unsloth\n",
    "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration\nMODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\nMAX_SEQ_LENGTH = 2048\nLOAD_IN_4BIT = True\n\n# LoRA configuration (from APIGen-MT paper)\nLORA_R = 16\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.0\nTARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n\n# Training configuration\nBATCH_SIZE = 2\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 3\nWARMUP_STEPS = 10\nLOGGING_STEPS = 10\nSAVE_STEPS = 100\n\n# Dataset configuration\nHF_DATASET_REPO = None  # Set to your HuggingFace repo, e.g., \"username/xlam-dataset\"\nLOCAL_DATASET_PATH = \"xlam_v2_formatted.jsonl\"  # XLAM 2.0 formatted dataset\n\n# Output configuration\nOUTPUT_DIR = \"./xlam_checkpoints\"\nFINAL_MODEL_NAME = \"xlam-qwen2.5-7b-lora\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"4-bit quantization: {LOAD_IN_4BIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"LoRA configured with rank={LORA_R}, alpha={LORA_ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from HuggingFace or local file\n",
    "if HF_DATASET_REPO:\n",
    "    print(f\"Loading dataset from HuggingFace: {HF_DATASET_REPO}\")\n",
    "    dataset = load_dataset(HF_DATASET_REPO, split=\"train\")\n",
    "else:\n",
    "    print(f\"Loading dataset from local file: {LOCAL_DATASET_PATH}\")\n",
    "    # Read JSONL file\n",
    "    data = []\n",
    "    with open(LOCAL_DATASET_PATH) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    dataset = Dataset.from_list(data)\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"Example sample: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert XLAM 2.0 format to chat messages for training\ndef xlam_to_chat_messages(sample):\n    \"\"\"\n    Convert XLAM 2.0 format to chat messages.\n    \n    XLAM format has:\n    - conversations: list of turns with from/value\n    - tools: JSON string of tool definitions\n    - system: domain policy\n    \n    We convert to messages format for training.\n    \"\"\"\n    messages = []\n    \n    # Add system message (domain policy)\n    if sample.get(\"system\"):\n        messages.append({\n            \"role\": \"system\",\n            \"content\": sample[\"system\"]\n        })\n    \n    # Convert conversation turns to messages\n    role_mapping = {\n        \"human\": \"user\",\n        \"gpt\": \"assistant\",\n        \"function_call\": \"assistant\",\n        \"observation\": \"user\"\n    }\n    \n    for turn in sample[\"conversations\"]:\n        role = role_mapping.get(turn[\"from\"], \"user\")\n        messages.append({\n            \"role\": role,\n            \"content\": turn[\"value\"]\n        })\n    \n    return {\"messages\": messages}\n\n# Format messages into Qwen chat template\ndef format_chat_template(sample):\n    \"\"\"\n    Convert messages to Qwen 2.5 chat template format.\n    \n    Qwen format:\n    <|im_start|>system\n    {system_message}<|im_end|>\n    <|im_start|>user\n    {user_message}<|im_end|>\n    <|im_start|>assistant\n    {assistant_message}<|im_end|>\n    \"\"\"\n    messages = sample[\"messages\"]\n    \n    # Use tokenizer's chat template if available\n    if hasattr(tokenizer, \"apply_chat_template\"):\n        formatted = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=False\n        )\n    else:\n        # Manual formatting\n        formatted = \"\"\n        for msg in messages:\n            role = msg[\"role\"]\n            content = msg[\"content\"]\n            formatted += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n    \n    return {\"text\": formatted}\n\n# Convert XLAM to messages, then to chat template\ndataset = dataset.map(xlam_to_chat_messages)\ndataset = dataset.map(format_chat_template, remove_columns=dataset.column_names)\nprint(f\"Dataset formatted for training\")\nprint(f\"Example formatted text:\\n{dataset[0]['text'][:500]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    report_to=\"none\",  # Change to \"wandb\" if you want logging\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=False,  # Disable packing for function calling (needs clear boundaries)\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU memory before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Memory: {start_gpu_memory} GB / {max_memory} GB reserved\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show GPU memory after training\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Peak memory reserved: {used_memory} GB ({used_percentage}%)\")\n",
    "print(f\"Memory used for training: {used_memory_for_training} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(FINAL_MODEL_NAME)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_NAME)\n",
    "print(f\"Model saved to {FINAL_MODEL_NAME}\")\n",
    "\n",
    "# Optionally push to HuggingFace Hub\n",
    "# model.push_to_hub(\"your-username/xlam-qwen2.5-7b-lora\", token=\"your_token\")\n",
    "# tokenizer.push_to_hub(\"your-username/xlam-qwen2.5-7b-lora\", token=\"your_token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test tools\n",
    "test_tools = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather for a location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City name\"},\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test message\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with access to tools.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Available tools:\\n{json.dumps(test_tools, indent=2)}\\n\\nUser: What's the weather in San Francisco?\"}\n",
    "]\n",
    "\n",
    "# Format with chat template\n",
    "test_input = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    test_input,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n=== Test Inference ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation (Optional)\n",
    "\n",
    "Compare fine-tuned model vs base model on function calling accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation test cases\n",
    "eval_cases = [\n",
    "    {\n",
    "        \"query\": \"Book a flight from NYC to LAX for tomorrow\",\n",
    "        \"expected_tool\": \"book_flight\",\n",
    "        \"tools\": [\n",
    "            {\"name\": \"book_flight\", \"description\": \"Book airline tickets\"},\n",
    "            {\"name\": \"get_weather\", \"description\": \"Get weather info\"},\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the temperature in Paris?\",\n",
    "        \"expected_tool\": \"get_weather\",\n",
    "        \"tools\": [\n",
    "            {\"name\": \"get_weather\", \"description\": \"Get weather info\"},\n",
    "            {\"name\": \"book_hotel\", \"description\": \"Book hotel rooms\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "def evaluate_tool_calling(model, tokenizer, test_cases):\n",
    "    \"\"\"Evaluate model's tool calling accuracy.\"\"\"\n",
    "    correct = 0\n",
    "    total = len(test_cases)\n",
    "    \n",
    "    for case in test_cases:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Available tools:\\n{json.dumps(case['tools'])}\\n\\nUser: {case['query']}\"}\n",
    "        ]\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(inputs, max_new_tokens=128, temperature=0.1)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Check if expected tool is called\n",
    "        if case['expected_tool'] in response:\n",
    "            correct += 1\n",
    "            print(f\"✓ {case['query']} -> {case['expected_tool']}\")\n",
    "        else:\n",
    "            print(f\"✗ {case['query']} -> Expected {case['expected_tool']}, got: {response[:100]}\")\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"\\nAccuracy: {accuracy:.1f}% ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "# Run evaluation\n",
    "print(\"=== Evaluating Fine-tuned Model ===\")\n",
    "finetuned_accuracy = evaluate_tool_calling(model, tokenizer, eval_cases)\n",
    "\n",
    "# Compare with base model\n",
    "print(\"\\n=== Loading Base Model for Comparison ===\")\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "FastLanguageModel.for_inference(base_model)\n",
    "\n",
    "print(\"\\n=== Evaluating Base Model ===\")\n",
    "base_accuracy = evaluate_tool_calling(base_model, base_tokenizer, eval_cases)\n",
    "\n",
    "print(f\"\\n=== Results ===\")\n",
    "print(f\"Base Model Accuracy: {base_accuracy:.1f}%\")\n",
    "print(f\"Fine-tuned Model Accuracy: {finetuned_accuracy:.1f}%\")\n",
    "print(f\"Improvement: {finetuned_accuracy - base_accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export for Production\n",
    "\n",
    "Merge LoRA weights with base model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save full model (larger but faster inference)\n",
    "model.save_pretrained_merged(\n",
    "    f\"{FINAL_MODEL_NAME}-merged\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",  # or \"merged_4bit\" for smaller size\n",
    ")\n",
    "print(f\"Merged model saved to {FINAL_MODEL_NAME}-merged\")\n",
    "\n",
    "# Save in GGUF format for llama.cpp (optional)\n",
    "# model.save_pretrained_gguf(\n",
    "#     f\"{FINAL_MODEL_NAME}-gguf\",\n",
    "#     tokenizer,\n",
    "#     quantization_method=\"q4_k_m\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}