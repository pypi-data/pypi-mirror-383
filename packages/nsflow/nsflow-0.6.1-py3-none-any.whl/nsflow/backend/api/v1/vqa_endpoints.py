# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# nsflow SDK Software in commercial settings.
#
# END COPYRIGHT
import logging
import os
import subprocess
import tempfile
from typing import Annotated
from typing import Optional

from fastapi import APIRouter
from fastapi import File
from fastapi import Form
from fastapi import HTTPException
from fastapi import UploadFile
from fastapi.responses import JSONResponse
from pydantic import StringConstraints
from werkzeug.utils import secure_filename

logging.basicConfig(level=logging.INFO)

# Adjust these to your repo/paths
working_directory = os.getcwd()
REPO_DIR = os.path.join(working_directory, "..", "ml-fastvlm")
PREDICT = os.path.join(REPO_DIR, "predict.py")
MODEL_PATH = os.path.join(REPO_DIR, "checkpoints")
DEFAULT_MODEL_NAME = "llava-fastvithd_0.5b_stage3"
PYTHON_CMD = os.path.join(REPO_DIR, "venv/bin/python")
ACCEPTABLE_MODEL_NAMES = [
    "llava-fastvithd_1.5b_stage2",
    "llava-fastvithd_7b_stage3",
    "llava-fastvithd_0.5b_stage2",
    "llava-fastvithd_1.5b_stage3",
    "llava-fastvithd_0.5b_stage3",
    "llava-fastvithd_7b_stage2",
]

router = APIRouter(prefix="/api/v1")

QuestionField = Annotated[str, StringConstraints(min_length=3, max_length=100, strip_whitespace=True)]


@router.post("/vqa")
async def vqa(
    question: QuestionField = Form(...),
    image: UploadFile = File(...),
    model_name: Optional[str] = Form(None),
    timeout_sec: int = Form(120),
):
    """
    Answer a query about the image using the model

    Args:
        question: Query about the image. Response generated by the model
        image: Image we have a query about
        model_path: Path to the model to be used to answer the query
        timeout_sec: timeout in seconds


    Returns:
        JSON response containing the answer in text

    To test the endpoint with curl

    curl -X POST http://127.0.0.1:8005/api/v1/vqa \
        -F 'question=How many people are in the photo?' \
        -F 'image=@/Users/joe/vacation.jpg' \
        -F 'model_name=llava-fastvithd_7b_stage3'
    """
    if model_name is None:
        model_name = DEFAULT_MODEL_NAME
    if model_name not in ACCEPTABLE_MODEL_NAMES:
        raise HTTPException(500, f"Invalid model name {model_name}!")
    model_name = secure_filename(model_name)
    model = os.path.join(MODEL_PATH, model_name)
    print(f"model: {model}")

    if not os.path.exists(PREDICT):
        raise HTTPException(500, f"predict.py not found at {PREDICT}")
    if not os.path.exists(model):
        raise HTTPException(500, f"model_path not found: {model}")

    # Save uploaded image to a temp file
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(image.filename or "")[-1]) as tmp:
            img_path = tmp.name
            content = await image.read()
            tmp.write(content)
    except Exception as e:
        raise HTTPException(400, f"Failed to save uploaded image: {e}") from e

    # Call predict.py
    cmd = [
        PYTHON_CMD,
        PREDICT,
        "--model-path",
        model,
        "--image-file",
        img_path,
        "--prompt",
        question,
    ]

    try:
        proc = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout_sec,
            check=False,  # don’t raise automatically; we’ll return stderr if needed
        )
    except subprocess.TimeoutExpired as e:
        os.unlink(img_path)
        raise HTTPException(504, f"Prediction timed out after {timeout_sec}s") from e

    # Clean up the temp file
    try:
        os.unlink(img_path)
    except Exception:
        pass

    # Parse/return
    payload = {
        "exit_code": proc.returncode,
        "stdout": proc.stdout.strip(),
        "stderr": proc.stderr.strip(),
    }
    if proc.returncode != 0:
        # Bubble up a 500 with stderr for easier debugging
        raise HTTPException(500, detail=payload)

    # If predict.py prints extra text, you can post-process `proc.stdout` here.
    return JSONResponse(
        {
            "answer": proc.stdout.strip(),
            "model_path": model,
        }
    )
