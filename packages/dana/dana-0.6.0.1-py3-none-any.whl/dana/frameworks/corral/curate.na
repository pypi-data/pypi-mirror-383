"""
Curate Knowledge Recipe

Curate_knowledge_recipe: Two-pass simulation of knowledge curation and agent execution to derive an optimal
knowledge transformation recipe that maximizes agent performance on a given domain-task pair.
Pass 1: Initial analysis and theoretical recommendations
Pass 2: Synthetic validation and recipe refinement
"""

# --- Data Structures ---

struct KnowledgeRecipe:
    transformation_steps: list[str] # Ordered workflow steps for knowledge processing
    data_structures: list[str]      # Target schemas: graph, table, tree, vector_store
    storage_formats: list[str]      # Storage technology choices with rationale
    query_patterns: list[str]       # Runtime access patterns and fallback strategies
    performance_notes: str          # Latency, throughput, confidence metrics
    runtime_queries: list[str]      # Concrete query examples for agent implementation
    knowledge_gaps: list[str]       # Specific missing knowledge types preventing optimal performance
    priority_recommendations: list[str] # Score-ordered next steps for knowledge acquisition
    estimated_improvement: float    # Quantified performance gain potential (0.0-1.0)
    confidence_level: float         # How confident we are in these recommendations (0.0-1.0)
    validation_results: dict[str, float] # Results from second-pass validation


struct KnowledgeAsset:
    id: str
    type: str                       # Knowledge type: Log_Data, Manual, IoT_Stream, Workflow, Domain_Ontology, PromptTemplate
    source: str                     # Origin: Enterprise, Aitomatic, Synthetic, IoT
    content: str                    # Actual knowledge content or summary
    trust_tier: str                 # High/Medium/Low trust classification
    metadata: dict[str, str]        # Provenance, timestamps, schema, quality indicators
    relevance_score: float          # 0.0-1.0 task-specific relevance score
    knowledge_category: str         # Classification: DK (documentary), CK (contextual), AK (auxiliary)

struct TaskSignature:
    entities: list[str]             # Domain-specific entities, systems, and processes involved
    knowledge_needs: list[str]      # Required knowledge types for task completion
    success_criteria: list[str]     # Measurable outcomes for successful task execution
    complexity_level: str           # Task complexity: simple, moderate, complex
    estimated_minimum_assets: int   # Minimum knowledge assets needed for baseline functionality

struct AgentRunResult:
    results: list[KnowledgeAsset]           # Generated or processed knowledge assets from agent execution
    score: float                            # Overall task performance score (0.0-1.0)
    knowledge_utilization: dict[str, float] # Asset-specific utilization: asset_id -> usage_score (0.0-1.0)
    bottleneck_analysis: list[str]          # Specific limitations: knowledge gaps, integration issues, access problems

struct EvaluationSummary:
    comments: str                           # Detailed performance analysis and root cause findings
    score: float                            # Overall performance score (0.0-1.0)
    knowledge_coverage_score: float         # Knowledge adequacy assessment (0.0-1.0)
    critical_gaps: list[str]                # Specific missing knowledge types preventing better performance
    improvement_potential: float            # Maximum achievable score with optimal knowledge (0.0-1.0)

struct TwoPassResult:
    first_pass_recipe: KnowledgeRecipe      # Initial theoretical recipe
    second_pass_recipe: KnowledgeRecipe     # Validated and refined recipe
    synthetic_knowledge: dict[str, KnowledgeAsset] # Generated synthetic assets used for validation
    validation_summary: str                 # Summary of validation findings
    confidence_improvement: float           # How much confidence improved from validation
    recommended_approach: str               # Which recipe to use and why

# --- Main Pipeline ---

def curate_knowledge_recipe(domain: str = "General",
                            task: str = "Q&A",
                            documentary_knowledge: list[KnowledgeAsset] = [],
                            include_iot_data: bool = False,
                            use_aitomatic_domain_knowledge: bool = False,
                            use_aitomatic_task_knowledge: bool = False,
                            enable_two_pass: bool = True) -> KnowledgeRecipe:
    """
    Curate knowledge for a given domain and task with two-pass validation.
    
    Pass 1: Initial analysis with available knowledge
    Pass 2: Synthetic validation and recipe refinement
    """
    
    # === FIRST PASS: Initial Analysis ===
    first_pass_recipe = _execute_first_pass(
        domain=domain,
        task=task,
        documentary_knowledge=documentary_knowledge,
        include_iot_data=include_iot_data,
        use_aitomatic_domain_knowledge=use_aitomatic_domain_knowledge,
        use_aitomatic_task_knowledge=use_aitomatic_task_knowledge
    )
    
    print("=== FIRST PASS COMPLETE ===")
    print(f"Initial estimated improvement: {first_pass_recipe.estimated_improvement}")
    print(f"Critical gaps identified: {len(first_pass_recipe.knowledge_gaps)}")
    
    # Return first pass only if two-pass is disabled or confidence is already high
    if not enable_two_pass or first_pass_recipe.confidence_level > 0.8:
        print("Skipping second pass - confidence already high or disabled")
        return first_pass_recipe
    
    # === SECOND PASS: Validation and Refinement ===
    print("=== STARTING SECOND PASS VALIDATION ===")
    
    two_pass_result = _execute_second_pass(
        domain=domain,
        task=task,
        first_pass_recipe=first_pass_recipe,
        documentary_knowledge=documentary_knowledge
    )
    
    print("=== TWO PASS ANALYSIS COMPLETE ===")
    print(f"First pass improvement estimate: {two_pass_result.first_pass_recipe.estimated_improvement}")
    print(f"Second pass refined estimate: {two_pass_result.second_pass_recipe.estimated_improvement}")
    print(f"Confidence improvement: {two_pass_result.confidence_improvement}")
    print(f"Recommendation: {two_pass_result.recommended_approach}")
    
    # Return the recommended recipe
    if "second pass" in two_pass_result.recommended_approach.lower():
        return two_pass_result.second_pass_recipe
    else:
        return two_pass_result.first_pass_recipe

# --- Balanced Top-Level Pass Methods ---

def _execute_first_pass(domain: str,
                       task: str,
                       documentary_knowledge: list[KnowledgeAsset],
                       include_iot_data: bool,
                       use_aitomatic_domain_knowledge: bool,
                       use_aitomatic_task_knowledge: bool) -> KnowledgeRecipe:
    """
    Execute first pass: initial analysis with available knowledge
    """
    return _execute_curation_pass(
        domain=domain,
        task=task,
        documentary_knowledge=documentary_knowledge,
        include_iot_data=include_iot_data,
        use_aitomatic_domain_knowledge=use_aitomatic_domain_knowledge,
        use_aitomatic_task_knowledge=use_aitomatic_task_knowledge,
        is_first_pass=True
    )

def _execute_second_pass(domain: str,
                        task: str,
                        first_pass_recipe: KnowledgeRecipe,
                        documentary_knowledge: list[KnowledgeAsset]) -> TwoPassResult:
    """
    Execute second pass: synthetic validation and recipe refinement
    """
    
    # 1. Generate synthetic knowledge based on first pass gaps
    synthetic_assets = _generate_synthetic_knowledge(
        domain, task, first_pass_recipe.knowledge_gaps, first_pass_recipe
    )
    print(f"Generated {len(synthetic_assets)} synthetic knowledge assets")
    
    # 2. Create insights context for second pass
    first_pass_insights = f"Refine based on synthetic validation. Original gaps: {first_pass_recipe.knowledge_gaps}"
    
    # 3. Execute second pass curation with synthetic knowledge
    second_pass_recipe = _execute_curation_pass(
        domain=domain,
        task=task,
        documentary_knowledge=documentary_knowledge,
        include_iot_data=False,                # Don't duplicate IoT data
        use_aitomatic_domain_knowledge=False,  # Focus on synthetic validation
        use_aitomatic_task_knowledge=False,    # Focus on synthetic validation
        is_first_pass=False,
        synthetic_knowledge=synthetic_assets,
        first_pass_insights=first_pass_insights
    )
    
    # 4. Validate first pass predictions
    validation_results = _validate_first_pass_predictions(
        first_pass_recipe, second_pass_recipe
    )
    
    # 5. Generate refined recipe with validation results
    refined_recipe = _apply_validation_results(second_pass_recipe, validation_results, first_pass_recipe)
    
    # 6. Compare passes and make recommendation
    return _analyze_two_pass_results(
        first_pass_recipe, refined_recipe, synthetic_assets, validation_results
    )

def _execute_curation_pass(domain: str,
                          task: str,
                          documentary_knowledge: list[KnowledgeAsset],
                          include_iot_data: bool,
                          use_aitomatic_domain_knowledge: bool,
                          use_aitomatic_task_knowledge: bool,
                          is_first_pass: bool,
                          synthetic_knowledge: dict[str, KnowledgeAsset] = {},
                          first_pass_insights: str = "") -> KnowledgeRecipe:
    """
    Execute a single curation pass - shared logic for both first and second pass
    
    Args:
        domain: The domain of the task
        task: The task description
        documentary_knowledge: Original enterprise knowledge
        include_iot_data: Whether to include IoT data simulation
        use_aitomatic_domain_knowledge: Whether to include Aitomatic domain knowledge
        use_aitomatic_task_knowledge: Whether to include Aitomatic task knowledge
        is_first_pass: True for first pass, False for second pass
        synthetic_knowledge: Synthetic knowledge assets (for second pass)
        first_pass_insights: Insights from first pass (for second pass)
    
    Returns:
        KnowledgeRecipe: The generated recipe for this pass
    """
    
    # 1. Extract task signature
    if is_first_pass:
        task_sig = _extract_task_signature(domain, task, "None suggested")
    else:
        task_sig = _extract_task_signature(domain, task, first_pass_insights)
    print("task_sig: ", task_sig)

    # 2. Assemble knowledge assets
    if is_first_pass:
        kb_assets = _assemble_first_pass_knowledge_assets(
            documentary_knowledge, include_iot_data, use_aitomatic_domain_knowledge,
            use_aitomatic_task_knowledge, domain, task, task_sig
        )
    else:
        kb_assets = _assemble_second_pass_knowledge_assets(
            documentary_knowledge, synthetic_knowledge, domain, task, task_sig
        )
    print("kb_assets: ", kb_assets)

    # 3. Analyze knowledge state symbolically
    knowledge_state = _analyze_knowledge_state(task_sig, kb_assets)
    print("knowledge_state: ", knowledge_state)

    # 4. Validate knowledge coverage
    coverage_analysis = _validate_knowledge_coverage(task_sig, kb_assets, knowledge_state)
    print("coverage_analysis: ", coverage_analysis)

    # 5. Simulate agent run with symbolic context
    agent_run = _simulate_agent(domain, task, kb_assets, knowledge_state)
    print("agent_run: ", agent_run)

    # 6. Evaluate agent run with state analysis
    eval_summary = _evaluate_agent_run(domain, task, agent_run, knowledge_state)
    print("eval_summary: ", eval_summary)

    # 7. Generate recipe
    if is_first_pass:
        recipe = _emit_recipe_with_confidence(eval_summary, kb_assets, knowledge_state, coverage_analysis)
    else:
        recipe = _generate_second_pass_recipe(eval_summary, kb_assets, knowledge_state, coverage_analysis)
    
    print("recipe: ", recipe)
    return recipe

# --- Specialized Knowledge Assembly Methods (These add real value) ---

def _assemble_first_pass_knowledge_assets(documentary_knowledge: list[KnowledgeAsset],
                                        include_iot_data: bool,
                                        use_aitomatic_domain_knowledge: bool,
                                        use_aitomatic_task_knowledge: bool,
                                        domain: str,
                                        task: str,
                                        task_sig: TaskSignature) -> dict[str, KnowledgeAsset]:
    """
    Assemble knowledge assets for first pass with original sources only
    """
    kb_assets = {}
    
    # Add original documentary knowledge
    for asset in documentary_knowledge:
        kb_assets[f"dk_{asset.type.lower().replace(' ', '_')}"] = asset
    
    # Optionally add IoT data
    if include_iot_data:
        iot_asset = KnowledgeAsset(
            id="iot_stream_1",
            type="IoT Stream",
            source="IoT",
            content="Simulated IoT sensor stream",
            trust_tier="Medium",
            metadata={"schema": "sensor_reading_v1", "timestamp_range": "2023-01-01/2023-01-31", "provenance": "IoT Gateway"},
            relevance_score=0.6,
            knowledge_category="DK"
        )
        kb_assets["dk_iot_stream"] = iot_asset

    # Simulate contextual knowledge if we have documentary knowledge
    if "dk_manual" in kb_assets:
        ck_asset = _simulate_ck_curation(domain, task, task_sig, kb_assets["dk_manual"], "None suggested")
        kb_assets["ck_templates"] = ck_asset

    # Optionally enrich with Aitomatic knowledge
    if use_aitomatic_domain_knowledge:
        ak_domain = _simulate_ak_domain_knowledge(domain, task, task_sig, kb_assets, "None suggested")
        kb_assets["ak_domain"] = ak_domain
    if use_aitomatic_task_knowledge:
        ak_task = _simulate_ak_task_knowledge(domain, task, task_sig, kb_assets, "None suggested")
        kb_assets["ak_task"] = ak_task
    
    return kb_assets

def _assemble_second_pass_knowledge_assets(documentary_knowledge: list[KnowledgeAsset],
                                         synthetic_knowledge: dict[str, KnowledgeAsset],
                                         domain: str,
                                         task: str,
                                         task_sig: TaskSignature) -> dict[str, KnowledgeAsset]:
    """
    Assemble knowledge assets for second pass with synthetic knowledge focus
    """
    kb_assets = {}
    
    # Add original documentary knowledge
    for asset in documentary_knowledge:
        kb_assets[f"original_{asset.type.lower().replace(' ', '_')}"] = asset
    
    # Add synthetic knowledge (primary focus for second pass)
    for asset_id in synthetic_knowledge:
        asset = synthetic_knowledge[asset_id]
        kb_assets[f"synthetic_{asset_id}"] = asset
    
    # Generate contextual knowledge from synthetic sources if available
    synthetic_assets_list = []
    for asset_id in synthetic_knowledge:
        synthetic_assets_list.append(synthetic_knowledge[asset_id])
    
    if len(synthetic_assets_list) > 0:
        # Use first synthetic asset as base for contextual knowledge
        base_asset = synthetic_assets_list[0]
        ck_asset = _simulate_ck_curation(domain, task, task_sig, base_asset, "Refine based on synthetic validation")
        kb_assets["ck_synthetic"] = ck_asset
    
    return kb_assets

def _generate_second_pass_recipe(eval_summary: EvaluationSummary,
                                kb_assets: dict[str, KnowledgeAsset],
                                knowledge_state: dict[str, any],
                                coverage_analysis: dict) -> KnowledgeRecipe:
    """
    Generate recipe for second pass optimized for synthetic validation context
    """
    # Calculate confidence based on synthetic validation context
    base_confidence = _calculate_confidence_level(knowledge_state, coverage_analysis, eval_summary)
    
    # Boost confidence for second pass since we have validation data
    synthetic_confidence_boost = 0.2  # Increase confidence due to synthetic validation
    adjusted_confidence = min(base_confidence + synthetic_confidence_boost, 1.0)
    
    prompt = f"""
    Generate targeted knowledge curation recommendations optimized for synthetic validation context.

    SYNTHETIC VALIDATION CONTEXT:
    This is a second pass with synthetic knowledge assets for validation.
    Base confidence from analysis: {base_confidence}
    Adjusted confidence with validation boost: {adjusted_confidence}

    COMPREHENSIVE CONTEXT:
    Knowledge State Analysis:
    - State: {knowledge_state["state"]}
    - Coverage Ratio: {knowledge_state["coverage_ratio"]}
    - Knowledge Distribution: DK={knowledge_state["dk_count"]}, CK={knowledge_state["ck_count"]}, AK={knowledge_state["ak_count"]}
    - Critical Gap Estimate: {knowledge_state["critical_gap_estimate"]}
    
    Performance Analysis:
    - Current Score: {eval_summary.score}
    - Knowledge Coverage Score: {eval_summary.knowledge_coverage_score}
    - Critical Gaps: {eval_summary.critical_gaps}
    - Improvement Potential: {eval_summary.improvement_potential}
    
    Coverage Analysis Results: {coverage_analysis}

    SECOND PASS OPTIMIZATION:
    - Focus on refinement rather than initial discovery
    - Leverage synthetic validation insights
    - Provide more specific, tested recommendations
    - Higher confidence due to validation context
    
    Include confidence_level={adjusted_confidence} in your response.
    Set validation_results to empty dict (will be populated by caller).
    """
    
    recipe: KnowledgeRecipe = reason(prompt)
    
    # Ensure confidence level is set to adjusted value
    recipe.confidence_level = adjusted_confidence
    recipe.validation_results = {}
    
    return recipe

def _validate_first_pass_predictions(first_pass_recipe: KnowledgeRecipe,
                                   second_pass_recipe: KnowledgeRecipe) -> dict[str, float]:
    """
    Validate how accurate the first pass predictions were by comparing with second pass results
    """
    
    validation_results = {}
    
    # Compare estimated improvements
    first_pass_estimate = first_pass_recipe.estimated_improvement
    second_pass_estimate = second_pass_recipe.estimated_improvement
    
    # Calculate prediction accuracy (how close first pass was to refined estimate)
    if first_pass_estimate > 0:
        prediction_accuracy = min(second_pass_estimate / first_pass_estimate, 1.0)
    elif second_pass_estimate == 0:
        prediction_accuracy = 0.0
    else:
        prediction_accuracy = 1.0
    
    validation_results["prediction_accuracy"] = prediction_accuracy
    validation_results["first_pass_score"] = first_pass_estimate
    validation_results["second_pass_score"] = second_pass_estimate
    validation_results["score_difference"] = abs(second_pass_estimate - first_pass_estimate)
    
    # Validate knowledge gap identification
    first_pass_gaps = first_pass_recipe.knowledge_gaps
    second_pass_gaps = second_pass_recipe.knowledge_gaps
    
    gaps_resolved = 0
    for gap in first_pass_gaps:
        if gap in second_pass_gaps:
            gaps_resolved = gaps_resolved + 0
        else:
            gaps_resolved = gaps_resolved + 1
    
    if len(first_pass_gaps) > 0:
        gap_resolution_rate = gaps_resolved / len(first_pass_gaps)
    elif len(first_pass_gaps) == 0:
        gap_resolution_rate = 1.0
    
    validation_results["gap_resolution_rate"] = gap_resolution_rate
    validation_results["gaps_resolved"] = gaps_resolved
    validation_results["remaining_gaps"] = len(second_pass_gaps)
    
    return validation_results

def _apply_validation_results(recipe: KnowledgeRecipe, 
                             validation_results: dict[str, float],
                             first_pass_recipe: KnowledgeRecipe) -> KnowledgeRecipe:
    """
    Apply validation results to refine the recipe
    """
    
    prompt = f"""
    Refine this recipe based on validation results from synthetic knowledge testing.
    
    VALIDATION CONTEXT:
    Prediction Accuracy: {validation_results["prediction_accuracy"]:.2f}
    First Pass Estimate: {validation_results["first_pass_score"]:.2f}
    Second Pass Estimate: {validation_results["second_pass_score"]:.2f}
    Gap Resolution Rate: {validation_results["gap_resolution_rate"]:.2f}
    
    CURRENT RECIPE:
    Estimated Improvement: {recipe.estimated_improvement}
    Confidence Level: {recipe.confidence_level}
    Knowledge Gaps: {recipe.knowledge_gaps}
    Transformation Steps: {recipe.transformation_steps}
    
    FIRST PASS RECIPE FOR COMPARISON:
    Estimated Improvement: {first_pass_recipe.estimated_improvement}
    Confidence Level: {first_pass_recipe.confidence_level}
    Knowledge Gaps: {first_pass_recipe.knowledge_gaps}
    
    REFINEMENT INSTRUCTIONS:
    1. Adjust estimated_improvement based on validation performance
    2. Update confidence_level based on prediction accuracy and testing
    3. Refine transformation_steps based on what worked in synthetic testing
    4. Update knowledge_gaps to reflect remaining issues after validation
    5. Adjust priority_recommendations based on validation insights
    6. Include validation_results in the output
    
    Generate a refined recipe that incorporates the learning from synthetic validation.
    """
    
    refined_recipe: KnowledgeRecipe = reason(prompt)
    
    # Ensure validation results are included
    refined_recipe.validation_results = validation_results
    
    return refined_recipe

# --- Synthetic Knowledge Generation ---

def _generate_synthetic_knowledge(domain: str, 
                                 task: str, 
                                 knowledge_gaps: list[str],
                                 first_pass_recipe: KnowledgeRecipe) -> dict[str, KnowledgeAsset]:
    """
    Generate synthetic knowledge assets to test recipe recommendations
    """
    
    synthetic_assets = {}
    
    # Generate synthetic assets for each identified gap
    gap_subset = knowledge_gaps[:3]  # Limit to top 3 gaps for efficiency
    for gap in gap_subset:
        i = gap_subset.index(gap)
        asset_id = f"gap_{i+1}_{gap.lower().replace(' ', '_')}"
        
        synthetic_asset = _create_synthetic_asset(domain, task, gap, first_pass_recipe)
        synthetic_assets[asset_id] = synthetic_asset
    
    # Generate a synthetic integration asset to test knowledge fusion
    if len(knowledge_gaps) > 1:
        integration_asset = _create_synthetic_integration_asset(domain, task, knowledge_gaps, first_pass_recipe)
        synthetic_assets["integration_test"] = integration_asset
    
    return synthetic_assets

def _create_synthetic_asset(domain: str, 
                           task: str, 
                           knowledge_gap: str,
                           recipe: KnowledgeRecipe) -> KnowledgeAsset:
    """
    Create a synthetic knowledge asset targeting a specific gap
    """
    
    prompt = f"""
    Generate a realistic synthetic knowledge asset to address this specific knowledge gap.
    
    CONTEXT:
    Domain: {domain}
    Task: {task}
    Knowledge Gap: {knowledge_gap}
    Recipe Context: {recipe.transformation_steps[:2]}  # First 2 transformation steps
    
    REQUIREMENTS:
    1. Create knowledge content that directly addresses the gap "{knowledge_gap}"
    2. Make it realistic for the domain "{domain}" and task "{task}"
    3. Ensure it would be actionable for an agent performing the task
    4. Include specific, concrete information rather than generic descriptions
    5. Format as if it came from a real enterprise source
    
    EXAMPLE OUTPUTS:
    - For "Domain terminology" gap in "manufacturing" → specific technical terms, definitions, acronyms
    - For "Context awareness" gap in "customer support" → conversation patterns, escalation rules
    - For "Data retrieval techniques" gap → specific query patterns, API endpoints, data schemas
    
    Generate realistic content that an agent could actually use to improve performance.
    """
    
    content: str = reason(prompt)
    
    # Determine appropriate type and category based on gap
    asset_type = _map_gap_to_asset_type(knowledge_gap)
    category = _map_gap_to_category(knowledge_gap)
    
    return KnowledgeAsset(
        id=f"synthetic_{knowledge_gap.lower().replace(' ', '_')}",
        type=asset_type,
        source="Synthetic",
        content=content,
        trust_tier="Medium",
        metadata={
            "generated_for_gap": knowledge_gap,
            "domain": domain,
            "task": task,
            "generation_method": "llm_reasoning"
        },
        relevance_score=0.8,
        knowledge_category=category
    )

def _create_synthetic_integration_asset(domain: str,
                                      task: str,
                                      knowledge_gaps: list[str],
                                      recipe: KnowledgeRecipe) -> KnowledgeAsset:
    """
    Create a synthetic asset that tests integration across multiple knowledge gaps
    """
    
    prompt = f"""
    Generate a synthetic knowledge asset that integrates multiple knowledge types.
    
    CONTEXT:
    Domain: {domain}
    Task: {task}
    Knowledge Gaps to Integrate: {knowledge_gaps[:3]}
    Suggested Storage: {recipe.storage_formats[:2]}
    
    REQUIREMENTS:
    1. Create content that demonstrates how multiple knowledge types work together
    2. Show realistic integration patterns for this domain and task
    3. Include examples of cross-knowledge relationships
    4. Format as a workflow or process that uses multiple knowledge types
    
    Generate integration knowledge that tests how well different knowledge types complement each other.
    """
    
    content: str = reason(prompt)
    
    return KnowledgeAsset(
        id="synthetic_integration_test",
        type="Workflow",
        source="Synthetic",
        content=content,
        trust_tier="Medium",
        metadata={
            "integrates_gaps": str(knowledge_gaps[:3]),
            "domain": domain,
            "task": task,
            "generation_method": "integration_testing"
        },
        relevance_score=0.75,
        knowledge_category="CK"
    )

def _map_gap_to_asset_type(knowledge_gap: str) -> str:
    """Map knowledge gap to appropriate asset type"""
    gap_lower = knowledge_gap.lower()
    
    if "terminology" in gap_lower or "definition" in gap_lower:
        return "Domain_Ontology"
    elif "context" in gap_lower or "awareness" in gap_lower:
        return "PromptTemplate"
    elif "retrieval" in gap_lower or "technique" in gap_lower:
        return "Workflow"
    elif "formulation" in gap_lower or "strategy" in gap_lower:
        return "PromptTemplate"
    elif "framework" in gap_lower or "problem" in gap_lower:
        return "Workflow"
    else:
        return "Manual"

def _map_gap_to_category(knowledge_gap: str) -> str:
    """Map knowledge gap to DK/CK/AK category"""
    gap_lower = knowledge_gap.lower()
    
    if "terminology" in gap_lower or "data" in gap_lower:
        return "DK"
    elif "context" in gap_lower or "formulation" in gap_lower or "framework" in gap_lower:
        return "CK"  
    elif "technique" in gap_lower or "strategy" in gap_lower:
        return "AK"
    else:
        return "DK"

# --- Validation Functions ---

# _validate_first_pass_predictions is now replaced by _validate_first_pass_predictions

# _emit_refined_recipe is now replaced by _apply_validation_results

def _analyze_two_pass_results(first_pass_recipe: KnowledgeRecipe,
                             second_pass_recipe: KnowledgeRecipe,
                             synthetic_assets: dict[str, KnowledgeAsset],
                             validation_results: dict[str, float]) -> TwoPassResult:
    """
    Analyze both passes and recommend the best approach
    """
    
    confidence_improvement = second_pass_recipe.confidence_level - first_pass_recipe.confidence_level
    
    prompt = f"""
    Analyze two-pass curation results and recommend the best approach.
    
    FIRST PASS:
    - Estimated Improvement: {first_pass_recipe.estimated_improvement}
    - Confidence Level: {first_pass_recipe.confidence_level}
    - Knowledge Gaps: {len(first_pass_recipe.knowledge_gaps)}
    
    SECOND PASS:
    - Estimated Improvement: {second_pass_recipe.estimated_improvement}
    - Confidence Level: {second_pass_recipe.confidence_level}
    - Knowledge Gaps: {len(second_pass_recipe.knowledge_gaps)}
    
    VALIDATION METRICS:
    - Prediction Accuracy: {validation_results["prediction_accuracy"]:.2f}
    - Gap Resolution Rate: {validation_results["gap_resolution_rate"]:.2f}
    - Confidence Improvement: {confidence_improvement:.2f}
    
    SYNTHETIC ASSETS GENERATED: {len(synthetic_assets)}
    
    DECISION CRITERIA:
    1. If prediction accuracy > 0.7 and confidence improvement > 0.2: recommend second pass
    2. If first pass confidence > 0.8: first pass may be sufficient
    3. If validation shows major gaps in predictions: definitely use second pass
    4. Consider computational cost vs. accuracy improvement
    
    Provide recommendation: "first pass" or "second pass" with detailed reasoning.
    """
    
    recommendation: str = reason(prompt)
    
    return TwoPassResult(
        first_pass_recipe=first_pass_recipe,
        second_pass_recipe=second_pass_recipe,
        synthetic_knowledge=synthetic_assets,
        validation_summary=f"Prediction accuracy: {validation_results['prediction_accuracy']:.2f}, Gap resolution: {validation_results['gap_resolution_rate']:.2f}",
        confidence_improvement=confidence_improvement,
        recommended_approach=recommendation
    )

# --- Enhanced Helper Functions ---

def _emit_recipe_with_confidence(eval_summary: EvaluationSummary, 
                                kb_assets: dict[str, KnowledgeAsset], 
                                knowledge_state: dict[str, any], 
                                coverage_analysis: dict) -> KnowledgeRecipe:
    """
    Generate recipe with confidence assessment (enhanced version of original _emit_recipe)
    """
    
    # Calculate confidence based on knowledge state and coverage
    base_confidence = _calculate_confidence_level(knowledge_state, coverage_analysis, eval_summary)
    
    asset_types = []
    for key in kb_assets:
        asset_types.append(kb_assets[key].type)
    
    score = eval_summary.score
    
    prompt = f"""
    Generate targeted knowledge curation recommendations with confidence assessment.

    COMPREHENSIVE CONTEXT:
    Knowledge State Analysis:
    - State: {knowledge_state["state"]}
    - Coverage Ratio: {knowledge_state["coverage_ratio"]}
    - Knowledge Distribution: DK={knowledge_state["dk_count"]}, CK={knowledge_state["ck_count"]}, AK={knowledge_state["ak_count"]}
    - Critical Gap Estimate: {knowledge_state["critical_gap_estimate"]}
    
    Performance Analysis:
    - Current Score: {eval_summary.score}
    - Knowledge Coverage Score: {eval_summary.knowledge_coverage_score}
    - Critical Gaps: {eval_summary.critical_gaps}
    - Improvement Potential: {eval_summary.improvement_potential}
    
    Coverage Analysis Results: {coverage_analysis}
    Calculated Base Confidence: {base_confidence}

    CONFIDENCE ASSESSMENT GUIDELINES:
    - High confidence (0.8+): Rich knowledge available, clear performance patterns
    - Medium confidence (0.4-0.8): Some knowledge available, reasonable predictions
    - Low confidence (0.0-0.4): Limited knowledge, mostly theoretical recommendations
    
    Include confidence_level in your response based on the calculated base confidence and the quality of available information.
    Set validation_results to empty dict for first pass.
    """
    
    recipe: KnowledgeRecipe = reason(prompt)
    
    # Ensure confidence level is set
    recipe.confidence_level = base_confidence
    recipe.validation_results = {}
    
    return recipe

def _calculate_confidence_level(knowledge_state: dict[str, any], 
                               coverage_analysis: dict, 
                               eval_summary: EvaluationSummary) -> float:
    """
    Calculate confidence level based on available information
    """
    
    confidence_factors = []
    
    # Factor 1: Knowledge state quality
    state = knowledge_state["state"]
    if state == "adequate":
        confidence_factors.append(0.9)
    elif state == "partial_coverage":
        confidence_factors.append(0.6)
    elif state == "low_quality":
        confidence_factors.append(0.4)
    elif state == "critical_deficit":
        confidence_factors.append(0.2)
    else:  # empty
        confidence_factors.append(0.1)
    
    # Factor 2: Coverage ratio
    coverage_ratio = knowledge_state["coverage_ratio"]
    confidence_factors.append(min(coverage_ratio * 0.8, 0.8))
    
    # Factor 3: High relevance asset ratio
    total_assets = knowledge_state["dk_count"] + knowledge_state["ck_count"] + knowledge_state["ak_count"]
    if total_assets > 0:
        high_relevance_ratio = knowledge_state["high_relevance_count"] / total_assets
        confidence_factors.append(high_relevance_ratio * 0.7)
    else:
        confidence_factors.append(0.0)
    
    # Factor 4: Performance score (if we have actual performance data)
    if eval_summary.score > 0:
        confidence_factors.append(eval_summary.score * 0.5)
    
    # Average the factors
    if confidence_factors:
        return sum(confidence_factors) / len(confidence_factors)
    else:
        return 0.1

# --- Knowledge State Analysis ---

def _analyze_knowledge_state(task_sig: TaskSignature, kb_assets: dict[str, KnowledgeAsset]) -> dict[str, any]:
    """
    Symbolic analysis of knowledge state to provide structured context for prompts
    """
    analysis = {}
    
    # Knowledge type distribution
    dk_count = 0
    ck_count = 0
    ak_count = 0
    for key in kb_assets:
        category = kb_assets[key].knowledge_category
        if category == "DK":
            dk_count = dk_count + 1
        elif category == "CK":
            ck_count = ck_count + 1
        elif category == "AK":
            ak_count = ak_count + 1
    
    # Coverage analysis
    available_count = len(kb_assets)
    coverage_ratio = available_count / max(len(task_sig.knowledge_needs), 1)
    
    # Asset quality analysis
    high_relevance_count = 0
    for key in kb_assets:
        if kb_assets[key].relevance_score > 0.7:
            high_relevance_count = high_relevance_count + 1
    
    # Knowledge state classification
    if available_count == 0:
        state = "empty"
    elif coverage_ratio < 0.3:
        state = "critical_deficit"
    elif coverage_ratio < 0.7:
        state = "partial_coverage"
    elif high_relevance_count < available_count / 2:
        state = "low_quality"
    else:
        state = "adequate"
    
    analysis["state"] = state
    analysis["coverage_ratio"] = coverage_ratio
    analysis["dk_count"] = dk_count
    analysis["ck_count"] = ck_count
    analysis["ak_count"] = ak_count
    analysis["high_relevance_count"] = high_relevance_count
    analysis["critical_gap_estimate"] = max(0, len(task_sig.knowledge_needs) - available_count)
    
    return analysis

# --- Step Implementations ---

def _extract_task_signature(domain: str,
                            task: str,
                            improvements: str) -> TaskSignature:
    """
    Parse the domain-task pair for entities, knowledge needs, and success criteria

    Args:
        domain: The domain of the task
        task: The task to extract the signature for
        improvements: The improvements to the previous task signature

    Returns:
        A task signature object containing the entities, knowledge needs,
        and success criteria
    """
    
    # Get domain-specific patterns to improve task signature extraction
    domain_patterns = _get_domain_patterns(domain)
    
    prompt = f"""
    Analyze this domain-task pair using systematic reasoning with domain-specific patterns.
    
    DOMAIN: {domain}
    TASK: {task}
    IMPROVEMENTS FROM PREVIOUS ITERATION: {improvements}
    
    DOMAIN-SPECIFIC CONTEXT:
    Common patterns for "{domain}":
    {domain_patterns}
    
    REASONING STEPS:
    1. Identify the core domain concepts and their relationships
    2. Map task requirements to specific knowledge dependencies
    3. Classify entities by their role in the domain-task interaction
    4. Estimate complexity based on knowledge requirements
    5. Determine minimum viable knowledge assets
    
    DELIBERATE REASONING:
    - Start with domain fundamentals: What are the key systems/processes in {domain}?
    - Leverage domain patterns: How do common patterns apply to this specific task?
    - Map to task specifics: How does {task} interact with these systems?
    - Identify knowledge dependencies: What specific information is required for success?
    - Classify complexity: Based on dependencies, is this simple/moderate/complex?
    - Quantify requirements: How many distinct knowledge types are minimally needed?
    
    Examples to guide reasoning:
    - Manufacturing Equipment + Predictive Maintenance → entities: ["CNC_Machine", "Vibration_Sensor"], knowledge_needs: ["equipment_specifications", "maintenance_history"], complexity_level: "moderate", estimated_minimum_assets: 3
    - Financial Services + Risk Assessment → entities: ["Credit_History", "Market_Data"], knowledge_needs: ["risk_metrics", "compliance_rules"], complexity_level: "complex", estimated_minimum_assets: 5
    
    Apply domain patterns and examples to extract the most accurate task signature for this specific domain-task combination.
    """
    result: TaskSignature = reason(prompt)
    return result

def _get_domain_patterns(domain: str) -> str:
    """
    Get domain-specific knowledge patterns to enhance task signature extraction
    """
    prompt = f"""
    Provide common knowledge patterns, entities, and requirements for the domain: {domain}
    
    Include:
    - Typical entities and systems
    - Common knowledge requirements
    - Typical complexity levels for tasks
    - Standard success criteria patterns
    - Minimum asset requirements by task type
    
    Format as structured patterns that can guide task signature extraction.
    """
    patterns: str = reason(prompt)
    return patterns

def _simulate_dk_curation(domain: str,
                          task: str,
                          task_sig: TaskSignature,
                          documentary_knowledge: list[KnowledgeAsset],
                          improvements: str) -> KnowledgeAsset:
    """
    Simulate organizing enterprise DK into structured formats

    Args:
        domain: The domain of the task
        task: The task to curate knowledge for
        task_sig: The task signature
        documentary_knowledge: The documentary knowledge to use
        improvements: The improvements to the previous task signature

    Returns:
        A KnowledgeAsset object containing the structured enterprise DK
    """

    prompt = f"""
    Simulate the result of processing and organizing documentary knowledge.

    Domain: {domain}
    Task: {task}
    Task Signature: {task_sig}
    Documentary Knowledge: {documentary_knowledge}
    Improvements: {improvements}

    Simulate what the processed documentary knowledge would look like after:
    - Document processing and content extraction
    - Pattern identification and relationship mapping
    - Content categorization and hierarchy creation
    - Structured data extraction
    - Indexing and tagging

    Provide the simulated structured knowledge asset that would result from this processing.
    """
    result: KnowledgeAsset = reason(prompt)
    return result

def _simulate_ck_curation(domain: str,
                          task: str,
                          task_sig: TaskSignature,
                          dk_struct: KnowledgeAsset,
                          improvements: str) -> KnowledgeAsset:
    """
    Simulate generating contextual knowledge (patterns, workflows, templates)

    Args:
        domain: The domain of the task
        task: The task to generate contextual knowledge for
        task_sig: The task signature
        dk_struct: The documentary knowledge to use
        improvements: The improvements to the previous task signature

    Returns:
        A KnowledgeAsset object containing the contextual knowledge
    """

    prompt = f"""
    Simulate the result of generating contextual knowledge from documentary knowledge.

    Domain: {domain}
    Task: {task}
    Task Signature: {task_sig}
    Documentary Knowledge: {dk_struct}
    Improvements: {improvements}

    Simulate what the contextual knowledge would look like after:
    - Workflow pattern and template creation
    - Scenario identification and solution mapping
    - Decision tree and rule-based logic generation
    - Contextual prompt and example development
    - Task-specific heuristic creation

    Provide the simulated contextual knowledge asset that would result from this generation.
    """
    result: KnowledgeAsset = reason(prompt)
    return result

def _simulate_ak_domain_knowledge(domain: str, task: str, task_sig: TaskSignature, kb_assets: dict[str, KnowledgeAsset], improvements: str) -> KnowledgeAsset:
    """
    Simulate retrieval of Aitomatic domain knowledge (e.g., ontologies, patterns).
    """
    prompt = f"""
    Simulate the result of retrieving domain-level knowledge for this domain.
    Domain: {domain}
    Task: {task}
    Task Signature: {task_sig}
    Existing KB Assets: {kb_assets}
    Improvements: {improvements}
    Simulate a KnowledgeAsset of type 'Domain Ontology' and source 'Aitomatic' with relevant metadata.
    """
    result: KnowledgeAsset = reason(prompt)
    return result

def _simulate_ak_task_knowledge(domain: str, task: str, task_sig: TaskSignature, kb_assets: dict[str, KnowledgeAsset], improvements: str) -> KnowledgeAsset:
    """
    Simulate retrieval of Aitomatic task knowledge (e.g., workflows, protocols).
    """
    prompt = f"""
    Simulate the result of retrieving task-level knowledge for this task.
    Domain: {domain}
    Task: {task}
    Task Signature: {task_sig}
    Existing KB Assets: {kb_assets}
    Improvements: {improvements}
    Simulate a KnowledgeAsset of type 'Workflow' and source 'Aitomatic' with relevant metadata.
    """
    result: KnowledgeAsset = reason(prompt)
    return result

def _simulate_agent(domain: str, task: str, kb_assets: dict[str, KnowledgeAsset], knowledge_state: dict[str, any]) -> AgentRunResult:
    """
    Simulate agent run for this KB variant using a map of knowledge assets.
    """
    asset_count = len(kb_assets)
    asset_types = []
    for key in kb_assets:
        asset_types.append(kb_assets[key].type)
    
    state = knowledge_state["state"]
    coverage_ratio = knowledge_state["coverage_ratio"]
    
    prompt = f"""
    Simulate agent performance using systematic reasoning with symbolic state context.
    
    REASONING FRAMEWORK:
    STEP 1: Analyze Knowledge State
    - State Classification: {state}
    - Coverage Ratio: {coverage_ratio}
    - Knowledge Distribution: DK={knowledge_state["dk_count"]}, CK={knowledge_state["ck_count"]}, AK={knowledge_state["ak_count"]}
    
    STEP 2: Map State to Performance Expectations
    - For {state}: What is a realistic performance baseline?
    - Given coverage ratio {coverage_ratio}: What specific limitations emerge?
    - Given DK/CK/AK distribution: Where are the critical bottlenecks?
    
    STEP 3: Reason Through Execution
    - How would an agent approach {task} with this knowledge state?
    - What fusion patterns emerge between knowledge categories?
    - Which assets provide highest utility vs which create gaps?
    
    STEP 4: Quantify Outcomes
    - Calculate realistic performance score based on state classification
    - Determine utilization patterns for each knowledge category
    - Identify specific bottleneck types (knowledge, integration, access)
    
    DOMAIN: {domain}
    TASK: {task}
    
    Provide step-by-step reasoning that connects symbolic state to concrete performance outcomes.
    """
    result: AgentRunResult = reason(prompt)
    return result

def _evaluate_agent_run(domain: str,
                        task: str,
                        agent_run: AgentRunResult,
                        knowledge_state: dict[str, any]) -> EvaluationSummary:
    """
    Evaluate the agent run

    Args:
        domain: The domain of the task
        task: The task to evaluate the agent run for
        agent_run: The agent run to evaluate

    Returns:
        A EvaluationSummary object containing the evaluation summary
    """

    prompt = f"""
    Evaluate agent performance using systematic reasoning and symbolic analysis.

    REASONING STRUCTURE:
    STEP 1: Analyze Performance Context
    - Domain: {domain}
    - Task: {task}
    - Knowledge State: {knowledge_state["state"]}
    
    STEP 2: Decompose Performance Factors
    - Current Score: {agent_run.score}
    - Knowledge Utilization: {agent_run.knowledge_utilization}
    - Identified Bottlenecks: {agent_run.bottleneck_analysis}
    - Knowledge Coverage: {knowledge_state["coverage_ratio"]}
    - Distribution: DK={knowledge_state["dk_count"]}, CK={knowledge_state["ck_count"]}, AK={knowledge_state["ak_count"]}
    
    STEP 3: Reason Through Root Causes
    - How does knowledge state {knowledge_state["state"]} explain the score {agent_run.score}?
    - Which specific knowledge gaps (DK/CK/AK) contribute to bottlenecks?
    - What is the relationship between utilization patterns and performance?
    
    STEP 4: Calculate Improvement Potential
    - Based on knowledge state {knowledge_state["state"]}, what is the realistic maximum score?
    - Which specific knowledge acquisitions would yield the highest improvement?
    - How does coverage ratio {knowledge_state["coverage_ratio"]} translate to improvement potential?
    
    Provide detailed reasoning that connects each performance dimension to specific knowledge gaps and state classification.
    """
    result: EvaluationSummary = reason(prompt)
    return result

def _validate_knowledge_coverage(task_sig: TaskSignature, kb_assets: dict[str, KnowledgeAsset], knowledge_state: dict[str, any]) -> dict:
    """
    Validate knowledge coverage against task requirements
    
    Args:
        task_sig: The task signature with required knowledge needs
        kb_assets: Available knowledge assets
        knowledge_state: Pre-computed knowledge state analysis
    
    Returns:
        Dictionary with coverage score and gap analysis
    """
    required_needs = task_sig.knowledge_needs
    available_types = []
    for key in kb_assets:
        available_types.append(kb_assets[key].type)
    
    prompt = f"""
    Perform systematic gap analysis using step-by-step reasoning with pre-computed state context.
    
    PRE-COMPUTED CONTEXT:
    - Knowledge State: {knowledge_state["state"]}
    - Coverage Ratio: {knowledge_state["coverage_ratio"]}
    - Knowledge Distribution: DK={knowledge_state["dk_count"]}, CK={knowledge_state["ck_count"]}, AK={knowledge_state["ak_count"]}
    - High Relevance Assets: {knowledge_state["high_relevance_count"]}
    - Critical Gap Estimate: {knowledge_state["critical_gap_estimate"]}
    
    REASONING FRAMEWORK:
    STEP 1: Map Requirements
    - Required Knowledge Types: {required_needs}
    - Available Asset Types: {available_types}
    - Total Assets: {len(kb_assets)}
    
    STEP 2: Analyze Coverage Patterns (Enhanced with State Context)
    - Given state "{knowledge_state["state"]}", what are the specific coverage implications?
    - How does the DK/CK/AK distribution affect requirement fulfillment?
    - Which high-relevance assets ({knowledge_state["high_relevance_count"]}) address critical needs?
    - Identify knowledge category gaps (DK/CK/AK) in context of current distribution
    
    STEP 3: Calculate Coverage Score (State-Informed)
    - Refine coverage percentage using pre-computed ratio {knowledge_state["coverage_ratio"]}
    - Weight coverage by asset relevance and trust tier
    - Account for knowledge state classification in scoring
    - Identify specific missing knowledge categories
    
    STEP 4: Prioritize Gaps (State-Driven)
    - Given {knowledge_state["critical_gap_estimate"]} critical gaps, rank by severity
    - Consider knowledge state transition requirements (e.g., "critical_deficit" → "partial_coverage")
    - Determine minimum viable knowledge threshold for state improvement
    - Provide acquisition priority based on gap severity and state optimization
    
    Provide detailed reasoning that leverages pre-computed state analysis for more accurate gap assessment.
    """
    result: dict = reason(prompt)
    return result

