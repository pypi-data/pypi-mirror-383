"""
GPU Gateway for PaaS/SaaS Platform
æœ¬åœ°GPUèµ„æºçš„ç»Ÿä¸€ç½‘å…³å’Œç®¡ç†æœåŠ¡
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import aiohttp
from aiohttp import web
import time
import json
from pathlib import Path

from .provider import LocalGPUProvider
from .config import LocalGPUConfig, LocalServiceType, LocalBackend
from .health_checker import LocalHealthChecker
from ..core.base_deployment import DeploymentResult


logger = logging.getLogger(__name__)


class GPUPoolStatus(Enum):
    """GPUèµ„æºæ± çŠ¶æ€"""
    AVAILABLE = "available"
    BUSY = "busy" 
    MAINTENANCE = "maintenance"
    ERROR = "error"


@dataclass
class GPUNode:
    """GPUèŠ‚ç‚¹ä¿¡æ¯"""
    node_id: str
    hostname: str
    gpu_count: int
    gpu_memory_total: int  # MB
    gpu_memory_free: int   # MB
    status: GPUPoolStatus = GPUPoolStatus.AVAILABLE
    current_models: List[str] = field(default_factory=list)
    max_concurrent_requests: int = 10
    current_requests: int = 0
    last_heartbeat: float = field(default_factory=time.time)


@dataclass 
class TenantConfig:
    """ç§Ÿæˆ·é…ç½®"""
    tenant_id: str
    gpu_quota: int  # åˆ†é…çš„GPUæ•°é‡
    memory_quota: int  # åˆ†é…çš„GPUå†…å­˜MB
    priority: int = 1  # ä¼˜å…ˆçº§ 1-10
    allowed_models: List[str] = field(default_factory=list)
    rate_limit: int = 100  # æ¯åˆ†é’Ÿè¯·æ±‚æ•°


class GPUGateway:
    """
    GPUç½‘å…³æœåŠ¡ - PaaSå¹³å°çš„æœ¬åœ°GPUèµ„æºç®¡ç†
    
    åŠŸèƒ½:
    1. GPUèµ„æºæ± ç®¡ç†
    2. å¤šç§Ÿæˆ·èµ„æºéš”ç¦»
    3. è´Ÿè½½å‡è¡¡å’Œè·¯ç”±
    4. æœåŠ¡å‘ç°å’Œå¥åº·æ£€æŸ¥
    5. ä¸äº‘ç«¯APIçš„é€šä¿¡
    """
    
    def __init__(self, 
                 gateway_port: int = 8888,
                 cloud_api_url: Optional[str] = None,
                 workspace_dir: str = "./gpu_gateway"):
        self.gateway_port = gateway_port
        self.cloud_api_url = cloud_api_url
        self.workspace_dir = Path(workspace_dir)
        self.workspace_dir.mkdir(exist_ok=True)
        
        # GPUèµ„æºç®¡ç†
        self.gpu_nodes: Dict[str, GPUNode] = {}
        self.gpu_providers: Dict[str, LocalGPUProvider] = {}
        self.health_checkers: Dict[str, LocalHealthChecker] = {}
        
        # ç§Ÿæˆ·ç®¡ç†
        self.tenants: Dict[str, TenantConfig] = {}
        
        # è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
        self.request_queue = asyncio.Queue()
        self.model_routing: Dict[str, List[str]] = {}  # model_id -> [node_ids]
        
        # ç›‘æ§æ•°æ®
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_latency": 0.0,
            "gpu_utilization": {}
        }
        
        # å¯åŠ¨æ ‡å¿—
        self._running = False
        self._background_tasks = []
    
    async def start(self):
        """å¯åŠ¨GPUç½‘å…³æœåŠ¡"""
        logger.info("ğŸš€ Starting GPU Gateway...")
        
        # å‘ç°æœ¬åœ°GPUèŠ‚ç‚¹
        await self._discover_gpu_nodes()
        
        # å¯åŠ¨èƒŒæ™¯ä»»åŠ¡
        self._running = True
        self._background_tasks = [
            asyncio.create_task(self._heartbeat_monitor()),
            asyncio.create_task(self._resource_balancer()),
            asyncio.create_task(self._metrics_collector()),
            asyncio.create_task(self._cloud_sync()) if self.cloud_api_url else None
        ]
        self._background_tasks = [t for t in self._background_tasks if t]
        
        # å¯åŠ¨HTTPæœåŠ¡
        app = self._create_app()
        runner = web.AppRunner(app)
        await runner.setup()
        site = web.TCPSite(runner, '0.0.0.0', self.gateway_port)
        await site.start()
        
        logger.info(f"âœ… GPU Gateway started on port {self.gateway_port}")
        return runner
    
    async def stop(self):
        """åœæ­¢ç½‘å…³æœåŠ¡"""
        self._running = False
        
        # åœæ­¢èƒŒæ™¯ä»»åŠ¡
        for task in self._background_tasks:
            task.cancel()
        
        # åœæ­¢æ‰€æœ‰GPUæœåŠ¡
        for provider in self.gpu_providers.values():
            await provider.cleanup()
        
        logger.info("ğŸ›‘ GPU Gateway stopped")
    
    async def _discover_gpu_nodes(self):
        """å‘ç°æœ¬åœ°GPUèŠ‚ç‚¹"""
        logger.info("ğŸ” Discovering local GPU nodes...")
        
        try:
            # æ£€æµ‹æœ¬åœ°GPU
            from ...utils.gpu_utils import GPUManager
            gpu_manager = GPUManager()
            
            if gpu_manager.is_cuda_available():
                gpu_info = gpu_manager.get_gpu_info()
                
                for i, gpu in enumerate(gpu_info):
                    node_id = f"local-gpu-{i}"
                    node = GPUNode(
                        node_id=node_id,
                        hostname="localhost",
                        gpu_count=1,
                        gpu_memory_total=gpu.get('memory_total_mb', 0),
                        gpu_memory_free=gpu.get('memory_free_mb', 0)
                    )
                    
                    self.gpu_nodes[node_id] = node
                    
                    # åˆ›å»ºGPU Provider
                    provider = LocalGPUProvider(
                        workspace_dir=self.workspace_dir / node_id,
                        gpu_id=i
                    )
                    self.gpu_providers[node_id] = provider
                    
                    # åˆ›å»ºå¥åº·æ£€æŸ¥å™¨
                    health_checker = LocalHealthChecker()
                    self.health_checkers[node_id] = health_checker
                    
                    logger.info(f"âœ… Discovered GPU node: {node_id}")
            
            else:
                logger.warning("âš ï¸ No CUDA GPUs detected")
                
        except Exception as e:
            logger.error(f"âŒ GPU discovery failed: {e}")
    
    def register_tenant(self, tenant_config: TenantConfig):
        """æ³¨å†Œç§Ÿæˆ·"""
        self.tenants[tenant_config.tenant_id] = tenant_config
        logger.info(f"ğŸ‘¤ Registered tenant: {tenant_config.tenant_id}")
    
    async def deploy_model(self, 
                          tenant_id: str,
                          model_id: str, 
                          config: LocalGPUConfig,
                          preferred_node: Optional[str] = None) -> DeploymentResult:
        """ä¸ºç§Ÿæˆ·éƒ¨ç½²æ¨¡å‹"""
        
        # éªŒè¯ç§Ÿæˆ·æƒé™
        if tenant_id not in self.tenants:
            return DeploymentResult(
                success=False,
                error=f"Unknown tenant: {tenant_id}"
            )
        
        tenant = self.tenants[tenant_id]
        
        # æ£€æŸ¥æ¨¡å‹æƒé™
        if tenant.allowed_models and model_id not in tenant.allowed_models:
            return DeploymentResult(
                success=False,
                error=f"Model {model_id} not allowed for tenant {tenant_id}"
            )
        
        # é€‰æ‹©GPUèŠ‚ç‚¹
        node_id = preferred_node or await self._select_best_node(config, tenant)
        if not node_id:
            return DeploymentResult(
                success=False,
                error="No available GPU nodes"
            )
        
        # éƒ¨ç½²æ¨¡å‹
        try:
            provider = self.gpu_providers[node_id]
            result = await provider.deploy_model(config)
            
            if result.success:
                # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
                node = self.gpu_nodes[node_id]
                node.current_models.append(model_id)
                
                # æ›´æ–°è·¯ç”±è¡¨
                if model_id not in self.model_routing:
                    self.model_routing[model_id] = []
                self.model_routing[model_id].append(node_id)
                
                logger.info(f"âœ… Deployed {model_id} for tenant {tenant_id} on {node_id}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Deployment failed: {e}")
            return DeploymentResult(
                success=False,
                error=str(e)
            )
    
    async def inference_request(self,
                               tenant_id: str,
                               model_id: str,
                               request_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†æ¨ç†è¯·æ±‚"""
        
        # éªŒè¯ç§Ÿæˆ·
        if tenant_id not in self.tenants:
            return {"error": f"Unknown tenant: {tenant_id}"}
        
        # æ£€æŸ¥rate limiting
        tenant = self.tenants[tenant_id]
        # TODO: å®ç°rate limitingé€»è¾‘
        
        # é€‰æ‹©æœåŠ¡èŠ‚ç‚¹
        node_id = await self._route_request(model_id, tenant)
        if not node_id:
            return {"error": "No available nodes for model"}
        
        # æ‰§è¡Œæ¨ç†
        try:
            start_time = time.time()
            provider = self.gpu_providers[node_id]
            
            # æ ¹æ®è¯·æ±‚ç±»å‹è°ƒç”¨ä¸åŒæ–¹æ³•
            if "messages" in request_data:
                # Chat completion
                result = await provider.chat_completion(
                    model_id=model_id,
                    **request_data
                )
            else:
                # Text completion
                result = await provider.text_completion(
                    model_id=model_id,
                    **request_data
                )
            
            # æ›´æ–°æŒ‡æ ‡
            latency = time.time() - start_time
            await self._update_metrics(tenant_id, latency, True)
            
            # æ›´æ–°èŠ‚ç‚¹è´Ÿè½½
            node = self.gpu_nodes[node_id]
            node.current_requests = max(0, node.current_requests - 1)
            
            return result
            
        except Exception as e:
            await self._update_metrics(tenant_id, 0, False)
            logger.error(f"âŒ Inference failed: {e}")
            return {"error": str(e)}
    
    async def _select_best_node(self, 
                               config: LocalGPUConfig,
                               tenant: TenantConfig) -> Optional[str]:
        """é€‰æ‹©æœ€ä½³GPUèŠ‚ç‚¹"""
        available_nodes = []
        
        for node_id, node in self.gpu_nodes.items():
            if (node.status == GPUPoolStatus.AVAILABLE and 
                node.gpu_memory_free > 2000):  # è‡³å°‘2GBç©ºé—²
                
                # è®¡ç®—èŠ‚ç‚¹åˆ†æ•° (å†…å­˜è¶Šå¤šï¼Œå½“å‰è´Ÿè½½è¶Šå°‘ï¼Œåˆ†æ•°è¶Šé«˜)
                score = (
                    node.gpu_memory_free * 0.6 +  # å†…å­˜æƒé‡
                    (node.max_concurrent_requests - node.current_requests) * 0.3 +  # è´Ÿè½½æƒé‡
                    tenant.priority * 0.1  # ç§Ÿæˆ·ä¼˜å…ˆçº§æƒé‡
                )
                
                available_nodes.append((node_id, score))
        
        if available_nodes:
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„èŠ‚ç‚¹
            available_nodes.sort(key=lambda x: x[1], reverse=True)
            return available_nodes[0][0]
        
        return None
    
    async def _route_request(self, model_id: str, tenant: TenantConfig) -> Optional[str]:
        """è·¯ç”±æ¨ç†è¯·æ±‚åˆ°åˆé€‚çš„èŠ‚ç‚¹"""
        if model_id not in self.model_routing:
            return None
        
        # ä»éƒ¨ç½²äº†è¯¥æ¨¡å‹çš„èŠ‚ç‚¹ä¸­é€‰æ‹©è´Ÿè½½æœ€ä½çš„
        candidate_nodes = self.model_routing[model_id]
        best_node = None
        min_load = float('inf')
        
        for node_id in candidate_nodes:
            node = self.gpu_nodes[node_id]
            if (node.status == GPUPoolStatus.AVAILABLE and 
                node.current_requests < node.max_concurrent_requests):
                
                # è€ƒè™‘ç§Ÿæˆ·ä¼˜å…ˆçº§çš„è´Ÿè½½è®¡ç®—
                load = node.current_requests / node.max_concurrent_requests
                adjusted_load = load / max(tenant.priority, 1)
                
                if adjusted_load < min_load:
                    min_load = adjusted_load
                    best_node = node_id
        
        if best_node:
            # å¢åŠ èŠ‚ç‚¹è´Ÿè½½è®¡æ•°
            self.gpu_nodes[best_node].current_requests += 1
        
        return best_node
    
    async def _heartbeat_monitor(self):
        """ç›‘æ§GPUèŠ‚ç‚¹å¿ƒè·³"""
        while self._running:
            try:
                current_time = time.time()
                
                for node_id, node in self.gpu_nodes.items():
                    # æ£€æŸ¥å¿ƒè·³è¶…æ—¶
                    if current_time - node.last_heartbeat > 30:  # 30ç§’è¶…æ—¶
                        logger.warning(f"âš ï¸ Node {node_id} heartbeat timeout")
                        node.status = GPUPoolStatus.ERROR
                    
                    # æ›´æ–°GPUçŠ¶æ€
                    if node_id in self.health_checkers:
                        health = await self.health_checkers[node_id].check_service_health("gpu-status")
                        if health.get("success"):
                            node.status = GPUPoolStatus.AVAILABLE
                            node.gpu_memory_free = health.get("gpu_memory_free", 0)
                            node.last_heartbeat = current_time
                
                await asyncio.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ Heartbeat monitor error: {e}")
                await asyncio.sleep(5)
    
    async def _resource_balancer(self):
        """èµ„æºå‡è¡¡å™¨"""
        while self._running:
            try:
                # æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
                for node_id, node in self.gpu_nodes.items():
                    utilization = node.current_requests / node.max_concurrent_requests
                    
                    # å¦‚æœèŠ‚ç‚¹è¿‡è½½ï¼Œè€ƒè™‘è¿ç§»éƒ¨åˆ†æœåŠ¡
                    if utilization > 0.9:
                        logger.info(f"ğŸ”„ Node {node_id} is overloaded ({utilization:.1%})")
                        # TODO: å®ç°è´Ÿè½½è¿ç§»é€»è¾‘
                
                await asyncio.sleep(30)  # æ¯30ç§’å¹³è¡¡ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ Resource balancer error: {e}")
                await asyncio.sleep(10)
    
    async def _metrics_collector(self):
        """æŒ‡æ ‡æ”¶é›†å™¨"""
        while self._running:
            try:
                # æ”¶é›†GPUåˆ©ç”¨ç‡
                for node_id, node in self.gpu_nodes.items():
                    utilization = node.current_requests / node.max_concurrent_requests
                    self.metrics["gpu_utilization"][node_id] = utilization
                
                await asyncio.sleep(5)  # æ¯5ç§’æ”¶é›†ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ Metrics collector error: {e}")
                await asyncio.sleep(10)
    
    async def _cloud_sync(self):
        """ä¸äº‘ç«¯APIåŒæ­¥"""
        if not self.cloud_api_url:
            return
        
        while self._running:
            try:
                # å‘äº‘ç«¯æŠ¥å‘ŠGPUçŠ¶æ€
                status_data = {
                    "gateway_id": "local-gpu-gateway",
                    "nodes": [
                        {
                            "node_id": node.node_id,
                            "status": node.status.value,
                            "gpu_count": node.gpu_count,
                            "memory_free": node.gpu_memory_free,
                            "current_models": node.current_models,
                            "current_load": node.current_requests
                        }
                        for node in self.gpu_nodes.values()
                    ],
                    "metrics": self.metrics
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.cloud_api_url}/api/gpu-gateway/status",
                        json=status_data
                    ) as response:
                        if response.status == 200:
                            logger.debug("âœ… Status synced to cloud")
                        else:
                            logger.warning(f"âš ï¸ Cloud sync failed: {response.status}")
                
                await asyncio.sleep(60)  # æ¯åˆ†é’ŸåŒæ­¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ Cloud sync error: {e}")
                await asyncio.sleep(30)
    
    async def _update_metrics(self, tenant_id: str, latency: float, success: bool):
        """æ›´æ–°æŒ‡æ ‡"""
        self.metrics["total_requests"] += 1
        
        if success:
            self.metrics["successful_requests"] += 1
            # æ›´æ–°å¹³å‡å»¶è¿Ÿ
            current_avg = self.metrics["average_latency"]
            total_successful = self.metrics["successful_requests"]
            self.metrics["average_latency"] = (
                (current_avg * (total_successful - 1) + latency) / total_successful
            )
        else:
            self.metrics["failed_requests"] += 1
    
    def _create_app(self) -> web.Application:
        """åˆ›å»ºHTTP APIåº”ç”¨"""
        app = web.Application()
        
        # APIè·¯ç”±
        app.router.add_post('/deploy', self._handle_deploy)
        app.router.add_post('/inference', self._handle_inference)
        app.router.add_get('/status', self._handle_status)
        app.router.add_get('/metrics', self._handle_metrics)
        app.router.add_post('/tenants', self._handle_register_tenant)
        
        return app
    
    async def _handle_deploy(self, request: web.Request) -> web.Response:
        """å¤„ç†éƒ¨ç½²è¯·æ±‚"""
        try:
            data = await request.json()
            
            config = LocalGPUConfig(
                service_name=data["service_name"],
                service_type=LocalServiceType(data["service_type"]),
                model_id=data["model_id"],
                backend=LocalBackend(data.get("backend", "transformers"))
            )
            
            result = await self.deploy_model(
                tenant_id=data["tenant_id"],
                model_id=data["model_id"],
                config=config,
                preferred_node=data.get("preferred_node")
            )
            
            return web.json_response({
                "success": result.success,
                "error": result.error,
                "service_name": result.service_name,
                "service_info": result.service_info
            })
            
        except Exception as e:
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=400)
    
    async def _handle_inference(self, request: web.Request) -> web.Response:
        """å¤„ç†æ¨ç†è¯·æ±‚"""
        try:
            data = await request.json()
            
            result = await self.inference_request(
                tenant_id=data["tenant_id"],
                model_id=data["model_id"],
                request_data=data["request"]
            )
            
            return web.json_response(result)
            
        except Exception as e:
            return web.json_response({
                "error": str(e)
            }, status=400)
    
    async def _handle_status(self, request: web.Request) -> web.Response:
        """è·å–ç½‘å…³çŠ¶æ€"""
        status = {
            "gateway_status": "running" if self._running else "stopped",
            "total_nodes": len(self.gpu_nodes),
            "healthy_nodes": sum(
                1 for node in self.gpu_nodes.values() 
                if node.status == GPUPoolStatus.AVAILABLE
            ),
            "total_tenants": len(self.tenants),
            "deployed_models": len(self.model_routing),
            "nodes": [
                {
                    "node_id": node.node_id,
                    "status": node.status.value,
                    "gpu_memory_free": node.gpu_memory_free,
                    "current_requests": node.current_requests,
                    "models": node.current_models
                }
                for node in self.gpu_nodes.values()
            ]
        }
        
        return web.json_response(status)
    
    async def _handle_metrics(self, request: web.Request) -> web.Response:
        """è·å–æŒ‡æ ‡æ•°æ®"""
        return web.json_response(self.metrics)
    
    async def _handle_register_tenant(self, request: web.Request) -> web.Response:
        """æ³¨å†Œç§Ÿæˆ·"""
        try:
            data = await request.json()
            
            tenant_config = TenantConfig(
                tenant_id=data["tenant_id"],
                gpu_quota=data.get("gpu_quota", 1),
                memory_quota=data.get("memory_quota", 8192),
                priority=data.get("priority", 1),
                allowed_models=data.get("allowed_models", []),
                rate_limit=data.get("rate_limit", 100)
            )
            
            self.register_tenant(tenant_config)
            
            return web.json_response({
                "success": True,
                "tenant_id": tenant_config.tenant_id
            })
            
        except Exception as e:
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=400)


# ä¾¿æ·å‡½æ•°
async def create_gpu_gateway(gateway_port: int = 8888,
                           cloud_api_url: Optional[str] = None,
                           workspace_dir: str = "./gpu_gateway") -> GPUGateway:
    """åˆ›å»ºå¹¶å¯åŠ¨GPUç½‘å…³"""
    gateway = GPUGateway(
        gateway_port=gateway_port,
        cloud_api_url=cloud_api_url,
        workspace_dir=workspace_dir
    )
    
    await gateway.start()
    return gateway