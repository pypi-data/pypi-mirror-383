"""
Streaming build orchestration for memory-optimized builds.

Uses knowledge graph analysis to process pages in optimal order for memory efficiency.
Hub-first strategy: Keep highly connected pages in memory, stream leaves.
"""

from __future__ import annotations

import gc
from typing import TYPE_CHECKING, Any

from bengal.utils.logger import get_logger

if TYPE_CHECKING:
    from bengal.cache import DependencyTracker
    from bengal.core.page import Page
    from bengal.core.site import Site
    from bengal.orchestration.render import RenderOrchestrator
    from bengal.utils.build_stats import BuildStats

logger = get_logger(__name__)


class StreamingRenderOrchestrator:
    """
    Memory-optimized page rendering using knowledge graph analysis.

    Strategy:
    1. Build knowledge graph to identify connectivity
    2. Process hubs first (keep in memory - they're needed often)
    3. Stream leaves in batches (release immediately after rendering)
    4. Result: 80-90% memory reduction for large sites

    Best for: Sites with 5K+ pages
    """

    def __init__(self, site: Site):
        """
        Initialize streaming render orchestrator.

        Args:
            site: Site instance containing pages
        """
        self.site = site

    def process(
        self,
        pages: list[Page],
        parallel: bool = True,
        quiet: bool = False,
        tracker: DependencyTracker | None = None,
        stats: BuildStats | None = None,
        batch_size: int = 100,
        progress_manager: Any | None = None,
    ) -> None:
        """
        Render pages in memory-optimized batches using connectivity analysis.

        Args:
            pages: List of pages to render
            parallel: Whether to use parallel rendering
            quiet: Whether to suppress progress output (minimal output mode)
            tracker: Dependency tracker for incremental builds
            stats: Build statistics tracker
            batch_size: Number of leaves to process per batch
            progress_manager: Optional progress manager to use for unified progress display
        """
        total_pages = len(pages)

        # Warn if using memory optimization on small sites (overhead > benefit)
        RECOMMENDED_THRESHOLD = 5000
        WARNING_THRESHOLD = 1000

        if total_pages < WARNING_THRESHOLD:
            print("  âš ï¸  Memory optimization is designed for large sites (5K+ pages)")
            print(f"     Your site has {total_pages} pages - standard build is likely faster.")
            print("     Continuing anyway for testing/profiling purposes...")
        elif total_pages < RECOMMENDED_THRESHOLD:
            print(
                f"  â„¹ï¸  Site has {total_pages} pages - memory optimization may have marginal benefit."
            )

        logger.info(
            "streaming_render_start",
            total_pages=total_pages,
            batch_size=batch_size,
            recommended=total_pages >= RECOMMENDED_THRESHOLD,
        )

        # Import here to avoid circular dependency
        try:
            from bengal.analysis.knowledge_graph import KnowledgeGraph
        except ImportError:
            logger.warning(
                "streaming_render_fallback",
                reason="Knowledge graph not available, using standard rendering",
            )
            # Fall back to standard rendering
            from bengal.orchestration.render import RenderOrchestrator

            orchestrator = RenderOrchestrator(self.site)
            orchestrator.process(
                pages, parallel, quiet, tracker, stats, progress_manager=progress_manager
            )
            return

        # Build knowledge graph to analyze connectivity
        print("  ðŸ§  Analyzing connectivity for memory optimization...")
        graph = KnowledgeGraph(self.site)
        graph.build()

        # Get connectivity-based layers
        hubs, mid_tier, leaves = graph.get_layers()

        # Filter to only pages we're rendering
        pages_set = set(pages)
        hubs_to_render = [p for p in hubs if p in pages_set]
        mid_to_render = [p for p in mid_tier if p in pages_set]
        leaves_to_render = [p for p in leaves if p in pages_set]

        total_hubs = len(hubs_to_render)
        total_mid = len(mid_to_render)
        total_leaves = len(leaves_to_render)

        logger.info(
            "streaming_render_layers", hubs=total_hubs, mid_tier=total_mid, leaves=total_leaves
        )

        print(f"     Hubs: {total_hubs} (keep in memory)")
        print(f"     Mid-tier: {total_mid} (batch process)")
        print(f"     Leaves: {total_leaves} (stream & release)")

        # Import standard renderer for actual processing
        from bengal.orchestration.render import RenderOrchestrator

        renderer = RenderOrchestrator(self.site)

        # Phase 1: Render hubs (keep in memory - they're referenced often)
        if hubs_to_render:
            print(f"\n  ðŸ“ Rendering {total_hubs} hub page(s)...")
            renderer.process(
                hubs_to_render, parallel, quiet, tracker, stats, progress_manager=progress_manager
            )
            logger.debug("streaming_render_hubs_complete", count=total_hubs)

        # Phase 2: Render mid-tier in batches
        if mid_to_render:
            print(f"  ðŸ”— Rendering {total_mid} mid-tier page(s)...")
            self._render_batches(
                renderer,
                mid_to_render,
                batch_size,
                parallel,
                quiet,
                tracker,
                stats,
                "mid-tier",
                progress_manager=progress_manager,
            )
            logger.debug("streaming_render_mid_complete", count=total_mid)

        # Phase 3: Stream leaves in batches (release after each batch)
        if leaves_to_render:
            print(f"  ðŸƒ Streaming {total_leaves} leaf page(s)...")
            self._render_batches(
                renderer,
                leaves_to_render,
                batch_size,
                parallel,
                quiet,
                tracker,
                stats,
                "leaves",
                release_memory=True,
                progress_manager=progress_manager,
            )
            logger.debug("streaming_render_leaves_complete", count=total_leaves)

        logger.info(
            "streaming_render_complete",
            total_rendered=len(pages),
            hubs=total_hubs,
            mid=total_mid,
            leaves=total_leaves,
        )

        print("  âœ“ Memory-optimized render complete!")

    def _render_batches(
        self,
        renderer: RenderOrchestrator,
        pages: list[Page],
        batch_size: int,
        parallel: bool,
        quiet: bool,
        tracker: DependencyTracker | None,
        stats: BuildStats | None,
        batch_label: str = "pages",
        release_memory: bool = False,
        progress_manager: Any | None = None,
    ) -> None:
        """
        Render pages in batches with optional memory release.

        Args:
            renderer: RenderOrchestrator instance
            pages: Pages to render
            batch_size: Pages per batch
            parallel: Use parallel rendering
            quiet: Whether to suppress progress output
            tracker: Dependency tracker
            stats: Build statistics
            batch_label: Label for logging
            release_memory: Whether to force garbage collection after each batch
            progress_manager: Optional progress manager to use for unified progress display
        """
        total = len(pages)
        batches = (total + batch_size - 1) // batch_size  # Ceiling division

        for i in range(0, total, batch_size):
            batch = pages[i : i + batch_size]
            batch_num = (i // batch_size) + 1

            # Render this batch
            renderer.process(
                batch, parallel, quiet, tracker, stats, progress_manager=progress_manager
            )

            logger.debug(
                "streaming_render_batch_complete",
                batch_type=batch_label,
                batch_num=batch_num,
                batch_size=len(batch),
                total_batches=batches,
            )

            # Release memory if requested (for leaves)
            if release_memory:
                # Clear references to page content/metadata to free memory
                for page in batch:
                    # Keep essential metadata but release heavy content
                    if hasattr(page, "_content_cache"):
                        delattr(page, "_content_cache")
                    if hasattr(page, "_html_cache"):
                        delattr(page, "_html_cache")

                # Force garbage collection
                gc.collect()

                logger.debug(
                    "streaming_render_memory_released",
                    batch_num=batch_num,
                    pages_released=len(batch),
                )
