# Example environment variables for Project Vectorizer
# Copy this file to .env and fill in values as needed.

# =============================================================================
# EMBEDDING CONFIGURATION
# =============================================================================

# OpenAI API (optional; only needed if using OpenAI embeddings)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Embedding provider: "sentence-transformers" or "openai"
EMBEDDING_PROVIDER=sentence-transformers

# Embedding model to use
# For sentence-transformers: "all-MiniLM-L6-v2", "all-mpnet-base-v2", etc.
# For OpenAI: "text-embedding-ada-002", "text-embedding-3-small", etc.
EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# DATABASE CONFIGURATION (ChromaDB)
# =============================================================================

# ChromaDB storage path (for all data: vectors and metadata)
# Leave empty to use default: {project}/.vectorizer/chromadb
# CHROMADB_PATH=/custom/path/to/chromadb

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================

# Default chunk size in tokens (128-256 recommended for precise search)
CHUNK_SIZE=256

# Chunk overlap in tokens
CHUNK_OVERLAP=32

# Maximum file size to process (in MB)
MAX_FILE_SIZE_MB=10

# =============================================================================
# SEARCH CONFIGURATION
# =============================================================================

# Default search threshold for similarity matching
DEFAULT_SEARCH_THRESHOLD=0.3

# Default number of search results to return
DEFAULT_SEARCH_LIMIT=10

# Enable enhanced single-word search optimization
ENABLE_SINGLE_WORD_OPTIMIZATION=true

# =============================================================================
# MCP SERVER CONFIGURATION
# =============================================================================

# MCP server host and port
MCP_HOST=localhost
MCP_PORT=8000

# Enable HTTP fallback API
ENABLE_HTTP_FALLBACK=true

# CORS origins for HTTP API (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable verbose logging
VERBOSE_LOGGING=false

# Log file path (optional, logs to console if not set)
# LOG_FILE=./logs/vectorizer.log

# =============================================================================
# FILE PROCESSING CONFIGURATION
# =============================================================================

# File extensions to include (comma-separated)
INCLUDED_EXTENSIONS=.py,.js,.ts,.jsx,.tsx,.go,.rs,.java,.cpp,.c,.h,.hpp,.cs,.php,.rb,.swift,.kt,.scala,.clj,.sh,.bash,.md,.txt,.json,.yaml,.yml,.toml,.sql

# Patterns to exclude (comma-separated, supports glob patterns)
EXCLUDED_PATTERNS=node_modules/**,.git/**,__pycache__/**,*.pyc,.env,.env.*,dist/**,build/**,target/**,vendor/**,.vscode/**,.idea/**

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================

# Number of concurrent embedding generations
EMBEDDING_BATCH_SIZE=10

# Enable parallel processing for file indexing
ENABLE_PARALLEL_PROCESSING=true

# Maximum number of worker threads
MAX_WORKERS=4

# =============================================================================
# DEVELOPMENT CONFIGURATION
# =============================================================================

# Enable development mode with additional debugging
DEVELOPMENT_MODE=false

# Enable profiling for performance analysis
ENABLE_PROFILING=false

# Cache directory for embeddings and models
CACHE_DIR=./.vectorizer/cache
