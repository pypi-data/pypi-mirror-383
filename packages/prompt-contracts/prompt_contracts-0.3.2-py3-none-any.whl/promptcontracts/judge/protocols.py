"""
LLM-as-Judge protocols with bias control.

Provides standardized judge prompts, randomization, and cross-family
validation to mitigate bias in semantic evaluation.
"""

import random
from typing import Any, Optional

import numpy as np


def create_judge_prompt(
    task_description: str,
    output_to_judge: str,
    criteria: list[str],
    reference: Optional[str] = None,
    scale: str = "1-10",
) -> str:
    """
    Create standardized judge prompt.

    Args:
        task_description: Description of the task
        output_to_judge: LLM output to evaluate
        criteria: List of evaluation criteria
        reference: Optional reference/gold output
        scale: Rating scale (default "1-10")

    Returns:
        Judge prompt string

    Example:
        >>> prompt = create_judge_prompt(
        ...     "Summarize article",
        ...     "The article discusses AI safety...",
        ...     ["relevance", "coherence", "brevity"],
        ...     reference="Article summary: ..."
        ... )
    """
    criteria_text = "\n".join(f"- {c}" for c in criteria)

    prompt = f"""You are an expert evaluator. Assess the following output based on the criteria below.

**Task**: {task_description}

**Output to Evaluate**:
{output_to_judge}
"""

    if reference:
        prompt += f"""
**Reference Output**:
{reference}
"""

    prompt += f"""
**Evaluation Criteria**:
{criteria_text}

**Instructions**:
1. Evaluate the output on a scale of {scale} for each criterion
2. Provide a brief explanation for each score
3. Give an overall rating ({scale})
4. State your verdict: PASS or FAIL

**Format your response as**:
CRITERION SCORES:
[List each criterion with score and explanation]

OVERALL RATING: [score]

VERDICT: [PASS or FAIL]

EXPLANATION: [Brief justification for verdict]
"""

    return prompt


def randomize_judge_order(
    items: list[dict[str, Any]], seed: int = 42
) -> tuple[list[dict[str, Any]], list[int]]:
    """
    Randomize order of items for judge evaluation.

    Prevents order bias in LLM-as-judge evaluation.

    Args:
        items: List of items to judge
        seed: Random seed for reproducibility

    Returns:
        Tuple of (randomized_items, original_indices)

    Example:
        >>> items = [{"id": 1}, {"id": 2}, {"id": 3}]
        >>> shuffled, indices = randomize_judge_order(items, seed=42)
        >>> # Restore original order: [shuffled[i] for i in np.argsort(indices)]
    """
    random.seed(seed)
    indices = list(range(len(items)))
    random.shuffle(indices)

    randomized = [items[i] for i in indices]
    return randomized, indices


def mask_provider_metadata(output: str, provider_hints: Optional[list[str]] = None) -> str:
    """
    Remove provider-identifying metadata from output.

    Prevents judge from being biased by knowing which provider generated output.

    Args:
        output: LLM output string
        provider_hints: Optional list of provider-specific patterns to remove

    Returns:
        Masked output

    Example:
        >>> output = "Generated by GPT-4: The answer is..."
        >>> masked = mask_provider_metadata(output, ["Generated by GPT-4:", "by GPT"])
        >>> masked
        "The answer is..."
    """
    if provider_hints is None:
        # Default provider hints
        provider_hints = [
            "Generated by",
            "OpenAI",
            "GPT-4",
            "GPT-3",
            "Claude",
            "Anthropic",
            "LLaMA",
            "Mistral",
            "Gemini",
            "by AI",
        ]

    masked = output
    for hint in provider_hints:
        # Case-insensitive removal
        import re

        masked = re.sub(hint, "", masked, flags=re.IGNORECASE)

    # Clean up extra whitespace
    masked = " ".join(masked.split())

    return masked


def cohens_kappa(rater1_labels: list[int], rater2_labels: list[int]) -> float:
    """
    Calculate Cohen's κ for inter-rater agreement.

    Args:
        rater1_labels: Labels from rater 1
        rater2_labels: Labels from rater 2

    Returns:
        Cohen's κ coefficient

    Interpretation:
        < 0.00: Poor
        0.00-0.20: Slight
        0.21-0.40: Fair
        0.41-0.60: Moderate
        0.61-0.80: Substantial
        0.81-1.00: Almost perfect

    Example:
        >>> r1 = [1, 1, 0, 1, 0]
        >>> r2 = [1, 0, 0, 1, 0]
        >>> cohens_kappa(r1, r2)
        0.615
    """
    if len(rater1_labels) != len(rater2_labels):
        raise ValueError("Rater label lists must have same length")

    n = len(rater1_labels)
    if n == 0:
        return 0.0

    # Observed agreement
    agreements = sum(1 for r1, r2 in zip(rater1_labels, rater2_labels, strict=False) if r1 == r2)
    p_o = agreements / n

    # Expected agreement by chance
    # For binary labels (0, 1)
    count_r1_0 = sum(1 for r in rater1_labels if r == 0)
    count_r1_1 = sum(1 for r in rater1_labels if r == 1)
    count_r2_0 = sum(1 for r in rater2_labels if r == 0)
    count_r2_1 = sum(1 for r in rater2_labels if r == 1)

    p_e = (count_r1_0 / n) * (count_r2_0 / n) + (count_r1_1 / n) * (count_r2_1 / n)

    if p_e == 1.0:
        return 1.0  # Perfect agreement

    kappa = (p_o - p_e) / (1 - p_e)
    return kappa


def fleiss_kappa(ratings_matrix: list[list[int]]) -> float:
    """
    Calculate Fleiss' κ for multi-rater agreement.

    Args:
        ratings_matrix: Matrix where rows are items, columns are raters
                       Each cell contains category assignment (0, 1, 2, ...)

    Returns:
        Fleiss' κ coefficient

    Example:
        >>> # 3 raters, 5 items, binary categories
        >>> matrix = [
        ...     [1, 1, 1],  # Item 1: all agree on category 1
        ...     [0, 0, 1],  # Item 2: 2 agree on 0, 1 on 1
        ...     [1, 1, 1],
        ...     [0, 1, 0],
        ...     [1, 1, 1],
        ... ]
        >>> fleiss_kappa(matrix)
        0.611
    """
    ratings_matrix = np.array(ratings_matrix)
    n_items, n_raters = ratings_matrix.shape

    if n_items == 0 or n_raters < 2:
        return 0.0

    # Determine number of categories
    categories = np.unique(ratings_matrix)
    n_categories = len(categories)

    # Count assignments per category per item
    counts = np.zeros((n_items, n_categories))
    for i, item_ratings in enumerate(ratings_matrix):
        for rating in item_ratings:
            cat_idx = np.where(categories == rating)[0][0]
            counts[i, cat_idx] += 1

    # Calculate P_i (proportion of agreement for each item)
    P_i = (np.sum(counts**2, axis=1) - n_raters) / (n_raters * (n_raters - 1))
    P_bar = np.mean(P_i)  # Mean observed agreement

    # Calculate P_e (expected agreement by chance)
    p_j = np.sum(counts, axis=0) / (n_items * n_raters)  # Proportion of each category
    P_e = np.sum(p_j**2)

    if P_e == 1.0:
        return 1.0

    kappa = (P_bar - P_e) / (1 - P_e)
    return float(kappa)


def cross_family_judge_config(
    primary_model: str = "gpt-4o",
    secondary_model: str = "claude-3-sonnet",
    tertiary_model: Optional[str] = None,
) -> dict:
    """
    Create cross-family judge configuration.

    Uses judges from different model families to reduce family-specific biases.

    Args:
        primary_model: Primary judge model
        secondary_model: Secondary judge from different family
        tertiary_model: Optional third judge for tie-breaking

    Returns:
        Judge configuration dict

    Example:
        >>> config = cross_family_judge_config(
        ...     primary_model="gpt-4o",
        ...     secondary_model="claude-3-sonnet",
        ...     tertiary_model="gemini-pro"
        ... )
    """
    config = {
        "judges": [
            {"model": primary_model, "family": _model_family(primary_model), "weight": 1.0},
            {
                "model": secondary_model,
                "family": _model_family(secondary_model),
                "weight": 1.0,
            },
        ],
        "aggregation": "majority_vote",
        "randomize_order": True,
        "mask_provider_metadata": True,
        "reliability_check": {
            "method": "cohens_kappa",
            "threshold": 0.6,  # Substantial agreement required
        },
    }

    if tertiary_model:
        config["judges"].append(
            {
                "model": tertiary_model,
                "family": _model_family(tertiary_model),
                "weight": 0.5,  # Tie-breaker
            }
        )
        config["aggregation"] = "weighted_vote"

    return config


def _model_family(model_name: str) -> str:
    """Infer model family from model name."""
    model_lower = model_name.lower()
    if "gpt" in model_lower or "openai" in model_lower:
        return "openai"
    elif "claude" in model_lower or "anthropic" in model_lower:
        return "anthropic"
    elif "gemini" in model_lower or "bard" in model_lower:
        return "google"
    elif "llama" in model_lower:
        return "meta"
    elif "mistral" in model_lower:
        return "mistral"
    else:
        return "unknown"
