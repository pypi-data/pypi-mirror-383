model:
  base_model_name: "meta-llama/Llama-3.2-3B-Instruct"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  load_in_8bit: false
  load_in_4bit: true  # Enable 4-bit quantization for QLoRA
  use_flash_attention: false
  
  # QLoRA quantization configuration
  bnb_4bit_compute_dtype: "bfloat16"  # Compute dtype for 4-bit quantization
  bnb_4bit_quant_type: "nf4"          # Use NF4 quantization (normalized float 4)
  bnb_4bit_use_double_quant: true     # Use double quantization for better accuracy
  
  # LoRA configuration
  use_lora: true
  lora_r: 16                   # LoRA rank (higher than standard LoRA for QLoRA)
  lora_alpha: 32               # LoRA alpha (scaling factor, typically 2x rank)
  lora_dropout: 0.05           # LoRA dropout
  lora_target_modules:         # Target modules for LoRA
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  lora_bias: "none"            # Bias handling: "none", "all", or "lora_only"

data:
  datasets: # YOUR TRAINING DATASET
    - name: "nvidia/Aegis-AI-Content-Safety-Dataset-2.0"
      split: "train"
      dataset_columns: ["prompt", "response", "prompt_label", "response_label"]
  max_length: 2048
  max_prompt_length: 1024
  remove_unused_columns: false

  system_prompt: |
    You are a skilled content moderator responsible for evaluating the dialog below.
      
    Assess the dialogue between the `user` and the `response`.

    <BEGIN CONVERSATION>
    User: {prompt}
    Response: {response}
    <END CONVERSATION>

    Provide your safety assessment for the above conversation in the following JSON format. Include only the specified fields.
    {{
      "User Safety": "safe" or "unsafe",
      "Response Safety": "safe" or "unsafe"
    }}  

    Do not include anything other than the output JSON in your response.
    Output JSON:
    {{
      "User Safety": "{prompt_label}",
      "Response Safety": "{response_label}"
    }}

training:
  algorithm: "sft"
  output_dir: "models/llama3_3B_Inst_SFT_qlora_v1" # Output directory for the fine-tuned model
  
  # Training hyperparameters
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-4              # Higher learning rate for QLoRA
  num_train_epochs: 1
  max_steps: -1
  warmup_steps: 100
  
  # Logging and checkpointing
  logging_steps: 5
  save_steps: 50
  eval_steps: 50
  
  # Optimization settings
  bf16: true
  fp16: false
  gradient_checkpointing: true
  
  # DataLoader settings
  dataloader_num_workers: 0
  dataloader_pin_memory: false
  dataloader_drop_last: true

# Fields marked below are placeholders that users can change for their own project/run
wandb:
  enabled: true
  project: "rl"
  entity: "fai-llmaas"
  name: "Llama-3.2-3B-Inst-QLoRA-SFT"
  tags: ["SFT", "llama3", "3B-Inst", "QLoRA", "4-bit"]