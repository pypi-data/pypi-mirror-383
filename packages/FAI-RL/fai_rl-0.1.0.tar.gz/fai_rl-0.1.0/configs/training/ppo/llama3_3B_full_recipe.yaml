model:
  base_model_name: "meta-llama/Llama-3.2-3B-Instruct"
  value_model_name: "meta-llama/Llama-3.2-3B-Instruct"
  torch_dtype: "bfloat16"
  low_cpu_mem_usage: true
  load_in_8bit: false
  load_in_4bit: false
  use_flash_attention: false

data:
  datasets: # YOUR TRAINING DATASET
    - name: "trl-internal-testing/descriptiveness-sentiment-trl-style"
      split: "descriptiveness"
      prompt_column: "prompt"
      chosen_column: "chosen"
      rejected_column: "rejected"
  max_length: 1024          # ⬇️ Reduce from 2048 (biggest impact)
  max_prompt_length: 512    # ⬇️ Reduce from 1024
  remove_unused_columns: false
  dataset_num_proc: 1

training:
  algorithm: "ppo"
  output_dir: "models/llama3_3B_Inst_PPO_full_v1" # Output directory for the fine-tuned model
  
  # Training hyperparameters
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 32  # ⬆️ Increase to maintain effective batch size
  learning_rate: 1.0e-5
  num_train_epochs: 1
  max_steps: -1
  
  # PPO specific parameters
  gamma: 1.0                                # Discount factor
  lam: 0.95                                 # GAE lambda
  cliprange: 0.2                            # PPO clipping range
  cliprange_value: 0.2                      # Value function clipping range
  vf_coef: 0.1                              # Value function coefficient
  
  # Logging and checkpointing
  logging_steps: 5
  save_steps: 100
  
  # Optimization settings
  bf16: true
  gradient_checkpointing: true

# Fields marked below are placeholders that users can change for their own project/run
wandb:
  enabled: true
  project: "rl"
  entity: "fai-llmaas"
  name: "Llama-3.2-3B-Inst-Full-PPO"
  tags: ["PPO", "llama3", "3B-Inst", "GSM8K", "OpenMathInstruct-2"]
