import abc
from typing import Dict, List, Optional, Tuple

import mlflow
from neco.mlops.anomaly_detection.feature_engineer import TimeSeriesFeatureEngineer
import numpy as np
import pandas as pd
from loguru import logger
from hyperopt import fmin, tpe, Trials, STATUS_OK, hp, space_eval
from scipy.special import expit
from sklearn.metrics import (
    precision_recall_fscore_support,
    roc_auc_score,
    accuracy_score
)

from neco.mlops.utils.mlflow_utils import MLFlowUtils


class BaseAnomalyDetection(abc.ABC):
    """å¼‚å¸¸æ£€æµ‹åŸºç±»ï¼Œæä¾›é€šç”¨çš„è®­ç»ƒå’Œé¢„æµ‹åŠŸèƒ½"""

    def __init__(self):
        self.feature_engineer = None
        self.model_metadata = {}

    @abc.abstractmethod
    def build_model(self, train_params: dict):
        """æ„å»ºæ¨¡å‹å®ä¾‹"""
        pass

    def preprocess(self, df: pd.DataFrame, frequency: Optional[str] = None) -> Tuple[pd.DataFrame, List[str], Optional[str]]:
        """æ•°æ®é¢„å¤„ç†ï¼šæ—¶é—´æ ‡å‡†åŒ–ã€æ’åºã€ç¼ºå¤±å€¼å¡«å……"""
        if df is None:
            return None, [], frequency

        df = df.copy()

        # æ ‡å‡†åŒ–æ—¶é—´åˆ—å¹¶æ’åº
        if not np.issubdtype(df["timestamp"].dtype, np.datetime64):
            df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df = df.dropna(subset=["timestamp"]).sort_values("timestamp")

        # è®¾ç½®æ—¶é—´ç´¢å¼•ï¼Œæ¨æ–­é¢‘ç‡
        df = df.set_index("timestamp")
        if frequency is None:
            try:
                frequency = pd.infer_freq(df.index)
            except Exception as e:
                logger.warning(f"æ— æ³•æ¨æ–­æ—¶é—´é¢‘ç‡: {e}")
                frequency = None

        # å¤„ç†ç¼ºå¤±å€¼ï¼šæ—¶é—´æ’å€¼ -> å‰åå¡«å…… -> ä¸­ä½æ•°å…œåº•
        value_series = df["value"].astype(float)
        value_series = value_series.interpolate(method="time", limit_direction="both")
        value_series = value_series.ffill().bfill()

        if value_series.isna().any():
            median_value = value_series.median() if not np.isnan(value_series.median()) else 0.0
            value_series = value_series.fillna(median_value)

        df["value"] = value_series
        df = df.reset_index()

        return df, ["value"], frequency

    def predict(
        self,
        data: pd.DataFrame,
        model_name: str,
        model_version: str = "latest",
        threshold: float = 0.5
    ) -> pd.DataFrame:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå¼‚å¸¸æ£€æµ‹é¢„æµ‹"""
        # åŠ è½½æ¨¡å‹åŠå…ƒæ•°æ®
        model = self._load_model(model_name, model_version)
        
        # æ‰“å°åŠ è½½çš„ç‰¹å¾ä¿¡æ¯
        feature_cols = self.model_metadata.get("feature_cols")
        logger.info(f"ğŸ“‹ ä½¿ç”¨ {len(feature_cols)} ä¸ªç‰¹å¾è¿›è¡Œé¢„æµ‹")
        logger.debug(f"ç‰¹å¾åˆ—è¡¨: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}")
        
        # æ•°æ®é¢„å¤„ç†
        test_df, _, _ = self.preprocess(data, self.model_metadata.get("frequency"))

        # ç‰¹å¾æå–ï¼šç›´æ¥ä½¿ç”¨ä¿å­˜çš„ feature_cols,æ— éœ€ tsfresh_params
        feature_engineer = TimeSeriesFeatureEngineer(
            tsfresh_params=None,  # ä¸éœ€è¦,å› ä¸ºæˆ‘ä»¬ä½¿ç”¨ selected_features
            n_jobs=4
        )
        X_test, _, _ = feature_engineer.extract_features(
            test_df,
            selected_features=feature_cols,
            extract_labels=False
        )
        
        logger.info(f"âœ… æµ‹è¯•æ•°æ®ç‰¹å¾æå–å®Œæˆï¼Œå½¢çŠ¶: {X_test.shape}")

        # è¿›è¡Œé¢„æµ‹
        anomaly_scores = self._get_prediction_scores(model, X_test)
        anomaly_labels = self._apply_threshold(anomaly_scores, threshold)

        # æ„å»ºç»“æœ
        result_df = pd.DataFrame({
            'timestamp': test_df['timestamp'],
            'value': test_df['value'],
            'anomaly_probability': anomaly_scores,
            'anomaly_label': anomaly_labels
        })

        return result_df

    def train(
        self,
        model_name: str,
        train_dataframe: pd.DataFrame,
        val_dataframe: Optional[pd.DataFrame] = None,
        test_dataframe: Optional[pd.DataFrame] = None,
        train_config: dict = {},
        max_evals: int = 50,
        mlflow_tracking_url: Optional[str] = None,
        experiment_name: str = "Default",
        tsfresh_params: Optional[Dict] = None,
        n_jobs: int = 4,
        primary_metric: str = "f1",
        positive_label: int = 1,
        decision_threshold: float = 0.5
    ):
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹æ³¨å†Œåç§°
            train_dataframe: è®­ç»ƒæ•°æ®
            val_dataframe: éªŒè¯æ•°æ®ï¼ˆå¯é€‰ï¼‰
            test_dataframe: æµ‹è¯•æ•°æ®ï¼ˆå¯é€‰ï¼‰
            train_config: è¶…å‚æ•°æœç´¢ç©ºé—´é…ç½®
            max_evals: è¶…å‚æ•°ä¼˜åŒ–æœ€å¤§è¯„ä¼°æ¬¡æ•°
            mlflow_tracking_url: MLflow tracking æœåŠ¡åœ°å€
            experiment_name: å®éªŒåç§°
            tsfresh_params: tsfresh ç‰¹å¾æå–é…ç½®ï¼ˆNone è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰é»˜è®¤ç‰¹å¾ï¼‰
            n_jobs: å¹¶è¡Œä»»åŠ¡æ•°
            primary_metric: ä¼˜åŒ–çš„ä¸»è¦æŒ‡æ ‡ï¼ˆf1/auc/precision/recall/accuracyï¼‰
            positive_label: æ­£ç±»æ ‡ç­¾
            decision_threshold: å†³ç­–é˜ˆå€¼
        """
        MLFlowUtils.setup_experiment(mlflow_tracking_url, experiment_name)

        # åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨
        self.feature_engineer = TimeSeriesFeatureEngineer(
            tsfresh_params=tsfresh_params,
            n_jobs=n_jobs
        )

        # æ•°æ®é¢„å¤„ç†
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®é¢„å¤„ç†...")
        train_df_prep, val_df_prep, test_df_prep, frequency = self._preprocess_all_data(
            train_dataframe, val_dataframe, test_dataframe
        )

        # ç‰¹å¾å·¥ç¨‹
        logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        X_train, y_train, feature_cols = self.feature_engineer.extract_features(train_df_prep)
        
        # æ‰“å°ç‰¹å¾ä¿¡æ¯
        logger.info(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå…±æ‰¾åˆ° {len(feature_cols)} ä¸ªæœ‰æ•ˆç‰¹å¾")
        logger.info(f"ç‰¹å¾åç§°åˆ—è¡¨: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}")
        
        # å‡†å¤‡éªŒè¯é›†
        X_val, y_val = self._prepare_validation_set(
            val_df_prep, X_train, y_train, feature_cols
        )

        # è¶…å‚æ•°ä¼˜åŒ–
        logger.info(f"ğŸ” å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–ï¼Œæœ€å¤§è¯„ä¼°æ¬¡æ•°: {max_evals}")
        best_params, trials, train_scores, val_scores = self._optimize_hyperparameters(
            train_config, X_train, y_train, X_val, y_val,
            max_evals, primary_metric, positive_label, decision_threshold
        )

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        logger.info("ğŸš€ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        X_train_full, y_train_full = self._merge_train_val(
            X_train, y_train, X_val, y_val, val_df_prep
        )
        
        best_model = self._train_final_model(
            best_params, X_train_full, y_train_full,
            feature_cols, frequency, tsfresh_params
        )

        # è¯„ä¼°æ¨¡å‹
        val_metrics = self._evaluate_model(best_model, X_val, y_val, "éªŒè¯é›†")
        test_metrics = self._evaluate_test_set(
            best_model, test_df_prep, feature_cols,
            decision_threshold, positive_label
        )

        # è®°å½•åˆ°MLflow
        self._log_to_mlflow(
            best_params, max_evals, primary_metric, decision_threshold,
            len(feature_cols), len(X_train_full), train_scores, val_scores,
            val_metrics, test_metrics, best_model, model_name
        )

        logger.info(f"âœ… æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ")

        return {
            "best_params": best_params,
            "best_model": best_model,
            "feature_cols": feature_cols,
            "val_metrics": val_metrics,
            "test_metrics": test_metrics,
            "trials": trials
        }

    def _load_model(self, model_name: str, model_version: str):
        """åŠ è½½æ¨¡å‹åŠå…ƒæ•°æ®
        
        æ³¨æ„ï¼š
        - feature_cols æ˜¯å¿…éœ€çš„,ç”¨äºç‰¹å¾æå–
        - frequency å¯ä»¥ä¸º None(ä½¿ç”¨è‡ªåŠ¨æ¨æ–­)
        - tsfresh_params ä¸éœ€è¦åŠ è½½(æ¨ç†æ—¶åªéœ€è¦ feature_cols)
        """
        # ä½¿ç”¨ MLFlowUtils åŠ è½½æ¨¡å‹,åªåŠ è½½å¿…è¦çš„å…ƒæ•°æ®
        model, metadata = MLFlowUtils.load_model_with_metadata(
            model_name=model_name,
            model_version=model_version,
            metadata_attrs=["feature_cols", "frequency"]  # ç§»é™¤ tsfresh_params
        )
        
        self.model_metadata = metadata
        
        # åªæœ‰ feature_cols æ˜¯å¿…éœ€çš„
        if self.model_metadata.get("feature_cols") is None:
            raise ValueError("æ¨¡å‹ç¼ºå°‘å¿…éœ€çš„å…ƒæ•°æ®å±æ€§: feature_cols")
        
        # frequency ä¸º None æ˜¯åˆç†çš„
        if self.model_metadata.get("frequency") is None:
            logger.debug("frequency ä¸º None,å°†åœ¨é¢„æµ‹æ—¶è‡ªåŠ¨æ¨æ–­")
        
        return model

    def _prepare_validation_set(
        self,
        val_df_prep: Optional[pd.DataFrame],
        X_train: pd.DataFrame,
        y_train: pd.Series,
        feature_cols: List[str]
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """å‡†å¤‡éªŒè¯é›†"""
        if val_df_prep is None:
            logger.info("æœªæä¾›éªŒè¯é›†ï¼Œä»è®­ç»ƒé›†åˆ†å‰²20%")
            from sklearn.model_selection import train_test_split
            X_train_split, X_val, y_train_split, y_val = train_test_split(
                X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
            )
            return X_val, y_val
        else:
            logger.info("å¼€å§‹éªŒè¯é›†ç‰¹å¾å·¥ç¨‹...")
            X_val, y_val, _ = self.feature_engineer.extract_features(
                val_df_prep, selected_features=feature_cols
            )
            return X_val, y_val

    def _merge_train_val(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        val_df_prep: Optional[pd.DataFrame]
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆå¦‚æœéªŒè¯é›†æ˜¯ä»è®­ç»ƒé›†åˆ†å‰²çš„ï¼‰"""
        if val_df_prep is None:
            X_train_full = pd.concat([X_train, X_val], ignore_index=True)
            y_train_full = pd.concat([y_train, y_val], ignore_index=True)
        else:
            X_train_full, y_train_full = X_train, y_train
        return X_train_full, y_train_full

    def _evaluate_model(
        self,
        model,
        X: Optional[pd.DataFrame],
        y: Optional[pd.Series],
        dataset_name: str
    ) -> dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        metrics = {}
        if X is not None and y is not None:
            scores = self._get_prediction_scores(model, X)
            try:
                metrics["auc"] = float(roc_auc_score(y, scores))
                logger.info(f"{dataset_name} AUC: {metrics['auc']:.4f}")
            except Exception as e:
                logger.warning(f"è®¡ç®—{dataset_name}AUCå¤±è´¥: {e}")
        return metrics

    def _evaluate_test_set(
        self,
        model,
        test_df_prep: Optional[pd.DataFrame],
        feature_cols: List[str],
        threshold: float,
        positive_label: int
    ) -> dict:
        """è¯„ä¼°æµ‹è¯•é›†"""
        test_metrics = {}
        
        if (test_df_prep is not None and
            "label" in test_df_prep.columns and
            test_df_prep["label"].notna().any()):

            X_test, y_test, _ = self.feature_engineer.extract_features(
                test_df_prep, selected_features=feature_cols
            )

            test_scores = self._get_prediction_scores(model, X_test)

            try:
                test_metrics["auc"] = float(roc_auc_score(y_test, test_scores))
            except Exception as e:
                logger.warning(f"è®¡ç®—æµ‹è¯•é›†AUCå¤±è´¥: {e}")

            y_test_pred = self._apply_threshold(test_scores, threshold)
            P, R, F1, _ = precision_recall_fscore_support(
                y_test, y_test_pred, pos_label=positive_label,
                average="binary", zero_division=0
            )

            test_metrics.update({
                "precision": float(P),
                "recall": float(R),
                "f1": float(F1),
                "accuracy": float(accuracy_score(y_test, y_test_pred)),
                "threshold": float(threshold),
            })
            
            logger.info(f"æµ‹è¯•é›†æŒ‡æ ‡: F1={F1:.4f}, AUC={test_metrics.get('auc', 0):.4f}")

        return test_metrics

    def _optimize_hyperparameters(
        self,
        train_config: dict,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        max_evals: int,
        primary_metric: str,
        positive_label: int,
        decision_threshold: float
    ) -> Tuple[dict, Trials, List[float], List[float]]:
        """æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        space = self._build_search_space(train_config)
        if not space:
            logger.warning("æœªå®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            default_model = self.build_model({})
            return {}, Trials(), [], []

        logger.info(f"è¶…å‚æ•°æœç´¢ç©ºé—´: {list(space.keys())}")

        trials = Trials()
        train_scores_history = []
        val_scores_history = []
        
        # åœ¨é—­åŒ…å¤–éƒ¨ä¿å­˜å¯¹åŸºç±»å®ä¾‹çš„å¼•ç”¨
        base_instance = self

        def objective(params_raw):
            params = space_eval(space, params_raw)

            try:
                # ä½¿ç”¨ base_instance è€Œä¸æ˜¯ self
                model = base_instance.build_model(train_params=params)
                model.fit(X_train, y_train)

                # è¯„ä¼°æ€§èƒ½
                train_score = base_instance._evaluate_model_score(
                    model, X_train, y_train, primary_metric, positive_label, decision_threshold
                )
                val_score = base_instance._evaluate_model_score(
                    model, X_val, y_val, primary_metric, positive_label, decision_threshold
                )

                train_scores_history.append(train_score)
                val_scores_history.append(val_score)

                if len(train_scores_history) % 10 == 0:
                    logger.info(
                        f"ç¬¬ {len(train_scores_history)} æ¬¡è¯„ä¼° - è®­ç»ƒ{primary_metric}: {train_score:.4f}, éªŒè¯{primary_metric}: {val_score:.4f}")

                return {"loss": -float(val_score), "status": STATUS_OK}

            except Exception as e:
                logger.error(f"è¶…å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                logger.debug(traceback.format_exc())
                return {"loss": 1.0, "status": STATUS_OK}

        logger.info("å¼€å§‹è¶…å‚æ•°æœç´¢...")
        best_params_raw = fmin(
            fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals,
            trials=trials, rstate=np.random.default_rng(2025)
        )
        best_params = space_eval(space, best_params_raw)

        if train_scores_history and val_scores_history:
            best_idx = np.argmax(val_scores_history)
            logger.info(
                f"æœ€ä½³æ€§èƒ½ - è®­ç»ƒ{primary_metric}: {train_scores_history[best_idx]:.4f}, éªŒè¯{primary_metric}: {val_scores_history[best_idx]:.4f}")

        return best_params, trials, train_scores_history, val_scores_history

    def _evaluate_model_score(
        self,
        model,
        X: pd.DataFrame,
        y: pd.Series,
        metric: str,
        positive_label: int,
        threshold: float
    ) -> float:
        """è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„æ€§èƒ½"""
        scores = self._get_prediction_scores(model, X)
        return self._calculate_metric_score(y, scores, metric, threshold, positive_label)

    def _calculate_metric_score(
        self,
        y_true: pd.Series,
        scores: np.ndarray,
        metric: str,
        threshold: float,
        positive_label: int
    ) -> float:
        """è®¡ç®—æŒ‡å®šæŒ‡æ ‡çš„åˆ†æ•°"""
        metric = metric.lower()
        scores = np.asarray(scores)
        is_continuous = (np.issubdtype(scores.dtype, np.floating) and
                         np.unique(scores).size > 2)

        if metric == "auc" and is_continuous:
            return roc_auc_score(y_true, scores)

        # å…¶ä»–æŒ‡æ ‡éœ€è¦ç¦»æ•£é¢„æµ‹
        y_pred = ((scores >= threshold).astype(int) if is_continuous
                  else scores.astype(int))

        if metric == "auc":  # ç¦»æ•£æƒ…å†µä¸‹çš„AUCç”¨å‡†ç¡®ç‡ä»£æ›¿
            return accuracy_score(y_true, y_pred)

        P, R, F1, _ = precision_recall_fscore_support(
            y_true, y_pred, pos_label=positive_label, average="binary", zero_division=0
        )

        metric_map = {
            "f1": F1,
            "precision": P,
            "recall": R,
            "accuracy": accuracy_score(y_true, y_pred)
        }

        if metric not in metric_map:
            raise ValueError(f"ä¸æ”¯æŒçš„æŒ‡æ ‡: {metric}")

        return metric_map[metric]

    def _train_final_model(
        self,
        best_params: dict,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        feature_cols: List[str],
        frequency: Optional[str],
        tsfresh_params: Optional[Dict]
    ):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶ä¿å­˜å…ƒæ•°æ®"""
        best_model = self.build_model(train_params=best_params)
        best_model.fit(X_train, y_train)

        # ä¿å­˜å…ƒæ•°æ®åˆ°æ¨¡å‹å¯¹è±¡
        best_model.feature_cols = feature_cols
        best_model.frequency = frequency
        best_model.tsfresh_params = tsfresh_params
        
        logger.info(f"ğŸ’¾ æ¨¡å‹å…ƒæ•°æ®å·²ä¿å­˜: {len(feature_cols)} ä¸ªç‰¹å¾, é¢‘ç‡={frequency}")

        return best_model

    def _preprocess_all_data(
        self,
        train_df: pd.DataFrame,
        val_df: Optional[pd.DataFrame],
        test_df: Optional[pd.DataFrame]
    ) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[str]]:
        """é¢„å¤„ç†æ‰€æœ‰æ•°æ®é›†"""
        train_df_prep, _, frequency = self.preprocess(train_df, None)
        val_df_prep = self.preprocess(val_df, frequency)[0] if val_df is not None else None
        test_df_prep = self.preprocess(test_df, frequency)[0] if test_df is not None else None
        return train_df_prep, val_df_prep, test_df_prep, frequency

    def _build_search_space(self, train_config: dict) -> dict:
        """æ„å»ºè¶…å‚æ•°æœç´¢ç©ºé—´"""
        space = {}
        for key, cfg in train_config.items():
            param_type = str(cfg.get("type", "")).lower()

            if param_type == "randint":
                vmin, vmax = int(cfg["min"]), int(cfg["max"])
                if vmax < vmin:
                    raise ValueError(f"{key}: max({vmax}) < min({vmin})")
                space[key] = hp.randint(key, vmax - vmin + 1) + vmin

            elif param_type == "choice":
                opts = self._parse_choice_options(cfg["choice"])
                space[key] = hp.choice(key, opts)

        return space

    def _parse_choice_options(self, choices: List) -> List:
        """è§£æchoiceç±»å‹çš„é€‰é¡¹"""
        opts = []
        for c in choices:
            if isinstance(c, str):
                lc = c.strip().lower()
                if lc == "none":
                    opts.append(None)
                elif lc == "true":
                    opts.append(True)
                elif lc == "false":
                    opts.append(False)
                else:
                    opts.append(c)
            else:
                opts.append(c)
        return opts

    def _log_to_mlflow(
        self,
        best_params: dict,
        max_evals: int,
        primary_metric: str,
        decision_threshold: float,
        n_features: int,
        n_train_samples: int,
        train_scores: List[float],
        val_scores: List[float],
        val_metrics: dict,
        test_metrics: dict,
        best_model,
        model_name: str
    ):
        """è®°å½•è®­ç»ƒç»“æœåˆ°MLflow"""
        # åˆå¹¶æ‰€æœ‰å‚æ•°
        all_params = {
            **best_params,
            "max_evals": max_evals,
            "primary_metric": primary_metric,
            "decision_threshold": decision_threshold,
            "n_features": n_features,
            "train_samples": n_train_samples
        }
        
        # ä½¿ç”¨ MLFlowUtils çš„ä¸€ç«™å¼æ–¹æ³•è®°å½•è®­ç»ƒç»“æœ
        MLFlowUtils.log_training_results(
            params=all_params,
            train_scores=train_scores,
            val_scores=val_scores,
            metric_name=primary_metric,
            val_metrics=val_metrics,
            test_metrics=test_metrics,
            model=best_model,
            model_name=model_name
        )

    def _get_prediction_scores(self, model, X: pd.DataFrame) -> np.ndarray:
        """è·å–æ¨¡å‹é¢„æµ‹åˆ†æ•°"""
        if hasattr(model, "predict_proba"):
            proba = model.predict_proba(X)
            if isinstance(proba, np.ndarray) and proba.ndim == 2 and proba.shape[1] >= 2:
                return proba[:, 1]
            else:
                return model.predict(X).astype(float)
        elif hasattr(model, "decision_function"):
            decision_scores = model.decision_function(X)
            return expit(decision_scores)
        else:
            return model.predict(X).astype(float)

    def _apply_threshold(self, scores: np.ndarray, threshold: float) -> np.ndarray:
        """æ ¹æ®é˜ˆå€¼ç”Ÿæˆç¦»æ•£æ ‡ç­¾"""
        scores = np.asarray(scores)
        if np.issubdtype(scores.dtype, np.floating) and (np.unique(scores).size > 2):
            return (scores >= threshold).astype(int)
        else:
            return scores.astype(int)
            return scores.astype(int)
