from typing import TypedDict, Annotated
import math
from collections import deque
from typing import Optional

from langchain_core.messages import BaseMessage, HumanMessage
from loguru import logger
from neco.llm.chain.entity import BasicLLMRequest, BasicLLMResponse, ToolsServer
from neco.llm.chain.graph import BasicGraph
from neco.llm.chain.node import ToolsNodes
from pydantic import BaseModel
from langgraph.graph import add_messages
from pydantic import Field
from typing import List
from langgraph.graph import StateGraph
from langgraph.constants import END
from langchain_core.messages import AIMessage, BaseMessage
from langchain_core.output_parsers import JsonOutputToolsParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt import ToolNode
from collections import defaultdict
from typing import List, Tuple
from neco.core.utils.template_loader import TemplateLoader

class LatsAgentResponse(BasicLLMResponse):
    pass


class LatsAgentRequest(BasicLLMRequest):
    tools_servers: List[ToolsServer] = []
    langchain_tools: List[str] = []

class Reflection(BaseModel):
    reflections: str = Field(
        description="å¯¹å›ç­”çš„å……åˆ†æ€§ã€å¤šä½™æ€§å’Œæ•´ä½“è´¨é‡çš„è¯„ä»·å’Œåæ€"
    )

    score: int = Field(
        description="å¯¹å€™é€‰å›ç­”è´¨é‡çš„è¯„åˆ†ï¼ŒèŒƒå›´ä»0åˆ°10ã€‚",
        gte=0,
        lte=10,
    )
    found_solution: bool = Field(
        description="å›ç­”æ˜¯å¦å·²å®Œå…¨è§£å†³äº†é—®é¢˜æˆ–ä»»åŠ¡ã€‚"
    )

    def as_message(self):
        return HumanMessage(
            content=f"Reasoning: {self.reflections}\nScore: {self.score}"
        )

    @property
    def normalized_score(self) -> float:
        return self.score / 10.0


class Node:
    def __init__(
            self,
            messages: list[BaseMessage],
            reflection: Reflection,
            parent: Optional["Node"] = None,
    ):
        self.messages = messages
        self.parent = parent
        self.children = []
        self.value = 0
        self.visits = 0
        self.reflection = reflection
        self.depth = parent.depth + 1 if parent is not None else 1
        self._is_solved = reflection.found_solution if reflection else False
        if self._is_solved:
            self._mark_tree_as_solved()
        self.backpropagate(reflection.normalized_score)

    def __repr__(self) -> str:
        return (
            f"<Node value={self.value}, visits={self.visits},"
            f" solution={self.messages} reflection={self.reflection}/>"
        )

    @property
    def is_solved(self):
        """If any solutions exist, we can end the search."""
        return self._is_solved

    @property
    def is_terminal(self):
        return not self.children

    @property
    def best_child_score(self):
        """Return the child with the highest value."""
        if not self.children:
            return None
        return max(self.children, key=lambda child: int(child.is_solved) * child.value)

    @property
    def height(self) -> int:
        """Check for how far we've rolled out the tree."""
        if self.children:
            return 1 + max([child.height for child in self.children])
        return 1

    def upper_confidence_bound(self, exploration_weight=1.0):
        """Return the UCT score. This helps balance exploration vs. exploitation of a branch."""
        if self.parent is None:
            raise ValueError("Cannot obtain UCT from root node")
        if self.visits == 0:
            return self.value
        # Encourages exploitation of high-value trajectories
        average_reward = self.value / self.visits
        # Encourages exploration of less-visited trajectories
        exploration_term = math.sqrt(
            math.log(self.parent.visits) / self.visits)
        return average_reward + exploration_weight * exploration_term

    def backpropagate(self, reward: float):
        """Update the score of this node and its parents."""
        node = self
        while node:
            node.visits += 1
            node.value = (node.value * (node.visits - 1) +
                          reward) / node.visits
            node = node.parent

    def get_messages(self, include_reflections: bool = True):
        if include_reflections:
            return self.messages + [self.reflection.as_message()]
        return self.messages

    def get_trajectory(self, include_reflections: bool = True) -> list[BaseMessage]:
        """Get messages representing this search branch."""
        messages = []
        node = self
        while node:
            messages.extend(
                node.get_messages(
                    include_reflections=include_reflections)[::-1]
            )
            node = node.parent
        # Reverse the final back-tracked trajectory to return in the correct order
        return messages[::-1]  # root solution, reflection, child 1, ...

    def _get_all_children(self):
        all_nodes = []
        nodes = deque()
        nodes.append(self)
        while nodes:
            node = nodes.popleft()
            all_nodes.extend(node.children)
            for n in node.children:
                nodes.append(n)
        return all_nodes

    def get_best_solution(self):
        """Return the best solution from within the current sub-tree."""
        all_nodes = [self] + self._get_all_children()
        best_node = max(
            all_nodes,
            # We filter out all non-terminal, non-solution trajectories
            key=lambda node: int(
                node.is_terminal and node.is_solved) * node.value,
        )
        return best_node

    def _mark_tree_as_solved(self):
        parent = self.parent
        while parent:
            parent._is_solved = True
            parent = parent.parent


class LatsAgentState(TypedDict):
    messages: Annotated[list, add_messages]
    graph_request: LatsAgentRequest
    root: Node
    evaluation_results: Optional[list]  # ç”¨äºä¼ é€’è¯„ä»·è¡¨ä¿¡æ¯
    initial_evaluation: Optional[dict]  # ç”¨äºä¼ é€’åˆå§‹è¯„ä¼°ä¿¡æ¯


class LatsAgentNode(ToolsNodes):
    """LATS Agent èŠ‚ç‚¹å¤„ç†å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""

    # æ ¸å¿ƒé…ç½®
    MAX_CANDIDATES = 5
    MAX_TREE_HEIGHT = 5
    EXPLORATION_WEIGHT = 1.0

    def get_reflection_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–åæ€è¯„ä¼°é“¾"""
        async def reflection_chain_async(inputs):
            llm = self.get_llm_client(
                config["configurable"]["graph_request"], disable_stream=True)

            system_message = TemplateLoader.render_template(
                "prompts/lats_agent/reflection_evaluation")
            prompt = ChatPromptTemplate.from_messages([
                ("system", system_message),
                ("user", "{input}"),
                MessagesPlaceholder(variable_name="candidate"),
            ])

            result = await self.call_with_structured_output(
                llm=llm, prompt=prompt, pydantic_model=Reflection, messages=inputs
            )
            return result

        return reflection_chain_async

    def get_expansion_chain(self, state: LatsAgentState, config: RunnableConfig):
        """è·å–å€™é€‰ç”Ÿæˆé“¾"""
        def generate_candidates(messages) -> List[BaseMessage]:
            llm = self.get_llm_client(
                config["configurable"]["graph_request"], disable_stream=True)
            bound_kwargs = llm.bind_tools(tools=self.tools).kwargs

            candidates = []
            logger.debug(f"ç”Ÿæˆ {self.MAX_CANDIDATES} ä¸ªå€™é€‰è§£å†³æ–¹æ¡ˆ")

            for i in range(self.MAX_CANDIDATES):
                chat_result = llm.generate(
                    [messages.to_messages()],
                    callbacks=[],
                    run_name=f"GenerateCandidate_{i + 1}",
                    **bound_kwargs,
                )
                candidate = chat_result.generations[0][0].message
                candidates.append(candidate)

                # ç»Ÿè®¡ token ä½¿ç”¨
                if hasattr(candidate, 'usage_metadata'):
                    self.tools_prompt_tokens += candidate.usage_metadata.get(
                        'input_tokens', 0)
                    self.tools_completions_tokens += candidate.usage_metadata.get(
                        'output_tokens', 0)

            return candidates

        system_message = TemplateLoader.render_template(
            "prompts/lats_agent/candidate_generation")
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ])

        return prompt_template | generate_candidates

    def select(self, root: Node) -> Node:
        """ä½¿ç”¨ UCB ç®—æ³•é€‰æ‹©æœ€ä½³èŠ‚ç‚¹"""
        if not root.children:
            return root

        node = root
        while node.children:
            max_child = max(
                node.children,
                key=lambda child: child.upper_confidence_bound(
                    self.EXPLORATION_WEIGHT)
            )
            node = max_child

        logger.debug(f"é€‰æ‹©æ·±åº¦ä¸º {node.depth} çš„èŠ‚ç‚¹")
        return node

    async def _process_candidates(
        self,
        candidates: List[BaseMessage],
        state: LatsAgentState,
        config: RunnableConfig
    ) -> Tuple[List[List[BaseMessage]], List[Reflection]]:
        """å¤„ç†å€™é€‰æ–¹æ¡ˆï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨å’Œè¯„ä¼°"""
        # è§£æå·¥å…·è°ƒç”¨
        parser = JsonOutputToolsParser(return_id=True)
        parsed_tool_calls = parser.batch(candidates)

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        tool_node = ToolNode(self.tools, handle_tool_errors=True)
        collected_responses = defaultdict(list)

        for candidate_idx, tool_calls in enumerate(parsed_tool_calls):
            for tool_call in tool_calls:
                try:
                    response = tool_node.invoke({
                        "messages": [AIMessage(
                            content="",
                            tool_calls=[{
                                "name": tool_call["type"],
                                "args": tool_call["args"],
                                "id": tool_call["id"],
                            }]
                        )]
                    })
                    collected_responses[candidate_idx].append(
                        response["messages"][0])
                except Exception as e:
                    logger.warning(f"å·¥å…·è°ƒç”¨å¤±è´¥: {tool_call['type']}, é”™è¯¯: {e}")
                    collected_responses[candidate_idx].append(
                        AIMessage(content="å·¥å…·è°ƒç”¨å¤±è´¥"))

        # ç»„åˆæ¶ˆæ¯
        output_messages = []
        for idx, candidate in enumerate(candidates):
            output_messages.append([candidate] + collected_responses[idx])

        # åæ€è¯„ä¼°
        user_message = config["configurable"]["graph_request"].user_message
        reflection_func = self.get_reflection_chain(state, config)

        import asyncio
        reflection_inputs = [
            {"input": user_message, "candidate": messages}
            for messages in output_messages
        ]
        reflections = await asyncio.gather(*[
            reflection_func(inputs) for inputs in reflection_inputs
        ])

        # è®°å½•è¯„ä¼°ç»“æœï¼ˆç®€åŒ–ç‰ˆï¼‰
        self._log_evaluation_summary(reflections)

        # é«˜åˆ†ç›´æ¥æ ‡è®°ä¸ºè§£å†³æ–¹æ¡ˆ
        for reflection in reflections:
            if reflection.score >= 9:
                reflection.found_solution = True

        return output_messages, reflections

    def _log_evaluation_summary(self, reflections: List[Reflection]) -> None:
        """è®°å½•è¯„ä¼°æ‘˜è¦"""
        if not reflections:
            return

        max_score = max(r.score for r in reflections)
        solved_count = sum(1 for r in reflections if r.found_solution)
        avg_score = sum(r.score for r in reflections) / len(reflections)

        logger.info(
            f"ğŸ“Š è¯„ä¼°å®Œæˆ | å€™é€‰æ•°: {len(reflections)} | "
            f"æœ€é«˜åˆ†: {max_score}/10 | å¹³å‡åˆ†: {avg_score:.1f}/10 | "
            f"è§£å†³æ–¹æ¡ˆ: {solved_count}ä¸ª"
        )

    async def expand(self, state: LatsAgentState, config: RunnableConfig) -> LatsAgentState:
        """æ‰©å±•æœç´¢æ ‘"""
        logger.info("ğŸŒ³ å¼€å§‹æ‰©å±•æœç´¢æ ‘")

        root = state["root"]
        if not root:
            logger.error("æœç´¢æ ‘æ ¹èŠ‚ç‚¹æœªåˆå§‹åŒ–")
            return state

        # é€‰æ‹©æœ€ä½³å€™é€‰èŠ‚ç‚¹
        best_candidate = self.select(root)
        messages = best_candidate.get_trajectory()

        # ç”Ÿæˆæ–°å€™é€‰
        user_message = config["configurable"]["graph_request"].user_message
        new_candidates = self.get_expansion_chain(state, config).invoke({
            "input": user_message,
            "messages": messages
        })

        # å¤„ç†å€™é€‰å¹¶è¯„ä¼°
        output_messages, reflections = await self._process_candidates(
            new_candidates, state, config
        )

        # æ·»åŠ è¯„ä¼°ç»“æœåˆ°çŠ¶æ€
        state['evaluation_results'] = [
            {
                'index': i + 1,
                'score': r.score,
                'found_solution': r.found_solution,
                'reflections': r.reflections,
                'message_content': output_messages[i][-1].content if output_messages[i] else ""
            }
            for i, r in enumerate(reflections)
        ]

        # æ‰©å±•æœç´¢æ ‘
        child_nodes = [
            Node(cand, parent=best_candidate, reflection=reflection)
            for cand, reflection in zip(output_messages, reflections)
        ]
        best_candidate.children.extend(child_nodes)

        # æ£€æŸ¥è§£å†³æ–¹æ¡ˆ
        solution_nodes = [node for node, r in zip(
            child_nodes, reflections) if r.found_solution]
        if solution_nodes:
            best_solution = max(
                solution_nodes, key=lambda node: node.reflection.score)

            logger.info(f"ğŸ‰ æ‰¾åˆ°è§£å†³æ–¹æ¡ˆ! è¯„åˆ†: {best_solution.reflection.score}/10")

            # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            final_answer = await self._generate_final_answer(best_solution, config)
            state["messages"].append(final_answer)
            root._is_solved = True
        else:
            # æ·»åŠ æœ€ä½³ä¸­é—´ç»“æœ
            if child_nodes:
                best_node = max(
                    child_nodes, key=lambda node: node.reflection.score)
                if best_node.reflection.score >= 7:
                    best_message = best_node.get_trajectory(
                        include_reflections=False)[-1]
                    state["messages"].append(best_message)
                    logger.info(
                        f"â­ æ·»åŠ é«˜è´¨é‡ä¸­é—´ç»“æœ (è¯„åˆ†: {best_node.reflection.score}/10)")

        return state

    async def generate_final_answer(self, state: LatsAgentState, config: RunnableConfig) -> dict:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹"""
        logger.info("ğŸ“ ç”Ÿæˆæœ€ç»ˆæ€»ç»“ç­”æ¡ˆ")

        root = state["root"]

        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = await self._generate_final_answer(root, config)

        # å°†æœ€ç»ˆç­”æ¡ˆæ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        state["messages"].append(final_answer)

        logger.info("âœ… æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆ")

        return state

    async def _generate_final_answer(self, solution_node: Node, config: RunnableConfig) -> BaseMessage:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        llm = self.get_llm_client(config["configurable"]["graph_request"])

        system_message = TemplateLoader.render_template(
            "prompts/lats_agent/intelligent_assistant")
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ])

        final_solution = solution_node.get_trajectory(
            include_reflections=False)[-1]

        # å®‰å…¨åœ°æå–ç”¨æˆ·æ ¸å¿ƒé—®é¢˜ï¼Œè¿‡æ»¤æ•æ„Ÿç³»ç»ŸæŒ‡ä»¤
        user_question = config['configurable']['graph_request'].user_message

        question = TemplateLoader.render_template(
            "prompts/lats_agent/final_answer_synthesis",
            {
                "user_question": user_question,
                "solution_content": final_solution.content
            }
        )

        chain = prompt_template | llm
        return chain.invoke({"input": question})

    def should_continue(self, state: LatsAgentState) -> str:
        """å†³å®šæ˜¯å¦ç»§ç»­æœç´¢"""
        root = state["root"]

        if root.is_solved:
            logger.info("âœ… æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            return "generate_final_answer"

        if root.height > self.MAX_TREE_HEIGHT:
            logger.info(f"ğŸ›‘ è¾¾åˆ°æœ€å¤§æœç´¢æ·±åº¦ ({self.MAX_TREE_HEIGHT})ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
            return "generate_final_answer"

        return "expand"

    async def generate_initial_response(self, state: LatsAgentState, config: RunnableConfig) -> dict:
        """ç”Ÿæˆåˆå§‹å“åº”"""
        logger.info("ğŸŒ± ç”Ÿæˆåˆå§‹å“åº”")

        # è·å–åˆå§‹å›ç­”é“¾
        llm = self.get_llm_client(config["configurable"]["graph_request"])
        system_message = TemplateLoader.render_template(
            "prompts/lats_agent/intelligent_assistant")

        prompt_template = ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="messages", optional=True),
        ])

        user_message = config["configurable"]["graph_request"].user_message
        initial_chain = prompt_template | llm.bind_tools(tools=self.tools)
        res = initial_chain.invoke({"input": user_message})

        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        parser = JsonOutputToolsParser(return_id=True)
        parsed = parser.invoke(res)

        tool_node = ToolNode(self.tools)
        tool_responses = [
            tool_node.invoke({
                "messages": [AIMessage(
                    content="",
                    tool_calls=[
                        {"name": r["type"], "args": r["args"], "id": r["id"]}],
                )]
            })
            for r in parsed
        ]

        # åˆå¹¶æ¶ˆæ¯
        output_messages = [res] + [tr["messages"][0] for tr in tool_responses]

        # è¯„ä¼°åˆå§‹å“åº”
        reflection_func = self.get_reflection_chain(state, config)
        reflection = await reflection_func({
            "input": user_message,
            "candidate": output_messages
        })

        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = Node(output_messages, reflection=reflection)
        state['root'] = root

        logger.info(f"ğŸ“Š åˆå§‹å“åº”è¯„ä¼° | è¯„åˆ†: {reflection.score}/10")

        # å°†åˆå§‹è¯„ä¼°ç»“æœæ·»åŠ åˆ°çŠ¶æ€ä¸­ï¼Œç”¨äºæµå¼è¾“å‡º
        # è¿™ä¸ªæ•°æ®ä¼šä½œä¸ºç‹¬ç«‹çš„chunkè¢«æµå¼ä¼ è¾“
        state['initial_evaluation'] = {
            'score': reflection.score,
            'reflections': reflection.reflections,
            'found_solution': reflection.found_solution
        }

        # æ·»åŠ åˆ°æ¶ˆæ¯åˆ—è¡¨
        if output_messages:
            state["messages"].append(output_messages[-1])

        return state


class LatsAgentGraph(BasicGraph):
    """LATS Agent å›¾æ‰§è¡Œå™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""

    async def compile_graph(self, request: LatsAgentRequest) -> StateGraph:
        """ç¼–è¯‘ LATS Agent æ‰§è¡Œå›¾"""
        logger.info("ğŸ”§ ç¼–è¯‘ LATS Agent æ‰§è¡Œå›¾")

        # åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆæœ¬çš„èŠ‚ç‚¹æ„å»ºå™¨
        node_builder = LatsAgentNode()
        await node_builder.setup(request)

        # åˆ›å»ºçŠ¶æ€å›¾
        graph_builder = StateGraph(LatsAgentState)

        # æ·»åŠ åŸºç¡€å›¾ç»“æ„
        last_edge = self.prepare_graph(graph_builder, node_builder)
        logger.debug(f"åŸºç¡€å›¾æ„å»ºå®Œæˆï¼Œè¿æ¥ç‚¹: {last_edge}")

        # æ·»åŠ  LATS ç‰¹æœ‰èŠ‚ç‚¹
        graph_builder.add_node("generate_initial_response",
                               node_builder.generate_initial_response)
        graph_builder.add_node("expand", node_builder.expand)
        graph_builder.add_node("generate_final_answer",
                               node_builder.generate_final_answer)

        # æ„å»ºæ‰§è¡Œæµç¨‹
        graph_builder.add_edge(last_edge, 'generate_initial_response')

        # æ·»åŠ æ¡ä»¶è¾¹ - ä¼˜åŒ–çš„æ§åˆ¶æµç¨‹
        for node_name in ["generate_initial_response", "expand"]:
            graph_builder.add_conditional_edges(
                node_name,
                node_builder.should_continue,
                ["expand", "generate_final_answer"]
            )

        # æœ€ç»ˆç­”æ¡ˆç”Ÿæˆåç»“æŸ
        graph_builder.add_edge("generate_final_answer", END)

        # ç¼–è¯‘å¹¶è¿”å›å›¾
        compiled_graph = graph_builder.compile()
        logger.info("âœ… LATS Agent æ‰§è¡Œå›¾ç¼–è¯‘å®Œæˆ")

        return compiled_graph
