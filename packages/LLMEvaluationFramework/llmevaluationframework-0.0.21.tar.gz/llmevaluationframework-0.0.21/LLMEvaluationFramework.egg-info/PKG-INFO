Metadata-Version: 2.4
Name: LLMEvaluationFramework
Version: 0.0.21
Summary: Enterprise-Grade Python Framework for Large Language Model Evaluation & Testing
Home-page: https://github.com/isathish/LLMEvaluationFramework
Author: Sathishkumar Nagarajan
Author-email: mail@sathishkumarnagarajan.com
License: MIT
Project-URL: Bug Tracker, https://github.com/isathish/LLMEvaluationFramework/issues
Project-URL: Documentation, https://isathish.github.io/LLMEvaluationFramework/
Project-URL: Source, https://github.com/isathish/LLMEvaluationFramework
Keywords: llm,evaluation,testing,benchmarking,ai,ml,machine-learning,natural-language-processing,nlp,language-models,openai,gpt,anthropic,claude
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typing-extensions>=4.0.0
Requires-Dist: dataclasses>=0.6; python_version < "3.7"
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-cov>=2.0; extra == "dev"
Requires-Dist: black>=21.0; extra == "dev"
Requires-Dist: flake8>=3.8; extra == "dev"
Requires-Dist: mypy>=0.800; extra == "dev"
Provides-Extra: docs
Requires-Dist: mkdocs>=1.4; extra == "docs"
Requires-Dist: mkdocs-material>=8.0; extra == "docs"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ğŸš€ LLM Evaluation Framework

<div align="center">

[![License](https://img.shields.io/github/license/isathish/LLMEvaluationFramework?style=for-the-badge&color=blue)](LICENSE)
[![Tests](https://img.shields.io/badge/Tests-212%20Passed-success?style=for-the-badge&logo=pytest)](https://github.com/isathish/LLMEvaluationFramework)
[![Coverage](https://img.shields.io/badge/Coverage-89%25-success?style=for-the-badge&logo=codecov)](https://github.com/isathish/LLMEvaluationFramework)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=for-the-badge&logo=python)](https://python.org)
[![Documentation](https://img.shields.io/badge/Docs-MkDocs-blue?style=for-the-badge&logo=gitbook)](https://isathish.github.io/LLMEvaluationFramework/)

**ğŸŒŸ Enterprise-Grade Python Framework for Large Language Model Evaluation & Testing ğŸŒŸ**

*Built with production-ready standards â€¢ Type-safe â€¢ Comprehensive testing â€¢ Full CLI support*

[ğŸ“š **Documentation**](https://isathish.github.io/LLMEvaluationFramework/) â€¢ [ï¿½ **Quick Start**](#-quick-start) â€¢ [ğŸ’¡ **Examples**](examples/) â€¢ [ğŸ› **Report Issues**](https://github.com/isathish/LLMEvaluationFramework/issues)

</div>

---

## ğŸŒŸ What Makes This Special?

<table>
<tr>
<td width="50%">

### ğŸ¯ **Production Ready**
- **212 comprehensive tests** with **89% coverage**
- Complete type hints throughout codebase
- Robust error handling with custom exceptions
- Enterprise-grade logging and monitoring

### âš¡ **High Performance**
- Async inference engine for concurrent evaluations
- Batch processing capabilities
- Cost optimization and tracking
- Memory-efficient data handling

</td>
<td width="50%">

### ï¿½ï¸ **Developer Friendly**
- Intuitive CLI interface for all operations
- Comprehensive documentation with examples
- Modular architecture for easy extension
- Multiple storage backends (JSON, SQLite)

### ğŸ“Š **Rich Analytics**
- Multiple scoring strategies (Accuracy, F1, Custom)
- Detailed performance metrics
- Cost analysis and optimization
- Exportable evaluation reports

</td>
</tr>
</table>

---

## ï¿½ Quick Installation

```bash
# Install from PyPI (Recommended)
pip install LLMEvaluationFramework

# Or install from source for latest features
git clone https://github.com/isathish/LLMEvaluationFramework.git
cd LLMEvaluationFramework
pip install -e .
```

**Requirements**: Python 3.8+ â€¢ No external dependencies for core functionality

---

## âš¡ Quick Start

### ğŸ Python API (Recommended)

```python
from llm_evaluation_framework import (
    ModelRegistry, 
    ModelInferenceEngine, 
    TestDatasetGenerator
)

# 1ï¸âƒ£ Setup the registry and register your model
registry = ModelRegistry()
registry.register_model("gpt-3.5-turbo", {
    "provider": "openai",
    "api_cost_input": 0.0015,
    "api_cost_output": 0.002,
    "capabilities": ["reasoning", "creativity", "coding"]
})

# 2ï¸âƒ£ Generate test cases
generator = TestDatasetGenerator()
test_cases = generator.generate_test_cases(
    use_case={"domain": "general", "required_capabilities": ["reasoning"]},
    count=10
)

# 3ï¸âƒ£ Run evaluation
engine = ModelInferenceEngine(registry)
results = engine.evaluate_model("gpt-3.5-turbo", test_cases)

# 4ï¸âƒ£ Analyze results
print(f"âœ… Accuracy: {results['aggregate_metrics']['accuracy']:.1%}")
print(f"ğŸ’° Total Cost: ${results['aggregate_metrics']['total_cost']:.4f}")
print(f"â±ï¸  Total Time: {results['aggregate_metrics']['total_time']:.2f}s")
```

### ğŸ–¥ï¸ Command Line Interface

```bash
# Evaluate a model with specific capabilities
llm-eval evaluate --model gpt-3.5-turbo --test-cases 10 --capability reasoning

# Generate a custom test dataset
llm-eval generate --capability coding --count 20 --output my_dataset.json

# Score predictions against references
llm-eval score --predictions "Hello world" "Good morning" \
               --references "Hello world" "Good evening" \
               --metric accuracy

# List available capabilities and models
llm-eval list
```

---

## ï¿½ï¸ Core Architecture

<div align="center">

```mermaid
graph TB
    CLI[ğŸ–¥ï¸ CLI Interface<br/>llm-eval] --> Engine[âš™ï¸ Inference Engine<br/>ModelInferenceEngine]
    
    Engine --> Registry[ğŸ—„ï¸ Model Registry<br/>ModelRegistry]
    Engine --> Generator[ğŸ§ª Dataset Generator<br/>TestDatasetGenerator]
    Engine --> Scoring[ğŸ“Š Scoring Strategies<br/>AccuracyScoringStrategy]
    
    Registry --> Models[(ğŸ¤– Models<br/>gpt-3.5-turbo, gpt-4, etc.)]
    
    Engine --> Storage[ğŸ’¾ Persistence Layer]
    Storage --> JSON[ğŸ“„ JSON Store]
    Storage --> SQLite[ğŸ—ƒï¸ SQLite Store]
    
    Engine --> Utils[ğŸ› ï¸ Utilities]
    Utils --> Logger[ğŸ“ Advanced Logging]
    Utils --> ErrorHandler[ğŸ›¡ï¸ Error Handling]
    Utils --> AutoSuggest[ğŸ’¡ Auto Suggestions]
```

</div>

### ğŸ¯ Core Components

| Component | Description | Key Features |
|-----------|-------------|--------------|
| **ğŸ”¥ Inference Engine** | Execute and evaluate LLM inferences | Async processing, cost tracking, batch operations |
| **ğŸ—„ï¸ Model Registry** | Centralized model management | Multi-provider support, configuration management |
| **ğŸ§ª Dataset Generator** | Create synthetic test cases | Capability-based generation, domain-specific tests |
| **ğŸ“Š Scoring Strategies** | Multiple evaluation metrics | Accuracy, F1-score, custom metrics |
| **ğŸ’¾ Persistence Layer** | Dual storage backends | JSON files, SQLite database with querying |
| **ğŸ›¡ï¸ Error Handling** | Robust error management | Custom exceptions, retry mechanisms |
| **ğŸ“ Logging System** | Advanced logging capabilities | File rotation, structured logging |

---

## ğŸ¯ Feature Highlights

### ğŸš€ **What You Can Do**

<table>
<tr>
<td width="33%">

#### ğŸ”¬ **Research & Benchmarking**
- Compare multiple LLM providers
- Standardized evaluation metrics  
- Reproducible experiments
- Performance benchmarking

</td>
<td width="33%">

#### ğŸ¢ **Enterprise Integration**
- CI/CD pipeline integration
- Automated regression testing
- Cost optimization analysis
- Quality assurance workflows

</td>
<td width="33%">

#### ğŸ’° **Cost Management**
- Real-time cost tracking
- Provider cost comparison
- Budget optimization
- ROI analysis

</td>
</tr>
</table>

### ğŸ“Š **Supported Capabilities**

```python
# Available evaluation capabilities
CAPABILITIES = [
    "reasoning",      # Logical reasoning and problem-solving
    "creativity",     # Creative writing and ideation
    "factual",        # Factual accuracy and knowledge
    "instruction",    # Instruction following
    "coding"          # Code generation and debugging
]
```

### ğŸ® **Interactive Examples**

<details>
<summary>ğŸ” <strong>Click to see Advanced Usage Examples</strong></summary>

#### ğŸ“ˆ **Batch Evaluation with Multiple Models**

```python
from llm_evaluation_framework import ModelRegistry, ModelInferenceEngine
from llm_evaluation_framework.persistence import JSONStore

# Setup multiple models
registry = ModelRegistry()
models = {
    "gpt-3.5-turbo": {"provider": "openai", "cost_input": 0.0015},
    "gpt-4": {"provider": "openai", "cost_input": 0.03},
    "claude-3": {"provider": "anthropic", "cost_input": 0.015}
}

for name, config in models.items():
    registry.register_model(name, config)

# Run comparative evaluation
engine = ModelInferenceEngine(registry)
results = {}

for model_name in models.keys():
    print(f"ğŸš€ Evaluating {model_name}...")
    result = engine.evaluate_model(model_name, test_cases)
    results[model_name] = result
    
    # Save results
    store = JSONStore(f"results_{model_name}.json")
    store.save_evaluation_result(result)

# Compare results
for model, result in results.items():
    accuracy = result['aggregate_metrics']['accuracy']
    cost = result['aggregate_metrics']['total_cost']
    print(f"ğŸ“Š {model}: {accuracy:.1%} accuracy, ${cost:.4f} cost")
```

#### ğŸ¯ **Custom Scoring Strategy**

```python
from llm_evaluation_framework.evaluation.scoring_strategies import ScoringContext

class CustomCosineSimilarityStrategy:
    """Custom scoring using cosine similarity."""
    
    def calculate_score(self, predictions, references):
        # Your custom scoring logic here
        from sklearn.metrics.pairwise import cosine_similarity
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        vectorizer = TfidfVectorizer()
        vectors = vectorizer.fit_transform(predictions + references)
        
        pred_vectors = vectors[:len(predictions)]
        ref_vectors = vectors[len(predictions):]
        
        similarities = cosine_similarity(pred_vectors, ref_vectors)
        return similarities.diagonal().mean()

# Use custom strategy
custom_strategy = CustomCosineSimilarityStrategy()
context = ScoringContext(custom_strategy)
score = context.evaluate(predictions, references)
print(f"ğŸ¯ Custom similarity score: {score:.3f}")
```

#### ğŸ”„ **Async Evaluation Pipeline**

```python
import asyncio
from llm_evaluation_framework.engines.async_inference_engine import AsyncInferenceEngine

async def run_async_evaluation():
    """Run multiple evaluations concurrently."""
    
    async_engine = AsyncInferenceEngine(registry)
    
    # Define multiple evaluation tasks
    tasks = []
    for capability in ["reasoning", "creativity", "coding"]:
        task = async_engine.evaluate_async(
            model_name="gpt-3.5-turbo",
            test_cases=test_cases,
            capability=capability
        )
        tasks.append(task)
    
    # Run all evaluations concurrently
    results = await asyncio.gather(*tasks)
    
    # Process results
    for i, result in enumerate(results):
        capability = ["reasoning", "creativity", "coding"][i]
        accuracy = result['aggregate_metrics']['accuracy']
        print(f"âœ… {capability}: {accuracy:.1%}")

# Run async evaluation
asyncio.run(run_async_evaluation())
```

</details>

---

## ğŸ“š Documentation & Resources

<div align="center">

### ğŸ“– **Comprehensive Documentation Available**

[![Documentation](https://img.shields.io/badge/Read%20the%20Docs-blue?style=for-the-badge&logo=gitbook)](https://isathish.github.io/LLMEvaluationFramework/)

</div>

| Section | Description | Link |
|---------|-------------|------|
| ğŸš€ **Getting Started** | Installation, quick start, and basic concepts | [View Guide](https://isathish.github.io/LLMEvaluationFramework/categories/getting-started/) |
| ğŸ§  **Core Concepts** | Understanding the framework architecture | [Learn More](https://isathish.github.io/LLMEvaluationFramework/categories/core-concepts/) |
| ğŸ–¥ï¸ **CLI Usage** | Complete command-line interface documentation | [CLI Guide](https://isathish.github.io/LLMEvaluationFramework/categories/cli-usage/) |
| ğŸ“Š **API Reference** | Detailed API documentation with examples | [API Docs](https://isathish.github.io/LLMEvaluationFramework/categories/api-reference/) |
| ğŸ’¡ **Examples** | Practical examples and tutorials | [View Examples](https://isathish.github.io/LLMEvaluationFramework/categories/examples/) |
| ğŸ› ï¸ **Developer Guide** | Contributing guidelines and development setup | [Dev Guide](https://isathish.github.io/LLMEvaluationFramework/developer-guide/) |

---

## ğŸ§ª Testing & Quality

<div align="center">

### ğŸ† **High-Quality Codebase with Comprehensive Testing**

</div>

<table>
<tr>
<td width="25%" align="center">

**ğŸ“ˆ Test Coverage**
<br>
<strong style="font-size: 2em; color: #28a745;">89%</strong>
<br>
<em>Comprehensive test coverage</em>

</td>
<td width="25%" align="center">

**âœ… Total Tests**
<br>
<strong style="font-size: 2em; color: #007bff;">212</strong>
<br>
<em>All tests passing</em>

</td>
<td width="25%" align="center">

**ğŸ”§ Test Files**
<br>
<strong style="font-size: 2em; color: #6f42c1;">10+</strong>
<br>
<em>Modular test structure</em>

</td>
<td width="25%" align="center">

**âš¡ Test Types**
<br>
<strong style="font-size: 2em; color: #fd7e14;">4+</strong>
<br>
<em>Unit, Integration, Edge Cases</em>

</td>
</tr>
</table>

### ğŸš€ **Run Tests Locally**

```bash
# Run all tests
pytest

# Run with detailed coverage report
pytest --cov=llm_evaluation_framework --cov-report=html

# Run specific test categories
pytest tests/test_model_inference_engine_comprehensive.py  # Core engine tests
pytest tests/test_cli_comprehensive.py                     # CLI tests
pytest tests/test_persistence_comprehensive.py            # Storage tests

# View coverage report
open htmlcov/index.html
```

### ğŸ“Š **Test Categories**

| Test Type | Count | Description |
|-----------|-------|-------------|
| **ğŸ”§ Unit Tests** | 150+ | Individual component testing |
| **ğŸ”— Integration Tests** | 40+ | Component interaction testing |
| **ğŸ¯ Edge Case Tests** | 20+ | Error conditions and boundaries |
| **âš¡ Performance Tests** | 10+ | Speed and memory optimization |

---

## ğŸ¤ Contributing

<div align="center">

### ğŸŒŸ **We Welcome Contributors!**

[![Contributors](https://img.shields.io/github/contributors/isathish/LLMEvaluationFramework?style=for-the-badge)](https://github.com/isathish/LLMEvaluationFramework/graphs/contributors)
[![Issues](https://img.shields.io/github/issues/isathish/LLMEvaluationFramework?style=for-the-badge)](https://github.com/isathish/LLMEvaluationFramework/issues)
[![Pull Requests](https://img.shields.io/github/issues-pr/isathish/LLMEvaluationFramework?style=for-the-badge)](https://github.com/isathish/LLMEvaluationFramework/pulls)

</div>

### ğŸ› ï¸ **Development Setup**

```bash
# 1ï¸âƒ£ Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/LLMEvaluationFramework.git
cd LLMEvaluationFramework

# 2ï¸âƒ£ Create a virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3ï¸âƒ£ Install in development mode
pip install -e ".[dev]"

# 4ï¸âƒ£ Run tests to ensure everything works
pytest

# 5ï¸âƒ£ Install pre-commit hooks (optional but recommended)
pre-commit install
```

### ğŸ“ **Contribution Guidelines**

1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **âœ… Write** tests for your changes
4. **ğŸ§ª Run** the test suite (`pytest`)
5. **ğŸ“ Commit** your changes (`git commit -m 'Add amazing feature'`)
6. **ğŸš€ Push** to the branch (`git push origin feature/amazing-feature`)
7. **ğŸ”€ Open** a Pull Request

### ğŸ¯ **What We're Looking For**

- ğŸ› Bug fixes and improvements
- ğŸ“š Documentation enhancements
- âœ¨ New features and capabilities
- ğŸ§ª Additional test cases
- ğŸ¨ UI/UX improvements for CLI
- ğŸ”§ Performance optimizations

---

## ğŸ“‹ Requirements & Compatibility

### ğŸ **Python Version Support**

| Python Version | Status | Notes |
|----------------|--------|-------|
| **Python 3.8** | âœ… Supported | Minimum required version |
| **Python 3.9** | âœ… Supported | Fully tested |
| **Python 3.10** | âœ… Supported | Recommended |
| **Python 3.11** | âœ… Supported | Latest features |
| **Python 3.12+** | âœ… Supported | Future-ready |

### ğŸ“¦ **Dependencies**

```python
# Core dependencies (automatically installed)
REQUIRED = [
    # No external dependencies for core functionality!
    # Framework uses only Python standard library
]

# Optional development dependencies
DEVELOPMENT = [
    "pytest>=7.0.0",           # Testing framework
    "pytest-cov>=4.0.0",      # Coverage reporting
    "black>=22.0.0",           # Code formatting
    "flake8>=5.0.0",           # Code linting
    "mypy>=1.0.0",             # Type checking
    "pre-commit>=2.20.0",      # Git hooks
]
```

### ğŸŒ **Platform Support**

- âœ… **Linux** (Ubuntu, CentOS, RHEL)
- âœ… **macOS** (Intel & Apple Silicon)
- âœ… **Windows** (10, 11)
- âœ… **Docker** containers
- âœ… **CI/CD** environments (GitHub Actions, Jenkins, etc.)

---

## ğŸ“„ License

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**This project is licensed under the MIT License**

*You are free to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the software.*

[ğŸ“œ **Read the full license**](LICENSE)

</div>

---

## ğŸ™ Acknowledgments & Credits

<div align="center">

### ğŸŒŸ **Built with Love and Open Source**

</div>

- **ğŸš€ Inspiration**: Born from the need for standardized, reliable LLM evaluation tools
- **ğŸ—ï¸ Architecture**: Built with modern Python best practices and enterprise standards
- **ğŸ§ª Testing**: Comprehensive test coverage ensuring production reliability  
- **ğŸ‘¥ Community**: Driven by developers, researchers, and AI practitioners
- **ğŸ“š Documentation**: Extensive documentation for developers at all levels

### ğŸ”§ **Technology Stack**

| Technology | Purpose | Why We Chose It |
|------------|---------|----------------|
| **ğŸ Python 3.8+** | Core Language | Wide adoption, excellent ecosystem |
| **ğŸ“‹ Type Hints** | Code Safety | Better IDE support, fewer runtime errors |
| **ğŸ§ª Pytest** | Testing Framework | Industry standard, excellent plugin ecosystem |
| **ğŸ“Š SQLite** | Database Storage | Lightweight, serverless, reliable |
| **ğŸ“ MkDocs** | Documentation | Beautiful docs, Markdown-based |
| **ğŸ¨ Rich CLI** | User Interface | Modern, intuitive command-line experience |

---

## ğŸ“ Support & Community

<div align="center">

### ğŸ’¬ **Get Help & Connect**

[![GitHub Issues](https://img.shields.io/badge/Issues-Get%20Help-red?style=for-the-badge&logo=github)](https://github.com/isathish/LLMEvaluationFramework/issues)
[![GitHub Discussions](https://img.shields.io/badge/Discussions-Join%20Community-blue?style=for-the-badge&logo=github)](https://github.com/isathish/LLMEvaluationFramework/discussions)
[![Documentation](https://img.shields.io/badge/Docs-Read%20Here-green?style=for-the-badge&logo=gitbook)](https://isathish.github.io/LLMEvaluationFramework/)

</div>

### ğŸ†˜ **Getting Support**

| Type | Where to Go | Response Time |
|------|-------------|---------------|
| **ğŸ› Bug Reports** | [GitHub Issues](https://github.com/isathish/LLMEvaluationFramework/issues) | 24-48 hours |
| **â“ Questions** | [GitHub Discussions](https://github.com/isathish/LLMEvaluationFramework/discussions) | Community-driven |
| **ğŸ“š Documentation** | [Online Docs](https://isathish.github.io/LLMEvaluationFramework/) | Always available |
| **ğŸ’¡ Feature Requests** | [GitHub Issues](https://github.com/isathish/LLMEvaluationFramework/issues) | Weekly review |

### ğŸ“ˆ **Project Statistics**

<div align="center">

![GitHub stars](https://img.shields.io/github/stars/isathish/LLMEvaluationFramework?style=social)
![GitHub forks](https://img.shields.io/github/forks/isathish/LLMEvaluationFramework?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/isathish/LLMEvaluationFramework?style=social)

</div>

---

## ğŸ”— Important Links

<div align="center">

### ğŸŒ **Quick Access**

| Resource | Link | Description |
|----------|------|-------------|
| **ğŸ“¦ PyPI Package** | [pypi.org/project/llm-evaluation-framework](https://pypi.org/project/llm-evaluation-framework/) | Install via pip |
| **ğŸ“š Documentation** | [isathish.github.io/LLMEvaluationFramework](https://isathish.github.io/LLMEvaluationFramework/) | Complete documentation |
| **ğŸ’» Source Code** | [github.com/isathish/LLMEvaluationFramework](https://github.com/isathish/LLMEvaluationFramework) | View source & contribute |
| **ğŸ› Issue Tracker** | [github.com/.../issues](https://github.com/isathish/LLMEvaluationFramework/issues) | Report bugs & request features |
| **ğŸ’¬ Discussions** | [github.com/.../discussions](https://github.com/isathish/LLMEvaluationFramework/discussions) | Community discussion |

</div>

---

<div align="center">

## ğŸ‰ **Thank You for Using LLM Evaluation Framework!**

<br>

**Made with â¤ï¸ by [Sathish Kumar N](https://github.com/isathish)**

*If you find this project useful, please consider giving it a â­ï¸*

<br>

[![Star this repo](https://img.shields.io/github/stars/isathish/LLMEvaluationFramework?style=social)](https://github.com/isathish/LLMEvaluationFramework/stargazers)

<br>

---

### ğŸš€ **Ready to Get Started?**

```bash
pip install LLMEvaluationFramework
```

**[ğŸ“š Read the Documentation](https://isathish.github.io/LLMEvaluationFramework/) â€¢ [ğŸš€ View Examples](examples/) â€¢ [ğŸ’¬ Join Discussions](https://github.com/isathish/LLMEvaluationFramework/discussions)**

---

*Built for developers, researchers, and AI practitioners who demand reliable, production-ready LLM evaluation tools.*

</div>
